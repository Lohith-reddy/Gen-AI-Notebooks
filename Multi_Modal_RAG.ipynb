{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkYCUxT4SS62",
        "outputId": "433c372d-60b6-45ee-91a9-193fbee19f6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script isympy.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script dotenv.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script watchfiles.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script uvicorn.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script nltk.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script humanfriendly.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script pyproject-build.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script typer.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts opentelemetry-bootstrap.exe and opentelemetry-instrument.exe are installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script openai.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script fastapi.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script coloredlogs.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script onnxruntime_test.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script chroma.exe is installed in 'C:\\Users\\Sai Lohith Reddy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cohere 5.3.2 requires tokenizers<0.16.0,>=0.15.2, but you have tokenizers 0.20.1 which is incompatible.\n",
            "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n",
            "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.3.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f88bv5mESdzN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chromadb\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import Settings, SimpleDirectoryReader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLKK9DrG5l3X"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TDkAhLG_Snt"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNYCWovlrwqA",
        "outputId": "2b443f0c-fc6b-489b-d8c1-4c1334cc9a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  papers.zip\n",
            "  inflating: Memory papers - short/The role of REM sleep theta in emotional memory.pdf  \n",
            "  inflating: Memory papers - short/THe restless engram consoli never end.pdf  \n",
            "  inflating: Memory papers - short/evolution of hippo in relation to neocortex.pdf  \n",
            "  inflating: Memory papers - short/Unified model of spatial and episodic memory.pdf  \n",
            "  inflating: Memory papers - short/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf  \n",
            "  inflating: Memory papers - short/Boosting slow oscillation during sleep to improve memory in old people.pdf  \n"
          ]
        }
      ],
      "source": [
        "# prompt: unzip Memory papers - short.zip into papers folder\n",
        "!unzip papers.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuQSaXVJSjGG"
      },
      "outputs": [],
      "source": [
        "# prompt: read config file config_4o.json and read AZURE_OPENAI_KEY into api_key and AZURE_OPENAI_KEY into endpoint\n",
        "\n",
        "import json\n",
        "\n",
        "def read_config(config_file):\n",
        "  \"\"\"Reads a JSON config file and returns a dictionary.\"\"\"\n",
        "  with open(config_file, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "config = read_config('config_4o.json')\n",
        "api_key = config.get('AZURE_OPENAI_KEY')\n",
        "endpoint = config.get('AZURE_OPENAI_ENDPOINT')\n",
        "emb_key = config.get('EMB_MODEL_KEY')\n",
        "emb_endpoint = config.get('EMB_DEPLOYMENT')\n",
        "llamaparse_api_key = config.get('LLAMA_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIRDS0IASmEB"
      },
      "outputs": [],
      "source": [
        "embedding_function = chromadb.utils.embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_base=emb_endpoint,\n",
        "    api_key=emb_key,\n",
        "    api_type='azure',\n",
        "    api_version='2024-02-01',\n",
        "    model_name='text-embedding-ada-002'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-R7dZnsStpP"
      },
      "outputs": [],
      "source": [
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=emb_endpoint,\n",
        "    api_key=emb_key,\n",
        "    api_version='2024-02-01',\n",
        "    model='text-embedding-ada-002'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrb4FrubS0qN"
      },
      "outputs": [],
      "source": [
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=api_key,\n",
        "    api_version='2024-02-01',\n",
        "    azure_deployment='gpt-4o',\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UCSCQAMGJ0C"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm\n",
        "Settings.embedding = embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8S1Dk3qC0mH"
      },
      "source": [
        "### llama parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyQz518VGYui"
      },
      "outputs": [],
      "source": [
        "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUNLcMiaC8py"
      },
      "outputs": [],
      "source": [
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    skip_diagonal_text=True,\n",
        "    fast_mode=False,\n",
        "    num_workers=9,\n",
        "    check_interval=10,\n",
        "    api_key=llamaparse_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGwpE83MvVo9",
        "outputId": "8c8a4060-6d66-4422-b03c-12facbede77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started parsing the file under job_id 74d8a123-17c6-4738-a82a-83346f6821a2\n"
          ]
        }
      ],
      "source": [
        "json_objs = parser.get_json_result(\"papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf\")\n",
        "json_list = json_objs[0][\"pages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OnmM6b0Fvt7W",
        "outputId": "9abe8032-00d2-4dc5-a8b1-533d5d685e89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'page': 1,\n",
              " 'text': 'The Functional and Structural\\nNeuroanatomy of Systems Consolidation\\nfor Autobiographical and Semantic\\nMemory\\n\\nAdam J.O. Dede and Christine N. Smith\\n\\nAbstract                             It is well established that patients with memory impairment have more\\ndifficulty retrieving memories from the recent past relative to the remote past and\\nthat damage to the medial temporal lobe (MTL) plays a key role in this pattern of\\nimpairment. The precise role of the MTL and how it may interact with other brain\\nregions remains an area of active research. We investigated the role of structures in\\na memory network that supports remembering. Our chapter focuses on two types of\\nmemory: episodic memory and semantic memory. Findings from studies of patients\\nwith brain damage and neuroimaging studies in patients and healthy individuals\\nwere considered together to identify the functional and structural neuroanatomy of\\npast remembrance.\\n\\nKeywords\\n         Connectivity Retrograde amnesia  Autobiographical memory  Semantic memory Neuroimaging                 Lesion                          Patient\\nContents\\n\\n 1          Introduction..........................................................................................................................\\n 2          Autobiographical Memory ..................................................................................................\\n            2.1              Autobiographical Memory in Memory-Impaired Patients.........................................\\n            2.2              Possible Semanticization of Episodic Memories .......................................................\\n            2.3              The Autobiographical Memory Network...................................................................\\n            2.4              Effects of Lesions in the Autobiographical Memory Network Beyond the MTL....\\n\\nA.J.O. Dede\\nDepartment of Psychology, University of California San Diego, San Diego\\nCA 92093, USA\\nC.N. Smith\\nDepartment of Psychiatry, University of California San Diego, San Diego\\nCA 92093, USA\\nA.J.O. Dede  C.N. Smith (&)\\nVeteran Affairs San Diego Healthcare System, 3350 La Jolla Village Drive (116A),\\nSan Diego, CA 92161, USA\\ne-mail: cnsmith@ucsd.edu\\n\\n© Springer International Publishing Switzerland 2016\\nCurr Topics Behav Neurosci\\nDOI 10.1007/7854_2016_452',\n",
              " 'md': '# The Functional and Structural Neuroanatomy of Systems Consolidation for Autobiographical and Semantic Memory\\n\\n# Adam J.O. Dede and Christine N. Smith\\n\\n# Abstract\\n\\nIt is well established that patients with memory impairment have more difficulty retrieving memories from the recent past relative to the remote past and that damage to the medial temporal lobe (MTL) plays a key role in this pattern of impairment. The precise role of the MTL and how it may interact with other brain regions remains an area of active research. We investigated the role of structures in a memory network that supports remembering. Our chapter focuses on two types of memory: episodic memory and semantic memory. Findings from studies of patients with brain damage and neuroimaging studies in patients and healthy individuals were considered together to identify the functional and structural neuroanatomy of past remembrance.\\n\\n# Keywords\\n\\nConnectivity, Retrograde amnesia, Autobiographical memory, Semantic memory, Neuroimaging, Lesion, Patient\\n\\n# Contents\\n\\n1. Introduction\\n2. Autobiographical Memory\\n\\n# A.J.O. Dede\\n\\nDepartment of Psychology, University of California San Diego, San Diego CA 92093, USA\\n\\n# C.N. Smith\\n\\nDepartment of Psychiatry, University of California San Diego, San Diego CA 92093, USA\\n\\n# A.J.O. Dede  C.N. Smith (&)\\n\\nVeteran Affairs San Diego Healthcare System, 3350 La Jolla Village Drive (116A), San Diego, CA 92161, USA\\n\\n# e-mail\\n\\ncnsmith@ucsd.edu\\n\\n© Springer International Publishing Switzerland 2016\\n\\nCurr Topics Behav Neurosci\\n\\nDOI 10.1007/7854_2016_452',\n",
              " 'images': [{'name': 'img_p0_1.png',\n",
              "   'height': 26,\n",
              "   'width': 318,\n",
              "   'x': 53.8580017,\n",
              "   'y': 626.45699958,\n",
              "   'original_width': 1100,\n",
              "   'original_height': 90}],\n",
              " 'items': [{'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'The Functional and Structural Neuroanatomy of Systems Consolidation for Autobiographical and Semantic Memory',\n",
              "   'md': '# The Functional and Structural Neuroanatomy of Systems Consolidation for Autobiographical and Semantic Memory',\n",
              "   'bBox': {'x': 53, 'y': 72.14, 'w': 278.17, 'h': 15.94}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'Adam J.O. Dede and Christine N. Smith',\n",
              "   'md': '# Adam J.O. Dede and Christine N. Smith',\n",
              "   'bBox': {'x': 53, 'y': 162.14, 'w': 175.09, 'h': 9.96}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'Abstract',\n",
              "   'md': '# Abstract',\n",
              "   'bBox': {'x': 0, 'y': 0, 'w': 439.37, 'h': 666.14}},\n",
              "  {'type': 'text',\n",
              "   'value': 'It is well established that patients with memory impairment have more difficulty retrieving memories from the recent past relative to the remote past and that damage to the medial temporal lobe (MTL) plays a key role in this pattern of impairment. The precise role of the MTL and how it may interact with other brain regions remains an area of active research. We investigated the role of structures in a memory network that supports remembering. Our chapter focuses on two types of memory: episodic memory and semantic memory. Findings from studies of patients with brain damage and neuroimaging studies in patients and healthy individuals were considered together to identify the functional and structural neuroanatomy of past remembrance.',\n",
              "   'md': 'It is well established that patients with memory impairment have more difficulty retrieving memories from the recent past relative to the remote past and that damage to the medial temporal lobe (MTL) plays a key role in this pattern of impairment. The precise role of the MTL and how it may interact with other brain regions remains an area of active research. We investigated the role of structures in a memory network that supports remembering. Our chapter focuses on two types of memory: episodic memory and semantic memory. Findings from studies of patients with brain damage and neuroimaging studies in patients and healthy individuals were considered together to identify the functional and structural neuroanatomy of past remembrance.',\n",
              "   'bBox': {'x': 53, 'y': 72.14, 'w': 331.8, 'h': 15.94}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'Keywords',\n",
              "   'md': '# Keywords',\n",
              "   'bBox': {'x': 0, 'y': 0, 'w': 439.37, 'h': 666.14}},\n",
              "  {'type': 'text',\n",
              "   'value': 'Connectivity, Retrograde amnesia, Autobiographical memory, Semantic memory, Neuroimaging, Lesion, Patient',\n",
              "   'md': 'Connectivity, Retrograde amnesia, Autobiographical memory, Semantic memory, Neuroimaging, Lesion, Patient',\n",
              "   'bBox': {'x': 53, 'y': 126.14, 'w': 57.91, 'h': 15.94}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'Contents',\n",
              "   'md': '# Contents',\n",
              "   'bBox': {'x': 53, 'y': 385.14, 'w': 37.82, 'h': 9.96}},\n",
              "  {'type': 'text',\n",
              "   'value': '1. Introduction\\n2. Autobiographical Memory',\n",
              "   'md': '1. Introduction\\n2. Autobiographical Memory',\n",
              "   'bBox': {'x': 53, 'y': 126.14, 'w': 57.91, 'h': 15.94}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'A.J.O. Dede',\n",
              "   'md': '# A.J.O. Dede',\n",
              "   'bBox': {'x': 53, 'y': 481.14, 'w': 42.32, 'h': 8.47}},\n",
              "  {'type': 'text',\n",
              "   'value': 'Department of Psychology, University of California San Diego, San Diego CA 92093, USA',\n",
              "   'md': 'Department of Psychology, University of California San Diego, San Diego CA 92093, USA',\n",
              "   'bBox': {'x': 53, 'y': 491.14, 'w': 258.02, 'h': 8.47}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'C.N. Smith',\n",
              "   'md': '# C.N. Smith',\n",
              "   'bBox': {'x': 53, 'y': 515.14, 'w': 38.77, 'h': 8.47}},\n",
              "  {'type': 'text',\n",
              "   'value': 'Department of Psychiatry, University of California San Diego, San Diego CA 92093, USA',\n",
              "   'md': 'Department of Psychiatry, University of California San Diego, San Diego CA 92093, USA',\n",
              "   'bBox': {'x': 53, 'y': 501.14, 'w': 254.05, 'h': 8.47}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'A.J.O. Dede  C.N. Smith (&)',\n",
              "   'md': '# A.J.O. Dede  C.N. Smith (&)',\n",
              "   'bBox': {'x': 53, 'y': 481.14, 'w': 107.73, 'h': 8.47}},\n",
              "  {'type': 'text',\n",
              "   'value': 'Veteran Affairs San Diego Healthcare System, 3350 La Jolla Village Drive (116A), San Diego, CA 92161, USA',\n",
              "   'md': 'Veteran Affairs San Diego Healthcare System, 3350 La Jolla Village Drive (116A), San Diego, CA 92161, USA',\n",
              "   'bBox': {'x': 53, 'y': 560.14, 'w': 288.27, 'h': 8.47}},\n",
              "  {'type': 'heading',\n",
              "   'lvl': 1,\n",
              "   'value': 'e-mail',\n",
              "   'md': '# e-mail',\n",
              "   'bBox': {'x': 0, 'y': 0, 'w': 439.37, 'h': 666.14}},\n",
              "  {'type': 'text',\n",
              "   'value': 'cnsmith@ucsd.edu\\n\\n© Springer International Publishing Switzerland 2016\\n\\nCurr Topics Behav Neurosci\\n\\nDOI 10.1007/7854_2016_452',\n",
              "   'md': 'cnsmith@ucsd.edu\\n\\n© Springer International Publishing Switzerland 2016\\n\\nCurr Topics Behav Neurosci\\n\\nDOI 10.1007/7854_2016_452',\n",
              "   'bBox': {'x': 53, 'y': 600.14, 'w': 185.73, 'h': 8.47}}],\n",
              " 'status': 'OK',\n",
              " 'links': [{'text': 'Introduction..........................................................................................................................'},\n",
              "  {'text': 'Autobiographical Memory ..................................................................................................'},\n",
              "  {'text': 'Possible Semanticization of Episodic Memories .......................................................'},\n",
              "  {'text': 'The Autobiographical Memory Network...................................................................'},\n",
              "  {'text': 'Effects of Lesions in the Autobiographical Memory Network Beyond the MTL....'},\n",
              "  {'text': ''}]}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jxfmgaZVvWC8",
        "outputId": "c453c433-935d-4912-c328-03240909de58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Image for page 1: [{'name': 'img_p0_1.png', 'height': 26, 'width': 318, 'x': 53.8580017, 'y': 626.45699958, 'original_width': 1100, 'original_height': 90, 'path': 'images2/74d8a123-17c6-4738-a82a-83346f6821a2-img_p0_1.png', 'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2', 'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf', 'page_number': 1}]\n",
            "> Image for page 2: []\n",
            "> Image for page 3: []\n",
            "> Image for page 4: []\n",
            "> Image for page 5: []\n",
            "> Image for page 6: []\n",
            "> Image for page 7: []\n",
            "> Image for page 8: [{'name': 'img_p7_1.png', 'height': 278, 'width': 784, 'x': 56.67499922, 'y': 373.3239992200001, 'original_width': 680, 'original_height': 241, 'path': 'images2/74d8a123-17c6-4738-a82a-83346f6821a2-img_p7_1.png', 'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2', 'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf', 'page_number': 8}]\n",
            "> Image for page 9: []\n",
            "> Image for page 10: []\n",
            "> Image for page 11: []\n",
            "> Image for page 12: []\n",
            "> Image for page 13: []\n",
            "> Image for page 14: []\n",
            "> Image for page 15: []\n",
            "> Image for page 16: []\n",
            "> Image for page 17: []\n",
            "> Image for page 18: []\n",
            "> Image for page 19: [{'name': 'img_p18_1.png', 'height': 278, 'width': 784, 'x': 56.67499922, 'y': 378.87399466000005, 'original_width': 680, 'original_height': 241, 'path': 'images2/74d8a123-17c6-4738-a82a-83346f6821a2-img_p18_1.png', 'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2', 'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf', 'page_number': 19}]\n",
            "> Image for page 20: []\n",
            "> Image for page 21: []\n",
            "> Image for page 22: []\n",
            "> Image for page 23: []\n",
            "> Image for page 24: []\n",
            "> Image for page 25: []\n",
            "> Image for page 26: []\n",
            "> Image for page 27: []\n",
            "> Image for page 28: []\n",
            "> Image for page 29: []\n",
            "> Image for page 30: []\n",
            "> Image for page 31: []\n",
            "> Image for page 32: []\n"
          ]
        }
      ],
      "source": [
        "image_dicts = parser.get_images(\n",
        "    json_objs,\n",
        "    download_path=\"images3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjHiP60ZIOm3",
        "outputId": "dabbad1e-f771-4744-d6ca-b93bcfc279ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'img_p0_1.png',\n",
              "  'height': 26,\n",
              "  'width': 318,\n",
              "  'x': 53.8580017,\n",
              "  'y': 626.45699958,\n",
              "  'original_width': 1100,\n",
              "  'original_height': 90,\n",
              "  'path': 'images3/74d8a123-17c6-4738-a82a-83346f6821a2-img_p0_1.png',\n",
              "  'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2',\n",
              "  'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf',\n",
              "  'page_number': 1},\n",
              " {'name': 'img_p7_1.png',\n",
              "  'height': 278,\n",
              "  'width': 784,\n",
              "  'x': 56.67499922,\n",
              "  'y': 373.3239992200001,\n",
              "  'original_width': 680,\n",
              "  'original_height': 241,\n",
              "  'path': 'images3/74d8a123-17c6-4738-a82a-83346f6821a2-img_p7_1.png',\n",
              "  'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2',\n",
              "  'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf',\n",
              "  'page_number': 8},\n",
              " {'name': 'img_p18_1.png',\n",
              "  'height': 278,\n",
              "  'width': 784,\n",
              "  'x': 56.67499922,\n",
              "  'y': 378.87399466000005,\n",
              "  'original_width': 680,\n",
              "  'original_height': 241,\n",
              "  'path': 'images3/74d8a123-17c6-4738-a82a-83346f6821a2-img_p18_1.png',\n",
              "  'job_id': '74d8a123-17c6-4738-a82a-83346f6821a2',\n",
              "  'original_file_path': 'papers/THe functional and structural neuroanatomy of systems consolidation of autobio and semantic memory.pdf',\n",
              "  'page_number': 19}]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T7ch2yWI1HCA",
        "outputId": "a9762aa4-c9bd-460a-a54b-13a18a8e7afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading 74d8a123-17c6-4738-a82a-83346f6821a2-img_p0_1.png: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "Error loading 74d8a123-17c6-4738-a82a-83346f6821a2-img_p7_1.png: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "Error loading 74d8a123-17c6-4738-a82a-83346f6821a2-img_p18_1.png: too many indices for array: array is 1-dimensional, but 2 were indexed\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABMkAAAS0CAYAAAB67F+LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7NklEQVR4nO3df6yedX3/8XdbOKcY6QHX9bR0x3XgFBWk2MpZQWJczmyiqeOPxU4M7Rp/TO2McrIJFWhFlDK/SppItRF1+oeuqBFipKnDM4lRuxALTXQCBou2M54DneMcVrSFnuv7B+G4Y1vgPvSc+3Bej0dy/9GL6zr357ru9nqH57nPfWY1TdMUAAAAAASb3e4FAAAAAEC7iWQAAAAAxBPJAAAAAIgnkgEAAAAQTyQDAAAAIJ5IBgAAAEA8kQwAAACAeCIZAAAAAPFEMgAAAADiiWQAAAAAxGs5kn3ve9+rVatW1RlnnFGzZs2q22677RmPufPOO+vVr351dXZ21kte8pL64he/OIGlAsCTzCIApgPzCGBmaTmSHTx4sM4777zaunXrs9r/wQcfrDe96U31+te/vvbs2VMf+MAH6h3veEd9+9vfbnmxAFBlFgEwPZhHADPLrKZpmgkfPGtW3XrrrXXJJZccd58rrriibr/99vrJT34ytu1v//Zv65FHHqmdO3dO9KkBoKrMIgCmB/MI4PnvpMl+gl27dlVfX9+4bStXrqwPfOADxz3m0KFDdejQobE/j46O1m9+85v6oz/6o5o1a9ZkLRVgWmuaph599NE644wzavZsHynZionMoirzCOAPmUXPjf83AjgxJmseTXokGxwcrO7u7nHburu7a2RkpH7729/WKaecctQxmzdvrmuvvXaylwbwvLR///76kz/5k3Yv43llIrOoyjwCOB6zaGL8vxHAiXWi59GkR7KJ2LBhQ/X394/9eXh4uF784hfX/v37a968eW1cGUD7jIyMVE9PT5166qntXkoM8whgPLNo6plFAEebrHk06ZFs4cKFNTQ0NG7b0NBQzZs377jfue/s7KzOzs6jts+bN88gAOL50YrWTWQWVZlHAMdjFk2M/zcCOLFO9Dya9A8SWLFiRQ0MDIzbdscdd9SKFSsm+6kBoKrMIgCmB/MIYHprOZL97//+b+3Zs6f27NlTVU/+GuM9e/bUvn37qurJtwOvWbNmbP93v/vdtXfv3vrgBz9Y9913X33605+ur371q3X55ZefmDMAII5ZBMB0YB4BzCwtR7If/ehHdf7559f5559fVVX9/f11/vnn18aNG6uq6te//vXYUKiq+rM/+7O6/fbb64477qjzzjuvPvnJT9bnPve5Wrly5Qk6BQDSmEUATAfmEcDMMqtpmqbdi3gmIyMj1dXVVcPDw37uHojlXth+XgMgnftg+3kNACbvXjjpn0kGAAAAANOdSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxRDIAAAAA4olkAAAAAMQTyQAAAACIJ5IBAAAAEE8kAwAAACCeSAYAAABAPJEMAAAAgHgiGQAAAADxJhTJtm7dWkuWLKm5c+dWb29v3XXXXU+7/5YtW+plL3tZnXLKKdXT01OXX355/e53v5vQggHgKeYRAO1mFgHMHC1HsltuuaX6+/tr06ZNdffdd9d5551XK1eurIceeuiY+3/lK1+pK6+8sjZt2lT33ntvff7zn69bbrmlPvShDz3nxQOQyzwCoN3MIoCZpeVIduONN9Y73/nOWrduXb3iFa+obdu21Qte8IL6whe+cMz9f/jDH9ZFF11Ul156aS1ZsqTe8IY31Fvf+tZn/A4LADwd8wiAdjOLAGaWliLZ4cOHa/fu3dXX1/f7LzB7dvX19dWuXbuOecyFF15Yu3fvHrvx7927t3bs2FFvfOMbj/s8hw4dqpGRkXEPAHiKeQRAu5lFADPPSa3sfODAgTpy5Eh1d3eP297d3V333XffMY+59NJL68CBA/Xa1762mqapJ554ot797nc/7VuKN2/eXNdee20rSwMgiHkEQLuZRQAzz6T/dss777yzrr/++vr0pz9dd999d33jG9+o22+/va677rrjHrNhw4YaHh4ee+zfv3+ylwnADGceAdBuZhHA9NbSO8nmz59fc+bMqaGhoXHbh4aGauHChcc85pprrqnLLrus3vGOd1RV1bnnnlsHDx6sd73rXXXVVVfV7NlHd7rOzs7q7OxsZWkABDGPAGg3swhg5mnpnWQdHR21bNmyGhgYGNs2OjpaAwMDtWLFimMe89hjjx11s58zZ05VVTVN0+p6AcA8AqDtzCKAmaeld5JVVfX399fatWtr+fLldcEFF9SWLVvq4MGDtW7duqqqWrNmTS1evLg2b95cVVWrVq2qG2+8sc4///zq7e2tBx54oK655ppatWrV2EAAgFaZRwC0m1kEMLO0HMlWr15dDz/8cG3cuLEGBwdr6dKltXPnzrEPrNy3b9+4745cffXVNWvWrLr66qvrV7/6Vf3xH/9xrVq1qj72sY+duLMAII55BEC7mUUAM8us5nnwvt6RkZHq6uqq4eHhmjdvXruXA9AW7oXt5zUA0rkPtp/XAGDy7oWT/tstAQAAAGC6E8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAg3oQi2datW2vJkiU1d+7c6u3trbvuuutp93/kkUdq/fr1tWjRours7KyXvvSltWPHjgktGACeYh4B0G5mEcDMcVKrB9xyyy3V399f27Ztq97e3tqyZUutXLmy7r///lqwYMFR+x8+fLj+6q/+qhYsWFBf//rXa/HixfXLX/6yTjvttBOxfgBCmUcAtJtZBDCzzGqapmnlgN7e3nrNa15TN910U1VVjY6OVk9PT73vfe+rK6+88qj9t23bVv/v//2/uu++++rkk0+e0CJHRkaqq6urhoeHa968eRP6GgDPd+6F45lHAFPPfXA8swigPSbrXtjSj1sePny4du/eXX19fb//ArNnV19fX+3ateuYx3zzm9+sFStW1Pr166u7u7vOOeecuv766+vIkSPHfZ5Dhw7VyMjIuAcAPMU8AqDdzCKAmaelSHbgwIE6cuRIdXd3j9ve3d1dg4ODxzxm79699fWvf72OHDlSO3bsqGuuuaY++clP1kc/+tHjPs/mzZurq6tr7NHT09PKMgGY4cwjANrNLAKYeSb9t1uOjo7WggUL6rOf/WwtW7asVq9eXVdddVVt27btuMds2LChhoeHxx779++f7GUCMMOZRwC0m1kEML219MH98+fPrzlz5tTQ0NC47UNDQ7Vw4cJjHrNo0aI6+eSTa86cOWPbXv7yl9fg4GAdPny4Ojo6jjqms7OzOjs7W1kaAEHMIwDazSwCmHlaeidZR0dHLVu2rAYGBsa2jY6O1sDAQK1YseKYx1x00UX1wAMP1Ojo6Ni2n/3sZ7Vo0aJjDgEAeCbmEQDtZhYBzDwt/7hlf39/3XzzzfWlL32p7r333nrPe95TBw8erHXr1lVV1Zo1a2rDhg1j+7/nPe+p3/zmN/X+97+/fvazn9Xtt99e119/fa1fv/7EnQUAccwjANrNLAKYWVr6ccuqqtWrV9fDDz9cGzdurMHBwVq6dGnt3Llz7AMr9+3bV7Nn/7699fT01Le//e26/PLL61WvelUtXry43v/+99cVV1xx4s4CgDjmEQDtZhYBzCyzmqZp2r2IZzIyMlJdXV01PDxc8+bNa/dyANrCvbD9vAZAOvfB9vMaAEzevXDSf7slAAAAAEx3IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEm1Ak27p1ay1ZsqTmzp1bvb29dddddz2r47Zv316zZs2qSy65ZCJPCwDjmEcATAfmEcDM0HIku+WWW6q/v782bdpUd999d5133nm1cuXKeuihh572uF/84hf1j//4j3XxxRdPeLEA8BTzCIDpwDwCmDlajmQ33nhjvfOd76x169bVK17xitq2bVu94AUvqC984QvHPebIkSP1tre9ra699to688wzn9OCAaDKPAJgejCPAGaOliLZ4cOHa/fu3dXX1/f7LzB7dvX19dWuXbuOe9xHPvKRWrBgQb397W9/Vs9z6NChGhkZGfcAgKeYRwBMB1Mxj8wigKnTUiQ7cOBAHTlypLq7u8dt7+7ursHBwWMe8/3vf78+//nP18033/ysn2fz5s3V1dU19ujp6WllmQDMcOYRANPBVMwjswhg6kzqb7d89NFH67LLLqubb7655s+f/6yP27BhQw0PD4899u/fP4mrBGCmM48AmA4mMo/MIoCpc1IrO8+fP7/mzJlTQ0ND47YPDQ3VwoULj9r/5z//ef3iF7+oVatWjW0bHR198olPOqnuv//+Ouuss446rrOzszo7O1tZGgBBzCMApoOpmEdmEcDUaemdZB0dHbVs2bIaGBgY2zY6OloDAwO1YsWKo/Y/++yz68c//nHt2bNn7PHmN7+5Xv/619eePXu8VRiACTGPAJgOzCOAmaWld5JVVfX399fatWtr+fLldcEFF9SWLVvq4MGDtW7duqqqWrNmTS1evLg2b95cc+fOrXPOOWfc8aeddlpV1VHbAaAV5hEA04F5BDBztBzJVq9eXQ8//HBt3LixBgcHa+nSpbVz586xD6vct29fzZ49qR91BgDmEQDTgnkEMHPMapqmafcinsnIyEh1dXXV8PBwzZs3r93LAWgL98L28xoA6dwH289rADB590Lf0gAAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8SYUybZu3VpLliypuXPnVm9vb911113H3ffmm2+uiy++uE4//fQ6/fTTq6+v72n3B4BnyzwCYDowjwBmhpYj2S233FL9/f21adOmuvvuu+u8886rlStX1kMPPXTM/e+8885661vfWt/97ndr165d1dPTU294wxvqV7/61XNePAC5zCMApgPzCGDmmNU0TdPKAb29vfWa17ymbrrppqqqGh0drZ6ennrf+95XV1555TMef+TIkTr99NPrpptuqjVr1jyr5xwZGamurq4aHh6uefPmtbJcgBnDvXA88whg6rkPHm2q55HXAGDy7oUtvZPs8OHDtXv37urr6/v9F5g9u/r6+mrXrl3P6ms89thj9fjjj9eLXvSi4+5z6NChGhkZGfcAgKeYRwBMB1Mxj8wigKnTUiQ7cOBAHTlypLq7u8dt7+7ursHBwWf1Na644oo644wzxg2SP7R58+bq6uoae/T09LSyTABmOPMIgOlgKuaRWQQwdab0t1vecMMNtX379rr11ltr7ty5x91vw4YNNTw8PPbYv3//FK4SgJnOPAJgOng288gsApg6J7Wy8/z582vOnDk1NDQ0bvvQ0FAtXLjwaY/9xCc+UTfccEN95zvfqVe96lVPu29nZ2d1dna2sjQAgphHAEwHUzGPzCKAqdPSO8k6Ojpq2bJlNTAwMLZtdHS0BgYGasWKFcc97uMf/3hdd911tXPnzlq+fPnEVwsAZR4BMD2YRwAzS0vvJKuq6u/vr7Vr19by5cvrggsuqC1bttTBgwdr3bp1VVW1Zs2aWrx4cW3evLmqqv75n/+5Nm7cWF/5yldqyZIlYz+b/8IXvrBe+MIXnsBTASCJeQTAdGAeAcwcLUey1atX18MPP1wbN26swcHBWrp0ae3cuXPswyr37dtXs2f//g1qn/nMZ+rw4cP1N3/zN+O+zqZNm+rDH/7wc1s9ALHMIwCmA/MIYOaY1TRN0+5FPJORkZHq6uqq4eHhmjdvXruXA9AW7oXt5zUA0rkPtp/XAGDy7oVT+tstAQAAAGA6EskAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAg3oQi2datW2vJkiU1d+7c6u3trbvuuutp9//a175WZ599ds2dO7fOPffc2rFjx4QWCwD/l3kEwHRgHgHMDC1HsltuuaX6+/tr06ZNdffdd9d5551XK1eurIceeuiY+//whz+st771rfX2t7+97rnnnrrkkkvqkksuqZ/85CfPefEA5DKPAJgOzCOAmWNW0zRNKwf09vbWa17zmrrpppuqqmp0dLR6enrqfe97X1155ZVH7b969eo6ePBgfetb3xrb9hd/8Re1dOnS2rZt27N6zpGRkerq6qrh4eGaN29eK8sFmDHcC8czjwCmnvvg0aZ6HnkNACbvXnhSKzsfPny4du/eXRs2bBjbNnv27Orr66tdu3Yd85hdu3ZVf3//uG0rV66s22677bjPc+jQoTp06NDYn4eHh6vqyYsAkOqpe2CL39uYkcwjgPYwi8abinlkFgEcbbLmUUuR7MCBA3XkyJHq7u4et727u7vuu+++Yx4zODh4zP0HBweP+zybN2+ua6+99qjtPT09rSwXYEb67//+7+rq6mr3MtrKPAJoL7PoSVMxj8wigOM70fOopUg2VTZs2DDuuyuPPPJI/emf/mnt27cvchiPjIxUT09P7d+/P/Yt1enXIP38q1yDqie/c/ziF7+4XvSiF7V7KTHMo/H8O3QN0s+/yjUwi6aeWXS09H+H6edf5RpUuQaTNY9aimTz58+vOXPm1NDQ0LjtQ0NDtXDhwmMes3Dhwpb2r6rq7Oyszs7Oo7Z3dXVFvvhPmTdvXvT5V7kG6edf5RpUPfljHOnMo/by79A1SD//KtfALHrSVMwjs+j40v8dpp9/lWtQ5Rqc6HnU0lfr6OioZcuW1cDAwNi20dHRGhgYqBUrVhzzmBUrVozbv6rqjjvuOO7+APBMzCMApgPzCGBmafnHLfv7+2vt2rW1fPnyuuCCC2rLli118ODBWrduXVVVrVmzphYvXlybN2+uqqr3v//99brXva4++clP1pve9Kbavn17/ehHP6rPfvazJ/ZMAIhiHgEwHZhHADNHy5Fs9erV9fDDD9fGjRtrcHCwli5dWjt37hz78Ml9+/aNe7vbhRdeWF/5ylfq6quvrg996EP153/+53XbbbfVOeec86yfs7OzszZt2nTMtxknSD//Ktcg/fyrXIMq1+APmUdTL/38q1yD9POvcg3Sz/9YpnoeeQ1cg/Tzr3INqlyDyTr/WY3f3wwAAABAOJ+4CQAAAEA8kQwAAACAeCIZAAAAAPFEMgAAAADiTZtItnXr1lqyZEnNnTu3ent766677nra/b/2ta/V2WefXXPnzq1zzz23duzYMUUrnRytnP/NN99cF198cZ1++ul1+umnV19f3zNer+eDVv8OPGX79u01a9asuuSSSyZ3gZOs1fN/5JFHav369bVo0aLq7Oysl770pVH/DqqqtmzZUi972cvqlFNOqZ6enrr88svrd7/73RSt9sT63ve+V6tWraozzjijZs2aVbfddtszHnPnnXfWq1/96urs7KyXvOQl9cUvfnHS1znTpc+iKvOoyjwyj8wj86j9zCPzKH0WVZlHVeZRW+ZRMw1s37696ejoaL7whS80//mf/9m8853vbE477bRmaGjomPv/4Ac/aObMmdN8/OMfb3760582V199dXPyySc3P/7xj6d45SdGq+d/6aWXNlu3bm3uueee5t57723+7u/+runq6mr+67/+a4pXfuK0eg2e8uCDDzaLFy9uLr744uav//qvp2axk6DV8z906FCzfPny5o1vfGPz/e9/v3nwwQebO++8s9mzZ88Ur/zEafUafPnLX246OzubL3/5y82DDz7YfPvb324WLVrUXH755VO88hNjx44dzVVXXdV84xvfaKqqufXWW592/7179zYveMELmv7+/uanP/1p86lPfaqZM2dOs3PnzqlZ8AyUPouaxjxqGvPIPDKPzKP2M4/Mo/RZ1DTmUdOYR+2aR9Mikl1wwQXN+vXrx/585MiR5owzzmg2b958zP3f8pa3NG9605vGbevt7W3+/u//flLXOVlaPf8/9MQTTzSnnnpq86UvfWmyljjpJnINnnjiiebCCy9sPve5zzVr1659Xg+CVs//M5/5THPmmWc2hw8fnqolTrpWr8H69eubv/zLvxy3rb+/v7nooosmdZ1T4dkMgQ9+8IPNK1/5ynHbVq9e3axcuXISVzazpc+ipjGPmsY8Mo/Mo//LPGoP88g8Sp9FTWMeNY159H9N5Txq+49bHj58uHbv3l19fX1j22bPnl19fX21a9euYx6za9eucftXVa1cufK4+09nEzn/P/TYY4/V448/Xi960Ysma5mTaqLX4CMf+UgtWLCg3v72t0/FMifNRM7/m9/8Zq1YsaLWr19f3d3ddc4559T1119fR44cmapln1ATuQYXXnhh7d69e+wtx3v37q0dO3bUG9/4xilZc7vNpPvgdJA+i6rMoyrzyDwyjyZipt0L2808Mo/SZ1GVeVRlHk3EiboXnnQiFzURBw4cqCNHjlR3d/e47d3d3XXfffcd85jBwcFj7j84ODhp65wsEzn/P3TFFVfUGWeccdRfiOeLiVyD73//+/X5z3++9uzZMwUrnFwTOf+9e/fWv//7v9fb3va22rFjRz3wwAP13ve+tx5//PHatGnTVCz7hJrINbj00kvrwIED9drXvraapqknnnii3v3ud9eHPvShqVhy2x3vPjgyMlK//e1v65RTTmnTyp6f0mdRlXlUZR6ZR+bRRJhHJ5Z5ZB6lz6Iq86jKPJqIEzWP2v5OMp6bG264obZv31633nprzZ07t93LmRKPPvpoXXbZZXXzzTfX/Pnz272cthgdHa0FCxbUZz/72Vq2bFmtXr26rrrqqtq2bVu7lzZl7rzzzrr++uvr05/+dN199931jW98o26//fa67rrr2r00iGQemUfmkXkE00HaPDKLnmQemUcnStvfSTZ//vyaM2dODQ0Njds+NDRUCxcuPOYxCxcubGn/6Wwi5/+UT3ziE3XDDTfUd77znXrVq141mcucVK1eg5///Of1i1/8olatWjW2bXR0tKqqTjrppLr//vvrrLPOmtxFn0AT+TuwaNGiOvnkk2vOnDlj217+8pfX4OBgHT58uDo6OiZ1zSfaRK7BNddcU5dddlm94x3vqKqqc889tw4ePFjvete76qqrrqrZs2f29wCOdx+cN2+e79pPQPosqjKPqswj88g8mgjz6MQyj8yj9FlUZR5VmUcTcaLmUduvUkdHRy1btqwGBgbGto2OjtbAwECtWLHimMesWLFi3P5VVXfcccdx95/OJnL+VVUf//jH67rrrqudO3fW8uXLp2Kpk6bVa3D22WfXj3/849qzZ8/Y481vfnO9/vWvrz179lRPT89ULv85m8jfgYsuuqgeeOCBsQFYVfWzn/2sFi1a9LwbAFUTuwaPPfbYUTf6p4bik5/tOLPNpPvgdJA+i6rMoyrzyDwyjyZipt0L2808Mo/SZ1GVeVRlHk3ECbsXtvQx/5Nk+/btTWdnZ/PFL36x+elPf9q8613vak477bRmcHCwaZqmueyyy5orr7xybP8f/OAHzUknndR84hOfaO69995m06ZNz+tfc9zq+d9www1NR0dH8/Wvf7359a9/PfZ49NFH23UKz1mr1+APPd9/g0ur579v377m1FNPbf7hH/6huf/++5tvfetbzYIFC5qPfvSj7TqF56zVa7Bp06bm1FNPbf71X/+12bt3b/Nv//ZvzVlnndW85S1vadcpPCePPvpoc8899zT33HNPU1XNjTfe2Nxzzz3NL3/5y6ZpmubKK69sLrvssrH9n/oVx//0T//U3Hvvvc3WrVsn9CuO+b30WdQ05lHTmEfmkXlkHrWfeWQepc+ipjGPmsY8atc8mhaRrGma5lOf+lTz4he/uOno6GguuOCC5j/+4z/G/tvrXve6Zu3ateP2/+pXv9q89KUvbTo6OppXvvKVze233z7FKz6xWjn/P/3TP22q6qjHpk2bpn7hJ1Crfwf+r5kwCFo9/x/+8IdNb29v09nZ2Zx55pnNxz72seaJJ56Y4lWfWK1cg8cff7z58Ic/3Jx11lnN3Llzm56enua9731v8z//8z9Tv/AT4Lvf/e4x/10/dc5r165tXve61x11zNKlS5uOjo7mzDPPbP7lX/5lytc906TPoqYxj5rGPDKPzCPzqP3MI/MofRY1jXnUNOZRO+bRrKYJeN8dAAAAADyNtn8mGQAAAAC0m0gGAAAAQDyRDAAAAIB4IhkAAAAA8UQyAAAAAOKJZAAAAADEE8kAAAAAiCeSAQAAABBPJAMAAAAgnkgGAAAAQDyRDAAAAIB4IhkAAAAA8f4/SAnJLYnwH/4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1500x1500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def plot_images_grid(image_folder):\n",
        "    \"\"\"Plots images from a folder in a grid format with smaller size.\"\"\"\n",
        "\n",
        "    # Filter for valid image files\n",
        "    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(valid_extensions) and os.path.isfile(os.path.join(image_folder, f))]\n",
        "\n",
        "    num_images = len(image_files)\n",
        "    grid_size = (num_images // 8 + 1, 8) if num_images > 8 else (1, num_images)\n",
        "    fig, axs = plt.subplots(grid_size[0], grid_size[1], figsize=(15, 15))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        if i >= grid_size[0] * grid_size[1]:\n",
        "            break\n",
        "\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        try:\n",
        "            # Verify the image format and resize\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()  # Verify that it is, in fact, an image\n",
        "                img = Image.open(image_path)  # Resize image to 150x150 pixels\n",
        "                img = np.array(img)\n",
        "                row = i // grid_size[1]\n",
        "                col = i % grid_size[1]\n",
        "                axs[row, col].imshow(img)\n",
        "                axs[row, col].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {image_file}: {e}\")\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, grid_size[0] * grid_size[1]):\n",
        "        row = j // grid_size[1]\n",
        "        col = j % grid_size[1]\n",
        "        axs[row, col].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_images_grid('images3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5sblqGSnf7N"
      },
      "outputs": [],
      "source": [
        "json_objs =[]\n",
        "json_list = []\n",
        "image_dicts = []\n",
        "for pdf in os.listdir(\"LLM_papers\"):\n",
        "  if pdf.endswith(\".pdf\"):\n",
        "    parser = LlamaParse(verbose=True)\n",
        "    json_objs.extend(parser.get_json_result(pdf))\n",
        "    json_list.extend(json_objs[0][\"pages\"])\n",
        "    image_dicts.extend(parser.get_images(\n",
        "    json_objs,\n",
        "    download_path=\"images\"\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ76nNqSC8m_",
        "outputId": "de72938a-98e6-4d32-fe1b-727420b56662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started parsing the file under job_id e4a03116-400d-4157-a1c8-10a245b511ce\n",
            "Started parsing the file under job_id 4050a6ee-9b71-41e9-b4f3-02513f782ced\n",
            "Started parsing the file under job_id df2323de-fc17-479b-9741-f339b071bae9\n",
            "Started parsing the file under job_id 6e989bc0-7488-4796-82b4-33672f496920\n",
            "Started parsing the file under job_id a366c1e5-2515-40a6-b1f0-f4e64baf9595\n"
          ]
        }
      ],
      "source": [
        "file_extractor = {\".pdf\": parser}\n",
        "documents = SimpleDirectoryReader(\"LLM_papers\", file_extractor=file_extractor).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnlfSXIwI_hK",
        "outputId": "5dfe1d2d-d7ca-4497-ec81-908707d47e3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(id_='http://paulgraham.com/worked.html', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='![](https://s.turbifycdn.com/aah/paulgraham/essays-5.gif)|\\n![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif)|\\n[![](https://s.turbifycdn.com/aah/paulgraham/essays-6.gif)](index.html)  \\n  \\n| ![What I Worked On](https://s.turbifycdn.com/aah/paulgraham/what-i-worked-\\non-4.gif)  \\n  \\nFebruary 2021  \\n  \\nBefore college the two main things I worked on, outside of school, were\\nwriting and programming. I didn\\'t write essays. I wrote what beginning writers\\nwere supposed to write then, and probably still are: short stories. My stories\\nwere awful. They had hardly any plot, just characters with strong feelings,\\nwhich I imagined made them deep.  \\n  \\nThe first programs I tried writing were on the IBM 1401 that our school\\ndistrict used for what was then called \"data processing.\" This was in 9th\\ngrade, so I was 13 or 14. The school district\\'s 1401 happened to be in the\\nbasement of our junior high school, and my friend Rich Draves and I got\\npermission to use it. It was like a mini Bond villain\\'s lair down there, with\\nall these alien-looking machines \\x97 CPU, disk drives, printer, card reader \\x97\\nsitting up on a raised floor under bright fluorescent lights.  \\n  \\nThe language we used was an early version of Fortran. You had to type programs\\non punch cards, then stack them in the card reader and press a button to load\\nthe program into memory and run it. The result would ordinarily be to print\\nsomething on the spectacularly loud printer.  \\n  \\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in\\nretrospect there\\'s not much I could have done with it. The only form of input\\nto programs was data stored on punched cards, and I didn\\'t have any data\\nstored on punched cards. The only other option was to do things that didn\\'t\\nrely on any input, like calculate approximations of pi, but I didn\\'t know\\nenough math to do anything interesting of that type. So I\\'m not surprised I\\ncan\\'t remember any programs I wrote, because they can\\'t have done much. My\\nclearest memory is of the moment I learned it was possible for programs not to\\nterminate, when one of mine didn\\'t. On a machine without time-sharing, this\\nwas a social as well as a technical error, as the data center manager\\'s\\nexpression made clear.  \\n  \\nWith microcomputers, everything changed. Now you could have a computer sitting\\nright in front of you, on a desk, that could respond to your keystrokes as it\\nwas running instead of just churning through a stack of punch cards and then\\nstopping. [1]  \\n  \\nThe first of my friends to get a microcomputer built it himself. It was sold\\nas a kit by Heathkit. I remember vividly how impressed and envious I felt\\nwatching him sitting in front of it, typing programs right into the computer.  \\n  \\nComputers were expensive in those days and it took me years of nagging before\\nI convinced my father to buy one, a TRS-80, in about 1980\\\\. The gold standard\\nthen was the Apple II, but a TRS-80 was good enough. This was when I really\\nstarted programming. I wrote simple games, a program to predict how high my\\nmodel rockets would fly, and a word processor that my father used to write at\\nleast one book. There was only room in memory for about 2 pages of text, so\\nhe\\'d write 2 pages at a time and then print them out, but it was a lot better\\nthan a typewriter.  \\n  \\nThough I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_. In other words, like many a grad student, I was working energetically\\non multiple projects that were not my thesis.  \\n  \\nI didn\\'t see a way out of this situation. I didn\\'t want to drop out of grad\\nschool, but how else was I going to get out? I remember when my friend Robert\\nMorris got kicked out of Cornell for writing the internet worm of 1988, I was\\nenvious that he\\'d found such a spectacular way to get out of grad school.  \\n  \\nThen one day in April 1990 a crack appeared in the wall. I ran into professor\\nCheatham and he asked if I was far enough along to graduate that June. I\\ndidn\\'t have a word of my dissertation written, but in what must have been the\\nquickest bit of thinking in my life, I decided to take a shot at writing one\\nin the 5 weeks or so that remained before the deadline, reusing parts of _On\\nLisp_ where I could, and I was able to respond, with no perceptible delay\\n\"Yes, I think so. I\\'ll give you something to read in a few days.\"  \\n  \\nI picked applications of continuations as the topic. In retrospect I should\\nhave written about macros and embedded languages. There\\'s a whole world there\\nthat\\'s barely been explored. But all I wanted was to get out of grad school,\\nand my rapidly written dissertation sufficed, just barely.  \\n  \\nMeanwhile I was applying to art schools. I applied to two: RISD in the US, and\\nthe Accademia di Belli Arti in Florence, which, because it was the oldest art\\nschool, I imagined would be good. RISD accepted me, and I never heard back\\nfrom the Accademia, so off to Providence I went.  \\n  \\nI\\'d applied for the BFA program at RISD, which meant in effect that I had to\\ngo to college again. This was not as strange as it sounds, because I was only\\n25, and art schools are full of people of different ages. RISD counted me as a\\ntransfer sophomore and said I had to do the foundation that summer. The\\nfoundation means the classes that everyone has to take in fundamental subjects\\nlike drawing, color, and design.  \\n  \\nToward the end of the summer I got a big surprise: a letter from the\\nAccademia, which had been delayed because they\\'d sent it to Cambridge England\\ninstead of Cambridge Massachusetts, inviting me to take the entrance exam in\\nFlorence that fall. This was now only weeks away. My nice landlady let me\\nleave my stuff in her attic. I had some money saved from consulting work I\\'d\\ndone in grad school; there was probably enough to last a year if I lived\\ncheaply. Now all I had to do was learn Italian.  \\n  \\nOnly _stranieri_ (foreigners) had to take this entrance exam. In retrospect it\\nmay well have been a way of excluding them, because there were so many\\n_stranieri_ attracted by the idea of studying art in Florence that the Italian\\nstudents would otherwise have been outnumbered. I was in decent shape at\\npainting and drawing from the RISD foundation that summer, but I still don\\'t\\nknow how I managed to pass the written exam. I remember that I answered the\\nessay question by writing about Cezanne, and that I cranked up the\\nintellectual level as high as I could to make the most of my limited\\nvocabulary. [2]  \\n  \\nI\\'m only up to age 25 and already there are such conspicuous patterns. Here I\\nwas, yet again about to attend some august institution in the hopes of\\nlearning about some prestigious subject, and yet again about to be\\ndisappointed. The students and faculty in the painting department at the\\nAccademia were the nicest people you could imagine, but they had long since\\narrived at an arrangement whereby the students wouldn\\'t require the faculty to\\nteach anything, and in return the faculty wouldn\\'t require the students to\\nlearn anything. And at the same time all involved would adhere outwardly to\\nthe conventions of a 19th century atelier. We actually had one of those little\\nstoves, fed with kindling, that you see in 19th century studio paintings, and\\na nude model sitting as close to it as possible without getting burned. Except\\nhardly anyone else painted her besides me. The rest of the students spent\\ntheir time chatting or occasionally trying to imitate things they\\'d seen in\\nAmerican art magazines.  \\n  \\nOur model turned out to live just down the street from me. She made a living\\nfrom a combination of modelling and making fakes for a local antique dealer.\\nShe\\'d copy an obscure old painting out of a book, and then he\\'d take the copy\\nand maltreat it to make it look old. [3]  \\n  \\nWhile I was a student at the Accademia I started painting still lives in my\\nbedroom at night. These paintings were tiny, because the room was, and because\\nI painted them on leftover scraps of canvas, which was all I could afford at\\nthe time. Painting still lives is different from painting people, because the\\nsubject, as its name suggests, can\\'t move. People can\\'t sit for more than\\nabout 15 minutes at a time, and when they do they don\\'t sit very still. So the\\ntraditional m.o. for painting people is to know how to paint a generic person,\\nwhich you then modify to match the specific person you\\'re painting. Whereas a\\nstill life you can, if you want, copy pixel by pixel from what you\\'re seeing.\\nYou don\\'t want to stop there, of course, or you get merely photographic\\naccuracy, and what makes a still life interesting is that it\\'s been through a\\nhead. You want to emphasize the visual cues that tell you, for example, that\\nthe reason the color changes suddenly at a certain point is that it\\'s the edge\\nof an object. By subtly emphasizing such things you can make paintings that\\nare more realistic than photographs not just in some metaphorical sense, but\\nin the strict information-theoretic sense. [4]  \\n  \\nI liked painting still lives because I was curious about what I was seeing. In\\neveryday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual\\nperception is handled by low-level processes that merely tell your brain\\n\"that\\'s a water droplet\" without telling you details like where the lightest\\nand darkest points are, or \"that\\'s a bush\" without telling you the shape and\\nposition of every leaf. This is a feature of brains, not a bug. In everyday\\nlife it would be distracting to notice every leaf on every bush. But when you\\nhave to paint something, you have to look more closely, and when you do\\nthere\\'s a lot to see. You can still be noticing new things after days of\\ntrying to paint something people usually take for granted, just as you can\\nafter days of trying to write an essay about something people usually take for\\ngranted.  \\n  \\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to\\npaint. But it seemed a good enough bet to be worth trying.  \\n  \\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and\\ngave me a good grade, which he wrote down in a sort of passport each student\\nhad. But the Accademia wasn\\'t teaching me anything except Italian, and my\\nmoney was running out, so at the end of the first year I went back to the US.  \\n  \\nI wanted to go back to RISD, but I was now broke and RISD was very expensive,\\nso I decided to get a job for a year and then return to RISD the next fall. I\\ngot one at a company called Interleaf, which made software for creating\\ndocuments. You mean like Microsoft Word? Exactly. That was how I learned that\\nlow end software tends to eat high end software. But Interleaf still had a few\\nyears to live yet. [5]  \\n  \\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a\\nscripting language, and even made the scripting language a dialect of Lisp.\\nNow they wanted a Lisp hacker to write things in it. This was the closest\\nthing I\\'ve had to a normal job, and I hereby apologize to my boss and\\ncoworkers, because I was a bad employee. Their Lisp was the thinnest icing on\\na giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never\\nunderstood most of the software. Plus I was terribly irresponsible. This was\\nback when a programming job meant showing up every day during certain working\\nhours. That seemed unnatural to me, and on this point the rest of the world is\\ncoming around to my way of thinking, but at the time it caused a lot of\\nfriction. Toward the end of the year I spent much of my time surreptitiously\\nworking on _On Lisp_ , which I had by this time gotten a contract to publish.  \\n  \\nThe good part was that I got paid huge amounts of money, especially by art\\nstudent standards. In Florence, after paying my part of the rent, my budget\\nfor everything else had been $7 a day. Now I was getting paid more than 4\\ntimes that every hour, even when I was just sitting in a meeting. By living\\ncheaply I not only managed to save enough to go back to RISD, but also paid\\noff my college loans.  \\n  \\nI learned some useful things at Interleaf, though they were mostly about what\\nnot to do. I learned that it\\'s better for technology companies to be run by\\nproduct people than sales people (though sales is a real skill and people who\\nare good at it are really good at it), that it leads to bugs when code is\\nedited by too many people, that cheap office space is no bargain if it\\'s\\ndepressing, that planned meetings are inferior to corridor conversations, that\\nbig, bureaucratic customers are a dangerous source of money, and that there\\'s\\nnot much overlap between conventional office hours and the optimal time for\\nhacking, or conventional offices and the optimal place for it.  \\n  \\nBut the most important thing I learned, and which I used in both Viaweb and Y\\nCombinator, is that the low end eats the high end: that it\\'s good to be the\\n\"entry level\" option, even though that will be less prestigious, because if\\nyou\\'re not, someone else will be, and will squash you against the ceiling.\\nWhich in turn means that prestige is a danger sign.  \\n  \\nWhen I left to go back to RISD the next fall, I arranged to do freelance work\\nfor the group that did projects for customers, and this was how I survived for\\nthe next several years. When I came back to visit for a project later on,\\nsomeone told me about a new thing called HTML, which was, as he described it,\\na derivative of SGML. Markup language enthusiasts were an occupational hazard\\nat Interleaf and I ignored him, but this HTML thing later became a big part of\\nmy life.  \\n  \\nIn the fall of 1992 I moved back to Providence to continue at RISD. The\\nfoundation had merely been intro stuff, and the Accademia had been a (very\\ncivilized) joke. Now I was going to see what real art school was like. But\\nalas it was more like the Accademia than not. Better organized, certainly, and\\na lot more expensive, but it was now becoming clear that art school did not\\nbear the same relationship to art that medical school bore to medicine. At\\nleast not the painting department. The textile department, which my next door\\nneighbor belonged to, seemed to be pretty rigorous. No doubt illustration and\\narchitecture were too. But painting was post-rigorous. Painting students were\\nsupposed to express themselves, which to the more worldly ones meant to try to\\ncook up some sort of distinctive signature style.  \\n  \\nA signature style is the visual equivalent of what in show business is known\\nas a \"schtick\": something that immediately identifies the work as yours and no\\none else\\'s. For example, when you see a painting that looks like a certain\\nkind of cartoon, you know it\\'s by Roy Lichtenstein. So if you see a big\\npainting of this type hanging in the apartment of a hedge fund manager, you\\nknow he paid millions of dollars for it. That\\'s not always why artists have a\\nsignature style, but it\\'s usually why buyers pay a lot for such work. [6]  \\n  \\nThere were plenty of earnest students too: kids who \"could draw\" in high\\nschool, and now had come to what was supposed to be the best art school in the\\ncountry, to learn to draw even better. They tended to be confused and\\ndemoralized by what they found at RISD, but they kept going, because painting\\nwas what they did. I was not one of the kids who could draw in high school,\\nbut at RISD I was definitely closer to their tribe than the tribe of signature\\nstyle seekers.  \\n  \\nI learned a lot in the color class I took at RISD, but otherwise I was\\nbasically teaching myself to paint, and I could do that for free. So in 1993 I\\ndropped out. I hung around Providence for a bit, and then my college friend\\nNancy Parmet did me a big favor. A rent-controlled apartment in a building her\\nmother owned in New York was becoming vacant. Did I want it? It wasn\\'t much\\nmore than my current place, and New York was supposed to be where the artists\\nwere. So yes, I wanted it! [7]  \\n  \\nAsterix comics begin by zooming in on a tiny corner of Roman Gaul that turns\\nout not to be controlled by the Romans. You can do something similar on a map\\nof New York City: if you zoom in on the Upper East Side, there\\'s a tiny corner\\nthat\\'s not rich, or at least wasn\\'t in 1993. It\\'s called Yorkville, and that\\nwas my new home. Now I was a New York artist \\x97 in the strictly technical sense\\nof making paintings and living in New York.  \\n  \\nI was nervous about money, because I could sense that Interleaf was on the way\\ndown. Freelance Lisp hacking work was very rare, and I didn\\'t want to have to\\nprogram in another language, which in those days would have meant C++ if I was\\nlucky. So with my unerring nose for financial opportunity, I decided to write\\nanother book on Lisp. This would be a popular book, the sort of book that\\ncould be used as a textbook. I imagined myself living frugally off the\\nroyalties and spending all my time painting. (The painting on the cover of\\nthis book, _ANSI Common Lisp_ , is one that I painted around this time.)  \\n  \\nThe best thing about New York for me was the presence of Idelle and Julian\\nWeber. Idelle Weber was a painter, one of the early photorealists, and I\\'d\\ntaken her painting class at Harvard. I\\'ve never known a teacher more beloved\\nby her students. Large numbers of former students kept in touch with her,\\nincluding me. After I moved to New York I became her de facto studio\\nassistant.  \\n  \\nShe liked to paint on big, square canvases, 4 to 5 feet on a side. One day in\\nlate 1994 as I was stretching one of these monsters there was something on the\\nradio about a famous fund manager. He wasn\\'t that much older than me, and was\\nsuper rich. The thought suddenly occurred to me: why don\\'t I become rich? Then\\nI\\'ll be able to work on whatever I want.  \\n  \\nMeanwhile I\\'d been hearing more and more about this new thing called the World\\nWide Web. Robert Morris showed it to me when I visited him in Cambridge, where\\nhe was now in grad school at Harvard. It seemed to me that the web would be a\\nbig deal. I\\'d seen what graphical user interfaces had done for the popularity\\nof microcomputers. It seemed like the web would do the same for the internet.  \\n  \\nIf I wanted to get rich, here was the next train leaving the station. I was\\nright about that part. What I got wrong was the idea. I decided we should\\nstart a company to put art galleries online. I can\\'t honestly say, after\\nreading so many Y Combinator applications, that this was the worst startup\\nidea ever, but it was up there. Art galleries didn\\'t want to be online, and\\nstill don\\'t, not the fancy ones. That\\'s not how they sell. I wrote some\\nsoftware to generate web sites for galleries, and Robert wrote some to resize\\nimages and set up an http server to serve the pages. Then we tried to sign up\\ngalleries. To call this a difficult sale would be an understatement. It was\\ndifficult to give away. A few galleries let us make sites for them for free,\\nbut none paid us.  \\n  \\nThen some online stores started to appear, and I realized that except for the\\norder buttons they were identical to the sites we\\'d been generating for\\ngalleries. This impressive-sounding thing called an \"internet storefront\" was\\nsomething we already knew how to build.  \\n  \\nSo in the summer of 1995, after I submitted the camera-ready copy of _ANSI\\nCommon Lisp_ to the publishers, we started trying to write software to build\\nonline stores. At first this was going to be normal desktop software, which in\\nthose days meant Windows software. That was an alarming prospect, because\\nneither of us knew how to write Windows software or wanted to learn. We lived\\nin the Unix world. But we decided we\\'d at least try writing a prototype store\\nbuilder on Unix. Robert wrote a shopping cart, and I wrote a new site\\ngenerator for stores \\x97 in Lisp, of course.  \\n  \\nWe were working out of Robert\\'s apartment in Cambridge. His roommate was away\\nfor big chunks of time, during which I got to sleep in his room. For some\\nreason there was no bed frame or sheets, just a mattress on the floor. One\\nmorning as I was lying on this mattress I had an idea that made me sit up like\\na capital L. What if we ran the software on the server, and let users control\\nit by clicking on links? Then we\\'d never have to write anything to run on\\nusers\\' computers. We could generate the sites on the same server we\\'d serve\\nthem from. Users wouldn\\'t need anything more than a browser.  \\n  \\nThis kind of software, known as a web app, is common now, but at the time it\\nwasn\\'t clear that it was even possible. To find out, we decided to try making\\na version of our store builder that you could control through the browser. A\\ncouple days later, on August 12, we had one that worked. The UI was horrible,\\nbut it proved you could build a whole store through the browser, without any\\nclient software or typing anything into the command line on the server.  \\n  \\nNow we felt like we were really onto something. I had visions of a whole new\\ngeneration of software working this way. You wouldn\\'t need versions, or ports,\\nor any of that crap. At Interleaf there had been a whole group called Release\\nEngineering that seemed to be at least as big as the group that actually wrote\\nthe software. Now you could just update the software right on the server.  \\n  \\nWe started a new company we called Viaweb, after the fact that our software\\nworked via the web, and we got $10,000 in seed funding from Idelle\\'s husband\\nJulian. In return for that and doing the initial legal work and giving us\\nbusiness advice, we gave him 10% of the company. Ten years later this deal\\nbecame the model for Y Combinator\\'s. We knew founders needed something like\\nthis, because we\\'d needed it ourselves.  \\n  \\nAt this stage I had a negative net worth, because the thousand dollars or so I\\nhad in the bank was more than counterbalanced by what I owed the government in\\ntaxes. (Had I diligently set aside the proper proportion of the money I\\'d made\\nconsulting for Interleaf? No, I had not.) So although Robert had his graduate\\nstudent stipend, I needed that seed funding to live on.  \\n  \\nWe originally hoped to launch in September, but we got more ambitious about\\nthe software as we worked on it. Eventually we managed to build a WYSIWYG site\\nbuilder, in the sense that as you were creating pages, they looked exactly\\nlike the static ones that would be generated later, except that instead of\\nleading to static pages, the links all referred to closures stored in a hash\\ntable on the server.  \\n  \\nIt helped to have studied art, because the main goal of an online store\\nbuilder is to make users look legit, and the key to looking legit is high\\nproduction values. If you get page layouts and fonts and colors right, you can\\nmake a guy running a store out of his bedroom look more legit than a big\\ncompany.  \\n  \\n(If you\\'re curious why my site looks so old-fashioned, it\\'s because it\\'s still\\nmade with this software. It may look clunky today, but in 1996 it was the last\\nword in slick.)  \\n  \\nIn September, Robert rebelled. \"We\\'ve been working on this for a month,\" he\\nsaid, \"and it\\'s still not done.\" This is funny in retrospect, because he would\\nstill be working on it almost 3 years later. But I decided it might be prudent\\nto recruit more programmers, and I asked Robert who else in grad school with\\nhim was really good. He recommended Trevor Blackwell, which surprised me at\\nfirst, because at that point I knew Trevor mainly for his plan to reduce\\neverything in his life to a stack of notecards, which he carried around with\\nhim. But Rtm was right, as usual. Trevor turned out to be a frighteningly\\neffective hacker.  \\n  \\nIt was a lot of fun working with Robert and Trevor. They\\'re the two most\\n[_independent-minded_](think.html) people I know, and in completely different\\nways. If you could see inside Rtm\\'s brain it would look like a colonial New\\nEngland church, and if you could see inside Trevor\\'s it would look like the\\nworst excesses of Austrian Rococo.  \\n  \\nWe opened for business, with 6 stores, in January 1996. It was just as well we\\nwaited a few months, because although we worried we were late, we were\\nactually almost fatally early. There was a lot of talk in the press then about\\necommerce, but not many people actually wanted online stores. [8]  \\n  \\nThere were three main parts to the software: the editor, which people used to\\nbuild sites and which I wrote, the shopping cart, which Robert wrote, and the\\nmanager, which kept track of orders and statistics, and which Trevor wrote. In\\nits time, the editor was one of the best general-purpose site builders. I kept\\nthe code tight and didn\\'t have to integrate with any other software except\\nRobert\\'s and Trevor\\'s, so it was quite fun to work on. If all I\\'d had to do\\nwas work on this software, the next 3 years would have been the easiest of my\\nlife. Unfortunately I had to do a lot more, all of it stuff I was worse at\\nthan programming, and the next 3 years were instead the most stressful.  \\n  \\nThere were a lot of startups making ecommerce software in the second half of\\nthe 90s. We were determined to be the Microsoft Word, not the Interleaf. Which\\nmeant being easy to use and inexpensive. It was lucky for us that we were\\npoor, because that caused us to make Viaweb even more inexpensive than we\\nrealized. We charged $100 a month for a small store and $300 a month for a big\\none. This low price was a big attraction, and a constant thorn in the sides of\\ncompetitors, but it wasn\\'t because of some clever insight that we set the\\nprice low. We had no idea what businesses paid for things. $300 a month seemed\\nlike a lot of money to us.  \\n  \\nWe did a lot of things right by accident like that. For example, we did what\\'s\\nnow called \"doing things that [_don\\'t scale_](ds.html),\" although at the time\\nwe would have described it as \"being so lame that we\\'re driven to the most\\ndesperate measures to get users.\" The most common of which was building stores\\nfor them. This seemed particularly humiliating, since the whole raison d\\'etre\\nof our software was that people could use it to make their own stores. But\\nanything to get users.  \\n  \\nWe learned a lot more about retail than we wanted to know. For example, that\\nif you could only have a small image of a man\\'s shirt (and all images were\\nsmall then by present standards), it was better to have a closeup of the\\ncollar than a picture of the whole shirt. The reason I remember learning this\\nwas that it meant I had to rescan about 30 images of men\\'s shirts. My first\\nset of scans were so beautiful too.  \\n  \\nThough this felt wrong, it was exactly the right thing to be doing. Building\\nstores for users taught us about retail, and about how it felt to use our\\nsoftware. I was initially both mystified and repelled by \"business\" and\\nthought we needed a \"business person\" to be in charge of it, but once we\\nstarted to get users, I was converted, in much the same way I was converted to\\n[_fatherhood_](kids.html) once I had kids. Whatever users wanted, I was all\\ntheirs. Maybe one day we\\'d have so many users that I couldn\\'t scan their\\nimages for them, but in the meantime there was nothing more important to do.  \\n  \\nAnother thing I didn\\'t get at the time is that [_growth rate_](growth.html) is\\nthe ultimate test of a startup. Our growth rate was fine. We had about 70\\nstores at the end of 1996 and about 500 at the end of 1997. I mistakenly\\nthought the thing that mattered was the absolute number of users. And that is\\nthe thing that matters in the sense that that\\'s how much money you\\'re making,\\nand if you\\'re not making enough, you might go out of business. But in the long\\nterm the growth rate takes care of the absolute number. If we\\'d been a startup\\nI was advising at Y Combinator, I would have said: Stop being so stressed out,\\nbecause you\\'re doing fine. You\\'re growing 7x a year. Just don\\'t hire too many\\nmore people and you\\'ll soon be profitable, and then you\\'ll control your own\\ndestiny.  \\n  \\nAlas I hired lots more people, partly because our investors wanted me to, and\\npartly because that\\'s what startups did during the Internet Bubble. A company\\nwith just a handful of employees would have seemed amateurish. So we didn\\'t\\nreach breakeven until about when Yahoo bought us in the summer of 1998. Which\\nin turn meant we were at the mercy of investors for the entire life of the\\ncompany. And since both we and our investors were noobs at startups, the\\nresult was a mess even by startup standards.  \\n  \\nIt was a huge relief when Yahoo bought us. In principle our Viaweb stock was\\nvaluable. It was a share in a business that was profitable and growing\\nrapidly. But it didn\\'t feel very valuable to me; I had no idea how to value a\\nbusiness, but I was all too keenly aware of the near-death experiences we\\nseemed to have every few months. Nor had I changed my grad student lifestyle\\nsignificantly since we started. So when Yahoo bought us it felt like going\\nfrom rags to riches. Since we were going to California, I bought a car, a\\nyellow 1998 VW GTI. I remember thinking that its leather seats alone were by\\nfar the most luxurious thing I owned.  \\n  \\nThe next year, from the summer of 1998 to the summer of 1999, must have been\\nthe least productive of my life. I didn\\'t realize it at the time, but I was\\nworn out from the effort and stress of running Viaweb. For a while after I got\\nto California I tried to continue my usual m.o. of programming till 3 in the\\nmorning, but fatigue combined with Yahoo\\'s prematurely aged\\n[_culture_](yahoo.html) and grim cube farm in Santa Clara gradually dragged me\\ndown. After a few months it felt disconcertingly like working at Interleaf.  \\n  \\nYahoo had given us a lot of options when they bought us. At the time I thought\\nYahoo was so overvalued that they\\'d never be worth anything, but to my\\nastonishment the stock went up 5x in the next year. I hung on till the first\\nchunk of options vested, then in the summer of 1999 I left. It had been so\\nlong since I\\'d painted anything that I\\'d half forgotten why I was doing this.\\nMy brain had been entirely full of software and men\\'s shirts for 4 years. But\\nI had done this to get rich so I could paint, I reminded myself, and now I was\\nrich, so I should go paint.  \\n  \\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me\\nabout my plans. I told him all about the kinds of pictures I wanted to paint.\\nAt the time I was touched that he took such an interest in me. Now I realize\\nit was because he thought I was lying. My options at that point were worth\\nabout $2 million a month. If I was leaving that kind of money on the table, it\\ncould only be to go and start some new startup, and if I did, I might take\\npeople with me. This was the height of the Internet Bubble, and Yahoo was\\nground zero of it. My boss was at that moment a billionaire. Leaving then to\\nstart a new startup must have seemed to him an insanely, and yet also\\nplausibly, ambitious plan.  \\n  \\nBut I really was quitting to paint, and I started immediately. There was no\\ntime to lose. I\\'d already burned 4 years getting rich. Now when I talk to\\nfounders who are leaving after selling their companies, my advice is always\\nthe same: take a vacation. That\\'s what I should have done, just gone off\\nsomewhere and done nothing for a month or two, but the idea never occurred to\\nme.  \\n  \\nSo I tried to paint, but I just didn\\'t seem to have any energy or ambition.\\nPart of the problem was that I didn\\'t know many people in California. I\\'d\\ncompounded this problem by buying a house up in the Santa Cruz Mountains, with\\na beautiful view but miles from anywhere. I stuck it out for a few more\\nmonths, then in desperation I went back to New York, where unless you\\nunderstand about rent control you\\'ll be surprised to hear I still had my\\napartment, sealed up like a tomb of my old life. Idelle was in New York at\\nleast, and there were other people trying to paint there, even though I didn\\'t\\nknow any of them.  \\n  \\nWhen I got back to New York I resumed my old life, except now I was rich. It\\nwas as weird as it sounds. I resumed all my old patterns, except now there\\nwere doors where there hadn\\'t been. Now when I was tired of walking, all I had\\nto do was raise my hand, and (unless it was raining) a taxi would stop to pick\\nme up. Now when I walked past charming little restaurants I could go in and\\norder lunch. It was exciting for a while. Painting started to go better. I\\nexperimented with a new kind of still life where I\\'d paint one painting in the\\nold way, then photograph it and print it, blown up, on canvas, and then use\\nthat as the underpainting for a second still life, painted from the same\\nobjects (which hopefully hadn\\'t rotted yet).  \\n  \\nMeanwhile I looked for an apartment to buy. Now I could actually choose what\\nneighborhood to live in. Where, I asked myself and various real estate agents,\\nis the Cambridge of New York? Aided by occasional visits to actual Cambridge,\\nI gradually realized there wasn\\'t one. Huh.  \\n  \\nAround this time, in the spring of 2000, I had an idea. It was clear from our\\nexperience with Viaweb that web apps were the future. Why not build a web app\\nfor making web apps? Why not let people edit code on our server through the\\nbrowser, and then host the resulting applications for them? [9] You could run\\nall sorts of services on the servers that these applications could use just by\\nmaking an API call: making and receiving phone calls, manipulating images,\\ntaking credit card payments, etc.  \\n  \\nI got so excited about this idea that I couldn\\'t think about anything else. It\\nseemed obvious that this was the future. I didn\\'t particularly want to start\\nanother company, but it was clear that this idea would have to be embodied as\\none, so I decided to move to Cambridge and start it. I hoped to lure Robert\\ninto working on it with me, but there I ran into a hitch. Robert was now a\\npostdoc at MIT, and though he\\'d made a lot of money the last time I\\'d lured\\nhim into working on one of my schemes, it had also been a huge time sink. So\\nwhile he agreed that it sounded like a plausible idea, he firmly refused to\\nwork on it.  \\n  \\nHmph. Well, I\\'d do it myself then. I recruited Dan Giffin, who had worked for\\nViaweb, and two undergrads who wanted summer jobs, and we got to work trying\\nto build what it\\'s now clear is about twenty companies and several open source\\nprojects worth of software. The language for defining applications would of\\ncourse be a dialect of Lisp. But I wasn\\'t so naive as to assume I could spring\\nan overt Lisp on a general audience; we\\'d hide the parentheses, like Dylan\\ndid.  \\n  \\nBy then there was a name for the kind of company Viaweb was, an \"application\\nservice provider,\" or ASP. This name didn\\'t last long before it was replaced\\nby \"software as a service,\" but it was current for long enough that I named\\nthis new company after it: it was going to be called Aspra.  \\n  \\nI started working on the application builder, Dan worked on network\\ninfrastructure, and the two undergrads worked on the first two services\\n(images and phone calls). But about halfway through the summer I realized I\\nreally didn\\'t want to run a company \\x97 especially not a big one, which it was\\nlooking like this would have to be. I\\'d only started Viaweb because I needed\\nthe money. Now that I didn\\'t need money anymore, why was I doing this? If this\\nvision had to be realized as a company, then screw the vision. I\\'d build a\\nsubset that could be done as an open source project.  \\n  \\nMuch to my surprise, the time I spent working on this stuff was not wasted\\nafter all. After we started Y Combinator, I would often encounter startups\\nworking on parts of this new architecture, and it was very useful to have\\nspent so much time thinking about it and even trying to write some of it.  \\n  \\nThe subset I would build as an open source project was the new Lisp, whose\\nparentheses I now wouldn\\'t even have to hide. A lot of Lisp hackers dream of\\nbuilding a new Lisp, partly because one of the distinctive features of the\\nlanguage is that it has dialects, and partly, I think, because we have in our\\nminds a Platonic form of Lisp that all existing dialects fall short of. I\\ncertainly did. So at the end of the summer Dan and I switched to working on\\nthis new dialect of Lisp, which I called Arc, in a house I bought in\\nCambridge.  \\n  \\nThe following spring, lightning struck. I was invited to give a talk at a Lisp\\nconference, so I gave one about how we\\'d used Lisp at Viaweb. Afterward I put\\na postscript file of this talk online, on paulgraham.com, which I\\'d created\\nyears before using Viaweb but had never used for anything. In one day it got\\n30,000 page views. What on earth had happened? The referring urls showed that\\nsomeone had posted it on Slashdot. [10]  \\n  \\nWow, I thought, there\\'s an audience. If I write something and put it on the\\nweb, anyone can read it. That may seem obvious now, but it was surprising\\nthen. In the print era there was a narrow channel to readers, guarded by\\nfierce monsters known as editors. The only way to get an audience for anything\\nyou wrote was to get it published as a book, or in a newspaper or magazine.\\nNow anyone could publish anything.  \\n  \\nThis had been possible in principle since 1993, but not many people had\\nrealized it yet. I had been intimately involved with building the\\ninfrastructure of the web for most of that time, and a writer as well, and it\\nhad taken me 8 years to realize it. Even then it took me several years to\\nunderstand the implications. It meant there would be a whole new generation of\\n[_essays_](essay.html). [11]  \\n  \\nIn the print era, the channel for publishing essays had been vanishingly\\nsmall. Except for a few officially anointed thinkers who went to the right\\nparties in New York, the only people allowed to publish essays were\\nspecialists writing about their specialties. There were so many essays that\\nhad never been written, because there had been no way to publish them. Now\\nthey could be, and I was going to write them. [12]  \\n  \\nI\\'ve worked on several different things, but to the extent there was a turning\\npoint where I figured out what to work on, it was when I started publishing\\nessays online. From then on I knew that whatever else I did, I\\'d always write\\nessays too.  \\n  \\nI knew that online essays would be a [_marginal_](marginal.html) medium at\\nfirst. Socially they\\'d seem more like rants posted by nutjobs on their\\nGeoCities sites than the genteel and beautifully typeset compositions\\npublished in _The New Yorker_. But by this point I knew enough to find that\\nencouraging instead of discouraging.  \\n  \\nOne of the most conspicuous patterns I\\'ve noticed in my life is how well it\\nhas worked, for me at least, to work on things that weren\\'t prestigious. Still\\nlife has always been the least prestigious form of painting. Viaweb and Y\\nCombinator both seemed lame when we started them. I still get the glassy eye\\nfrom strangers when they ask what I\\'m writing, and I explain that it\\'s an\\nessay I\\'m going to publish on my web site. Even Lisp, though prestigious\\nintellectually in something like the way Latin is, also seems about as hip.  \\n  \\nIt\\'s not that unprestigious types of work are good per se. But when you find\\nyourself drawn to some kind of work despite its current lack of prestige, it\\'s\\na sign both that there\\'s something real to be discovered there, and that you\\nhave the right kind of motives. Impure motives are a big danger for the\\nambitious. If anything is going to lead you astray, it will be the desire to\\nimpress people. So while working on things that aren\\'t prestigious doesn\\'t\\nguarantee you\\'re on the right track, it at least guarantees you\\'re not on the\\nmost common type of wrong one.  \\n  \\nOver the next several years I wrote lots of essays about all kinds of\\ndifferent topics. O\\'Reilly reprinted a collection of them as a book, called\\n_Hackers & Painters_ after one of the essays in it. I also worked on spam\\nfilters, and did some more painting. I used to have dinners for a group of\\nfriends every thursday night, which taught me how to cook for groups. And I\\nbought another building in Cambridge, a former candy factory (and later, twas\\nsaid, porn studio), to use as an office.  \\n  \\nOne night in October 2003 there was a big party at my house. It was a clever\\nidea of my friend Maria Daniels, who was one of the thursday diners. Three\\nseparate hosts would all invite their friends to one party. So for every\\nguest, two thirds of the other guests would be people they didn\\'t know but\\nwould probably like. One of the guests was someone I didn\\'t know but would\\nturn out to like a lot: a woman called Jessica Livingston. A couple days later\\nI asked her out.  \\n  \\nJessica was in charge of marketing at a Boston investment bank. This bank\\nthought it understood startups, but over the next year, as she met friends of\\nmine from the startup world, she was surprised how different reality was. And\\nhow colorful their stories were. So she decided to compile a book of\\n[_interviews_](https://www.amazon.com/Founders-Work-Stories-Startups-\\nEarly/dp/1430210788) with startup founders.  \\n  \\nWhen the bank had financial problems and she had to fire half her staff, she\\nstarted looking for a new job. In early 2005 she interviewed for a marketing\\njob at a Boston VC firm. It took them weeks to make up their minds, and during\\nthis time I started telling her about all the things that needed to be fixed\\nabout venture capital. They should make a larger number of smaller investments\\ninstead of a handful of giant ones, they should be funding younger, more\\ntechnical founders instead of MBAs, they should let the founders remain as\\nCEO, and so on.  \\n  \\nOne of my tricks for writing essays had always been to give talks. The\\nprospect of having to stand up in front of a group of people and tell them\\nsomething that won\\'t waste their time is a great spur to the imagination. When\\nthe Harvard Computer Society, the undergrad computer club, asked me to give a\\ntalk, I decided I would tell them how to start a startup. Maybe they\\'d be able\\nto avoid the worst of the mistakes we\\'d made.  \\n  \\nSo I gave this talk, in the course of which I told them that the best sources\\nof seed funding were successful startup founders, because then they\\'d be\\nsources of advice too. Whereupon it seemed they were all looking expectantly\\nat me. Horrified at the prospect of having my inbox flooded by business plans\\n(if I\\'d only known), I blurted out \"But not me!\" and went on with the talk.\\nBut afterward it occurred to me that I should really stop procrastinating\\nabout angel investing. I\\'d been meaning to since Yahoo bought us, and now it\\nwas 7 years later and I still hadn\\'t done one angel investment.  \\n  \\nMeanwhile I had been scheming with Robert and Trevor about projects we could\\nwork on together. I missed working with them, and it seemed like there had to\\nbe something we could collaborate on.  \\n  \\nAs Jessica and I were walking home from dinner on March 11, at the corner of\\nGarden and Walker streets, these three threads converged. Screw the VCs who\\nwere taking so long to make up their minds. We\\'d start our own investment firm\\nand actually implement the ideas we\\'d been talking about. I\\'d fund it, and\\nJessica could quit her job and work for it, and we\\'d get Robert and Trevor as\\npartners too. [13]  \\n  \\nOnce again, ignorance worked in our favor. We had no idea how to be angel\\ninvestors, and in Boston in 2005 there were no Ron Conways to learn from. So\\nwe just made what seemed like the obvious choices, and some of the things we\\ndid turned out to be novel.  \\n  \\nThere are multiple components to Y Combinator, and we didn\\'t figure them all\\nout at once. The part we got first was to be an angel firm. In those days,\\nthose two words didn\\'t go together. There were VC firms, which were organized\\ncompanies with people whose job it was to make investments, but they only did\\nbig, million dollar investments. And there were angels, who did smaller\\ninvestments, but these were individuals who were usually focused on other\\nthings and made investments on the side. And neither of them helped founders\\nenough in the beginning. We knew how helpless founders were in some respects,\\nbecause we remembered how helpless we\\'d been. For example, one thing Julian\\nhad done for us that seemed to us like magic was to get us set up as a\\ncompany. We were fine writing fairly difficult software, but actually getting\\nincorporated, with bylaws and stock and all that stuff, how on earth did you\\ndo that? Our plan was not only to make seed investments, but to do for\\nstartups everything Julian had done for us.  \\n  \\nYC was not organized as a fund. It was cheap enough to run that we funded it\\nwith our own money. That went right by 99% of readers, but professional\\ninvestors are thinking \"Wow, that means they got all the returns.\" But once\\nagain, this was not due to any particular insight on our part. We didn\\'t know\\nhow VC firms were organized. It never occurred to us to try to raise a fund,\\nand if it had, we wouldn\\'t have known where to start. [14]  \\n  \\nThe most distinctive thing about YC is the batch model: to fund a bunch of\\nstartups all at once, twice a year, and then to spend three months focusing\\nintensively on trying to help them. That part we discovered by accident, not\\nmerely implicitly but explicitly due to our ignorance about investing. We\\nneeded to get experience as investors. What better way, we thought, than to\\nfund a whole bunch of startups at once? We knew undergrads got temporary jobs\\nat tech companies during the summer. Why not organize a summer program where\\nthey\\'d start startups instead? We wouldn\\'t feel guilty for being in a sense\\nfake investors, because they would in a similar sense be fake founders. So\\nwhile we probably wouldn\\'t make much money out of it, we\\'d at least get to\\npractice being investors on them, and they for their part would probably have\\na more interesting summer than they would working at Microsoft.  \\n  \\nWe\\'d use the building I owned in Cambridge as our headquarters. We\\'d all have\\ndinner there once a week \\x97 on tuesdays, since I was already cooking for the\\nthursday diners on thursdays \\x97 and after dinner we\\'d bring in experts on\\nstartups to give talks.  \\n  \\nWe knew undergrads were deciding then about summer jobs, so in a matter of\\ndays we cooked up something we called the Summer Founders Program, and I\\nposted an [_announcement_](summerfounder.html) on my site, inviting undergrads\\nto apply. I had never imagined that writing essays would be a way to get \"deal\\nflow,\" as investors call it, but it turned out to be the perfect source. [15]\\nWe got 225 applications for the Summer Founders Program, and we were surprised\\nto find that a lot of them were from people who\\'d already graduated, or were\\nabout to that spring. Already this SFP thing was starting to feel more serious\\nthan we\\'d intended.  \\n  \\nWe invited about 20 of the 225 groups to interview in person, and from those\\nwe picked 8 to fund. They were an impressive group. That first batch included\\nreddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron\\nSwartz, who had already helped write the RSS spec and would a few years later\\nbecome a martyr for open access, and Sam Altman, who would later become the\\nsecond president of YC. I don\\'t think it was entirely luck that the first\\nbatch was so good. You had to be pretty bold to sign up for a weird thing like\\nthe Summer Founders Program instead of a summer job at a legit place like\\nMicrosoft or Goldman Sachs.  \\n  \\nThe deal for startups was based on a combination of the deal we did with\\nJulian ($10k for 10%) and what Robert said MIT grad students got for the\\nsummer ($6k). We invested $6k per founder, which in the typical two-founder\\ncase was $12k, in return for 6%. That had to be fair, because it was twice as\\ngood as the deal we ourselves had taken. Plus that first summer, which was\\nreally hot, Jessica brought the founders free air conditioners. [16]  \\n  \\nFairly quickly I realized that we had stumbled upon the way to scale startup\\nfunding. Funding startups in batches was more convenient for us, because it\\nmeant we could do things for a lot of startups at once, but being part of a\\nbatch was better for the startups too. It solved one of the biggest problems\\nfaced by founders: the isolation. Now you not only had colleagues, but\\ncolleagues who understood the problems you were facing and could tell you how\\nthey were solving them.  \\n  \\nAs YC grew, we started to notice other advantages of scale. The alumni became\\na tight community, dedicated to helping one another, and especially the\\ncurrent batch, whose shoes they remembered being in. We also noticed that the\\nstartups were becoming one another\\'s customers. We used to refer jokingly to\\nthe \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots\\nof startups get their initial set of customers almost entirely from among\\ntheir batchmates.  \\n  \\nI had not originally intended YC to be a full-time job. I was going to do\\nthree things: hack, write essays, and work on YC. As YC grew, and I grew more\\nexcited about it, it started to take up a lot more than a third of my\\nattention. But for the first few years I was still able to work on other\\nthings.  \\n  \\nIn the summer of 2006, Robert and I started working on a new version of Arc.\\nThis one was reasonably fast, because it was compiled into Scheme. To test\\nthis new Arc, I wrote Hacker News in it. It was originally meant to be a news\\naggregator for startup founders and was called Startup News, but after a few\\nmonths I got tired of reading about nothing but startups. Plus it wasn\\'t\\nstartup founders we wanted to reach. It was future startup founders. So I\\nchanged the name to Hacker News and the topic to whatever engaged one\\'s\\nintellectual curiosity.  \\n  \\nHN was no doubt good for YC, but it was also by far the biggest source of\\nstress for me. If all I\\'d had to do was select and help founders, life would\\nhave been so easy. And that implies that HN was a mistake. Surely the biggest\\nsource of stress in one\\'s work should at least be something close to the core\\nof the work. Whereas I was like someone who was in pain while running a\\nmarathon not from the exertion of running, but because I had a blister from an\\nill-fitting shoe. When I was dealing with some urgent problem during YC, there\\nwas about a 60% chance it had to do with HN, and a 40% chance it had do with\\neverything else combined. [17]  \\n  \\nAs well as HN, I wrote all of YC\\'s internal software in Arc. But while I\\ncontinued to work a good deal _in_ Arc, I gradually stopped working _on_ Arc,\\npartly because I didn\\'t have time to, and partly because it was a lot less\\nattractive to mess around with the language now that we had all this\\ninfrastructure depending on it. So now my three projects were reduced to two:\\nwriting essays and working on YC.  \\n  \\nYC was different from other kinds of work I\\'ve done. Instead of deciding for\\nmyself what to work on, the problems came to me. Every 6 months there was a\\nnew batch of startups, and their problems, whatever they were, became our\\nproblems. It was very engaging work, because their problems were quite varied,\\nand the good founders were very effective. If you were trying to learn the\\nmost you could about startups in the shortest possible time, you couldn\\'t have\\npicked a better way to do it.  \\n  \\nThere were parts of the job I didn\\'t like. Disputes between cofounders,\\nfiguring out when people were lying to us, fighting with people who maltreated\\nthe startups, and so on. But I worked hard even at the parts I didn\\'t like. I\\nwas haunted by something Kevin Hale once said about companies: \"No one works\\nharder than the boss.\" He meant it both descriptively and prescriptively, and\\nit was the second part that scared me. I wanted YC to be good, so if how hard\\nI worked set the upper bound on how hard everyone else worked, I\\'d better work\\nvery hard.  \\n  \\nOne day in 2010, when he was visiting California for interviews, Robert Morris\\ndid something astonishing: he offered me unsolicited advice. I can only\\nremember him doing that once before. One day at Viaweb, when I was bent over\\ndouble from a kidney stone, he suggested that it would be a good idea for him\\nto take me to the hospital. That was what it took for Rtm to offer unsolicited\\nadvice. So I remember his exact words very clearly. \"You know,\" he said, \"you\\nshould make sure Y Combinator isn\\'t the last cool thing you do.\"  \\n  \\nAt the time I didn\\'t understand what he meant, but gradually it dawned on me\\nthat he was saying I should quit. This seemed strange advice, because YC was\\ndoing great. But if there was one thing rarer than Rtm offering advice, it was\\nRtm being wrong. So this set me thinking. It was true that on my current\\ntrajectory, YC would be the last thing I did, because it was only taking up\\nmore of my attention. It had already eaten Arc, and was in the process of\\neating essays too. Either YC was my life\\'s work or I\\'d have to leave\\neventually. And it wasn\\'t, so I would.  \\n  \\nIn the summer of 2012 my mother had a stroke, and the cause turned out to be a\\nblood clot caused by colon cancer. The stroke destroyed her balance, and she\\nwas put in a nursing home, but she really wanted to get out of it and back to\\nher house, and my sister and I were determined to help her do it. I used to\\nfly up to Oregon to visit her regularly, and I had a lot of time to think on\\nthose flights. On one of them I realized I was ready to hand YC over to\\nsomeone else.  \\n  \\nI asked Jessica if she wanted to be president, but she didn\\'t, so we decided\\nwe\\'d try to recruit Sam Altman. We talked to Robert and Trevor and we agreed\\nto make it a complete changing of the guard. Up till that point YC had been\\ncontrolled by the original LLC we four had started. But we wanted YC to last\\nfor a long time, and to do that it couldn\\'t be controlled by the founders. So\\nif Sam said yes, we\\'d let him reorganize YC. Robert and I would retire, and\\nJessica and Trevor would become ordinary partners.  \\n  \\nWhen we asked Sam if he wanted to be president of YC, initially he said no. He\\nwanted to start a startup to make nuclear reactors. But I kept at it, and in\\nOctober 2013 he finally agreed. We decided he\\'d take over starting with the\\nwinter 2014 batch. For the rest of 2013 I left running YC more and more to\\nSam, partly so he could learn the job, and partly because I was focused on my\\nmother, whose cancer had returned.  \\n  \\nShe died on January 15, 2014. We knew this was coming, but it was still hard\\nwhen it did.  \\n  \\nI kept working on YC till March, to help get that batch of startups through\\nDemo Day, then I checked out pretty completely. (I still talk to alumni and to\\nnew startups working on things I\\'m interested in, but that only takes a few\\nhours a week.)  \\n  \\nWhat should I do next? Rtm\\'s advice hadn\\'t included anything about that. I\\nwanted to do something completely different, so I decided I\\'d paint. I wanted\\nto see how good I could get if I really focused on it. So the day after I\\nstopped working on YC, I started painting. I was rusty and it took a while to\\nget back into shape, but it was at least completely engaging. [18]  \\n  \\nI spent most of the rest of 2014 painting. I\\'d never been able to work so\\nuninterruptedly before, and I got to be better than I had been. Not good\\nenough, but better. Then in November, right in the middle of a painting, I ran\\nout of steam. Up till that point I\\'d always been curious to see how the\\npainting I was working on would turn out, but suddenly finishing this one\\nseemed like a chore. So I stopped working on it and cleaned my brushes and\\nhaven\\'t painted since. So far anyway.  \\n  \\nI realize that sounds rather wimpy. But attention is a zero sum game. If you\\ncan choose what to work on, and you choose a project that\\'s not the best one\\n(or at least a good one) for you, then it\\'s getting in the way of another\\nproject that is. And at 50 there was some opportunity cost to screwing around.  \\n  \\nI started writing essays again, and wrote a bunch of new ones over the next\\nfew months. I even wrote a couple that [_weren\\'t_](know.html) about startups.\\nThen in March 2015 I started working on Lisp again.  \\n  \\nThe distinctive thing about Lisp is that its core is a language defined by\\nwriting an interpreter in itself. It wasn\\'t originally intended as a\\nprogramming language in the ordinary sense. It was meant to be a formal model\\nof computation, an alternative to the Turing machine. If you want to write an\\ninterpreter for a language in itself, what\\'s the minimum set of predefined\\noperators you need? The Lisp that John McCarthy invented, or more accurately\\ndiscovered, is an answer to that question. [19]  \\n  \\nMcCarthy didn\\'t realize this Lisp could even be used to program computers till\\nhis grad student Steve Russell suggested it. Russell translated McCarthy\\'s\\ninterpreter into IBM 704 machine language, and from that point Lisp started\\nalso to be a programming language in the ordinary sense. But its origins as a\\nmodel of computation gave it a power and elegance that other languages\\ncouldn\\'t match. It was this that attracted me in college, though I didn\\'t\\nunderstand why at the time.  \\n  \\nMcCarthy\\'s 1960 Lisp did nothing more than interpret Lisp expressions. It was\\nmissing a lot of things you\\'d want in a programming language. So these had to\\nbe added, and when they were, they weren\\'t defined using McCarthy\\'s original\\naxiomatic approach. That wouldn\\'t have been feasible at the time. McCarthy\\ntested his interpreter by hand-simulating the execution of programs. But it\\nwas already getting close to the limit of interpreters you could test that way\\n\\x97 indeed, there was a bug in it that McCarthy had overlooked. To test a more\\ncomplicated interpreter, you\\'d have had to run it, and computers then weren\\'t\\npowerful enough.  \\n  \\nNow they are, though. Now you could continue using McCarthy\\'s axiomatic\\napproach till you\\'d defined a complete programming language. And as long as\\nevery change you made to McCarthy\\'s Lisp was a discoveredness-preserving\\ntransformation, you could, in principle, end up with a complete language that\\nhad this quality. Harder to do than to talk about, of course, but if it was\\npossible in principle, why not try? So I decided to take a shot at it. It took\\n4 years, from March 26, 2015 to October 12, 2019. It was fortunate that I had\\na precisely defined goal, or it would have been hard to keep at it for so\\nlong.  \\n  \\nI wrote this new Lisp, called [_Bel_](bel.html), in itself in Arc. That may\\nsound like a contradiction, but it\\'s an indication of the sort of trickery I\\nhad to engage in to make this work. By means of an egregious collection of\\nhacks I managed to make something close enough to an interpreter written in\\nitself that could actually run. Not fast, but fast enough to test.  \\n  \\nI had to ban myself from writing essays during most of this time, or I\\'d never\\nhave finished. In late 2015 I spent 3 months writing essays, and when I went\\nback to working on Bel I could barely understand the code. Not so much because\\nit was badly written as because the problem is so convoluted. When you\\'re\\nworking on an interpreter written in itself, it\\'s hard to keep track of what\\'s\\nhappening at what level, and errors can be practically encrypted by the time\\nyou get them.  \\n  \\nSo I said no more essays till Bel was done. But I told few people about Bel\\nwhile I was working on it. So for years it must have seemed that I was doing\\nnothing, when in fact I was working harder than I\\'d ever worked on anything.\\nOccasionally after wrestling for hours with some gruesome bug I\\'d check\\nTwitter or HN and see someone asking \"Does Paul Graham still code?\"  \\n  \\nWorking on Bel was hard but satisfying. I worked on it so intensively that at\\nany given time I had a decent chunk of the code in my head and could write\\nmore there. I remember taking the boys to the coast on a sunny day in 2015 and\\nfiguring out how to deal with some problem involving continuations while I\\nwatched them play in the tide pools. It felt like I was doing life right. I\\nremember that because I was slightly dismayed at how novel it felt. The good\\nnews is that I had more moments like this over the next few years.  \\n  \\nIn the summer of 2016 we moved to England. We wanted our kids to see what it\\nwas like living in another country, and since I was a British citizen by\\nbirth, that seemed the obvious choice. We only meant to stay for a year, but\\nwe liked it so much that we still live there. So most of Bel was written in\\nEngland.  \\n  \\nIn the fall of 2019, Bel was finally finished. Like McCarthy\\'s original Lisp,\\nit\\'s a spec rather than an implementation, although like McCarthy\\'s Lisp it\\'s\\na spec expressed as code.  \\n  \\nNow that I could write essays again, I wrote a bunch about topics I\\'d had\\nstacked up. I kept writing essays through 2020, but I also started to think\\nabout other things I could work on. How should I choose what to do? Well, how\\nhad I chosen what to work on in the past? I wrote an essay for myself to\\nanswer that question, and I was surprised how long and messy the answer turned\\nout to be. If this surprised me, who\\'d lived it, then I thought perhaps it\\nwould be interesting to other people, and encouraging to those with similarly\\nmessy lives. So I wrote a more detailed version for others to read, and this\\nis the last sentence of it.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n**Notes**  \\n  \\n[1] My experience skipped a step in the evolution of computers: time-sharing\\nmachines with interactive OSes. I went straight from batch processing to\\nmicrocomputers, which made microcomputers seem all the more exciting.  \\n  \\n[2] Italian words for abstract concepts can nearly always be predicted from\\ntheir English cognates (except for occasional traps like _polluzione_). It\\'s\\nthe everyday words that differ. So if you string together a lot of abstract\\nconcepts with a few simple verbs, you can make a little Italian go a long way.  \\n  \\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight\\ndown the spine of old Florence: past the Pitti, across the bridge, past\\nOrsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli\\nto Piazza San Marco. I saw Florence at street level in every possible\\ncondition, from empty dark winter evenings to sweltering summer days when the\\nstreets were packed with tourists.  \\n  \\n[4] You can of course paint people like still lives if you want to, and\\nthey\\'re willing. That sort of portrait is arguably the apex of still life\\npainting, though the long sitting does tend to produce pained expressions in\\nthe sitters.  \\n  \\n[5] Interleaf was one of many companies that had smart people and built\\nimpressive technology, and yet got crushed by Moore\\'s Law. In the 1990s the\\nexponential growth in the power of commodity (i.e. Intel) processors rolled up\\nhigh-end, special-purpose hardware and software companies like a bulldozer.  \\n  \\n[6] The signature style seekers at RISD weren\\'t specifically mercenary. In the\\nart world, money and coolness are tightly coupled. Anything expensive comes to\\nbe seen as cool, and anything seen as cool will soon become equally expensive.  \\n  \\n[7] Technically the apartment wasn\\'t rent-controlled but rent-stabilized, but\\nthis is a refinement only New Yorkers would know or care about. The point is\\nthat it was really cheap, less than half market price.  \\n  \\n[8] Most software you can launch as soon as it\\'s done. But when the software\\nis an online store builder and you\\'re hosting the stores, if you don\\'t have\\nany users yet, that fact will be painfully obvious. So before we could launch\\npublicly we had to launch privately, in the sense of recruiting an initial set\\nof users and making sure they had decent-looking stores.  \\n  \\n[9] We\\'d had a code editor in Viaweb for users to define their own page\\nstyles. They didn\\'t know it, but they were editing Lisp expressions\\nunderneath. But this wasn\\'t an app editor, because the code ran when the\\nmerchants\\' sites were generated, not when shoppers visited them.  \\n  \\n[10] This was the first instance of what is now a familiar experience, and so\\nwas what happened next, when I read the comments and found they were full of\\nangry people. How could I claim that Lisp was better than other languages?\\nWeren\\'t they all Turing complete? People who see the responses to essays I\\nwrite sometimes tell me how sorry they feel for me, but I\\'m not exaggerating\\nwhen I reply that it has always been like this, since the very beginning. It\\ncomes with the territory. An essay must tell readers things they [_don\\'t\\nalready know_](useful.html), and some people dislike being told such things.  \\n  \\n[11] People put plenty of stuff on the internet in the 90s of course, but\\nputting something online is not the same as publishing it online. Publishing\\nonline means you treat the online version as the (or at least a) primary\\nversion.  \\n  \\n[12] There is a general lesson here that our experience with Y Combinator also\\nteaches: Customs continue to constrain you long after the restrictions that\\ncaused them have disappeared. Customary VC practice had once, like the customs\\nabout publishing essays, been based on real constraints. Startups had once\\nbeen much more expensive to start, and proportionally rare. Now they could be\\ncheap and common, but the VCs\\' customs still reflected the old world, just as\\ncustoms about writing essays still reflected the constraints of the print era.  \\n  \\nWhich in turn implies that people who are independent-minded (i.e. less\\ninfluenced by custom) will have an advantage in fields affected by rapid\\nchange (where customs are more likely to be obsolete).  \\n  \\nHere\\'s an interesting point, though: you can\\'t always predict which fields\\nwill be affected by rapid change. Obviously software and venture capital will\\nbe, but who would have predicted that essay writing would be?  \\n  \\n[13] Y Combinator was not the original name. At first we were called Cambridge\\nSeed. But we didn\\'t want a regional name, in case someone copied us in Silicon\\nValley, so we renamed ourselves after one of the coolest tricks in the lambda\\ncalculus, the Y combinator.  \\n  \\nI picked orange as our color partly because it\\'s the warmest, and partly\\nbecause no VC used it. In 2005 all the VCs used staid colors like maroon, navy\\nblue, and forest green, because they were trying to appeal to LPs, not\\nfounders. The YC logo itself is an inside joke: the Viaweb logo had been a\\nwhite V on a red circle, so I made the YC logo a white Y on an orange square.  \\n  \\n[14] YC did become a fund for a couple years starting in 2009, because it was\\ngetting so big I could no longer afford to fund it personally. But after\\nHeroku got bought we had enough money to go back to being self-funded.  \\n  \\n[15] I\\'ve never liked the term \"deal flow,\" because it implies that the number\\nof new startups at any given time is fixed. This is not only false, but it\\'s\\nthe purpose of YC to falsify it, by causing startups to be founded that would\\nnot otherwise have existed.  \\n  \\n[16] She reports that they were all different shapes and sizes, because there\\nwas a run on air conditioners and she had to get whatever she could, but that\\nthey were all heavier than she could carry now.  \\n  \\n[17] Another problem with HN was a bizarre edge case that occurs when you both\\nwrite essays and run a forum. When you run a forum, you\\'re assumed to see if\\nnot every conversation, at least every conversation involving you. And when\\nyou write essays, people post highly imaginative misinterpretations of them on\\nforums. Individually these two phenomena are tedious but bearable, but the\\ncombination is disastrous. You actually have to respond to the\\nmisinterpretations, because the assumption that you\\'re present in the\\nconversation means that not responding to any sufficiently upvoted\\nmisinterpretation reads as a tacit admission that it\\'s correct. But that in\\nturn encourages more; anyone who wants to pick a fight with you senses that\\nnow is their chance.  \\n  \\n[18] The worst thing about leaving YC was not working with Jessica anymore.\\nWe\\'d been working on YC almost the whole time we\\'d known each other, and we\\'d\\nneither tried nor wanted to separate it from our personal lives, so leaving\\nwas like pulling up a deeply rooted tree.  \\n  \\n[19] One way to get more precise about the concept of invented vs discovered\\nis to talk about space aliens. Any sufficiently advanced alien civilization\\nwould certainly know about the Pythagorean theorem, for example. I believe,\\nthough with less certainty, that they would also know about the Lisp in\\nMcCarthy\\'s 1960 paper.  \\n  \\nBut if so there\\'s no reason to suppose that this is the limit of the language\\nthat might be known to them. Presumably aliens need numbers and errors and I/O\\ntoo. So it seems likely there exists at least one path out of McCarthy\\'s Lisp\\nalong which discoveredness is preserved.  \\n  \\n  \\n  \\n**Thanks** to Trevor Blackwell, John Collison, Patrick Collison, Daniel\\nGackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for\\nreading drafts of this.  \\n  \\n  \\n---  \\n  \\n  \\n\\n* * *  \\n  \\n---\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQS6DWSCI_a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ukw6aQu-QZr"
      },
      "outputs": [],
      "source": [
        "# prompt: save documents to a pickle file\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Assuming 'documents' is the variable containing your list of Document objects\n",
        "with open('documents.pkl', 'wb') as f:\n",
        "  pickle.dump(documents, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJWpd8CV94MB",
        "outputId": "c31da38e-ff6c-4d3c-f284-14c465a7f1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'file_path': '/content/LLM_papers/LongFormer - Long  document transformer.pdf',\n",
              " 'file_name': 'LongFormer - Long  document transformer.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 539211,\n",
              " 'creation_date': '2024-10-23',\n",
              " 'last_modified_date': '2024-10-22'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzVJ3g-cHW27",
        "outputId": "013ce58d-da47-492e-eb77-af6b31afc9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(id_='ada9b534-8c6b-4a14-a307-584945e834e2', embedding=None, metadata={'file_path': '/content/LLM_papers/LongFormer - Long  document transformer.pdf', 'file_name': 'LongFormer - Long  document transformer.pdf', 'file_type': 'application/pdf', 'file_size': 539211, 'creation_date': '2024-10-23', 'last_modified_date': '2024-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Longformer: The Long-Document Transformer\\n\\nIz Beltagy∗ Matthew E. Peters∗ Arman Cohan∗\\n\\nAllen Institute for Artificial Intelligence, Seattle, WA, USA\\n\\n{beltagy,matthewp,armanc}@allenai.org\\n\\n# Abstract\\n\\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.1\\n\\n# 1 Introduction\\n\\nTransformers (Vaswani et al., 2017) have achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling (Dai et al., 2019; Radford et al., 2019) and discriminative language understanding (Devlin et al., 2019). This success is partly due to the self-attention component which enables the network to capture contextual information from the entire sequence. While powerful, the memory and computational requirements of self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to process long sequences.\\n\\nTo address this limitation, we present Longformer, a modified Transformer architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for processing long documents (Fig 1). This is an advantage for natural language tasks such as long document classification, question answering (QA), and coreference resolution, where existing approaches partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of BERT-style pretrained models. Such partitioning could potentially result in loss of important cross-partition information, and to mitigate this problem, existing methods often rely on complex architectures to address such interactions. On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the\\n\\n# Figure 1\\n\\nRuntime and memory of full self-attention and different implementations of Longformer’s self-attention; Longformer-loop is non-vectorized, Longformer-chunk is vectorized, and Longformer-cuda is a custom cuda kernel implementations. Longformer’s memory usage scales linearly with the sequence length, unlike the full self-attention mechanism that runs out of memory for long sequences on current GPUs. Different implementations vary in speed, with the vectorized Longformer-chunk being the fastest. More details are in section 3.2.\\n\\n1 https://github.com/allenai/longformer\\n---\\nneed for task-specific architectures. Recent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to document-level NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer’s attention mechanism can act as a drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks.\\n\\nLongformer’s attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential – the local attention is primarily used to build contextual representations, while the global attention allows Longformer to build full sequence representations for prediction.\\n\\nWe first evaluate Longformer on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling.\\n\\nThen, to evaluate Longformer’s ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint. After pretraining, we apply it to downstream language tasks through finetuning and demonstrate that Longformer consistently outperforms RoBERTa on a wide range of document-level natural language tasks including text classification, QA, and coreference resolution, achieving state-of-the-art results on two of these datasets.\\n\\nWe finally introduce a variant of Longformer which instead of an encoder-only Transformer architecture, it follows an encoder-decoder architecture similar to the original Transformer model (Vaswani et al., 2017), and it is intended for sequence-to-sequence (seq2seq) learning (Sutskever et al., 2014). We call this model Longformer-Encoder-Decoder (LED) that uses Longformer’s efficient attention pattern on the encoder network, allowing it to address long document seq2seq tasks such as summarization. We demonstrate the effectiveness of LED on the arXiv summarization dataset (Cohan et al., 2018).\\n\\n# 2 Related Work\\n\\nLong-Document Transformers Tab. 1 summarizes recent prior work on long documents. Two types of self-attention approaches have been explored. The first is a left-to-right (ltr) approach that processes the document in chunks moving from left-to-right. While such models have been successful in autoregressive language modeling, they are unsuitable for transfer learning approaches with tasks that benefit from bidirectional context.\\n\\nOur work falls within the other general approach that defines some form of sparse attention pattern and avoids computing the full quadratic attention matrix multiplication. The model with the most similar attention pattern to ours is Sparse Transformer (Child et al., 2019), which uses a form of dilated sliding window of blocks of size 8x8 provided by BlockSparse (Gray et al., 2017). Our implementation ( §3) also includes a custom CUDA kernel, but it is more flexible and maintainable than BlockSparse which is implemented in C++, and designed for a specific version of TensorFlow. We also introduce additional task motivated global attention patterns suitable for common NLP tasks (§ 3) and show they are essential for good performance in the transfer learning setting.\\n\\nA few models tried tasks other than autoregressive language modeling, which is a step forward because arguably focusing on language modeling as the primary evaluation has led to the development of models with limited applicability. BP-Transformer (Ye et al., 2019) evaluated on machine\\n\\n|Model|attention|char-LM|other pretrain tasks| |\\n|---|---|---|---|---|\\n|Transformer-XL (2019)|ltr|yes|no|no|\\n|Adaptive Span (2019)|ltr|yes|no|no|\\n|Compressive (2020)|ltr|yes|no|no|\\n|Reformer (2020)|sparse|yes|no|no|\\n|Sparse (2019)|sparse|yes|no|no|\\n|Routing (2020)|sparse|yes|no|no|\\n|BP-Transformer (2019)|sparse|yes|MT|no|\\n|Blockwise (2019)|sparse|no|QA|yes|\\n|Our Longformer|sparse|yes|multiple|yes|\\n\\nTable 1: Summary of prior work on adapting Transformers for long documents. ltr: left-to-right.\\n---\\n# Task-specific Models for Long Documents\\n\\nMany task-specific approaches have been developed to workaround the 512 limit of pretrained transformer models like BERT. The simplest approach just truncates the document, commonly used for classification (Xie et al., 2019). Another approach chunks the document into chunks of length 512 (could be overlapping), processes each chunk separately, then combines the activations with a task specific model (Joshi et al., 2019). A third approach popular for multihop and open domain QA tasks uses a two-stage model where the first stage retrieves relevant documents that are passed onto the second stage for answer extraction (Clark and Gardner, 2017; Chen et al., 2017). All of these approaches suffer from information loss due to truncation or cascading errors from the two stage approach. In contrast, Longformer can process long sequences without truncating or chunking, allowing us to adopt a much simpler approach that concatenates the available context and processes it in a single pass.\\n\\nA few contemporaneous works have explored similar ideas to Longformer using local + global attention in Transformers, and pre-training it for long document natural language tasks. In particular, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents. Different from Longformer, ETC uses relative position embeddings (which we only used for the Autoregressive LM setting), introduces an additional training objective (CPC loss) for pre-training, and configures global attention in a slightly different way. It shows strong results on several tasks including reading comprehension and classification. GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory. BigBird (Zaheer et al., 2020) is an extension over ETC with evaluation on additional tasks, including summarization. Importantly, through theoretical analysis, BigBird shows that sparse Transformers are universal approximators of sequence functions and preserve these properties of the full self-attention.\\n\\n# 3 Longformer\\n\\nThe original Transformer model has a self-attention component with O(n2) time and memory complexity where n is the input sequence length. To address this challenge, we sparsify the full self-attention matrix according to an “attention pattern” specifying pairs of input locations attending to one another. Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. This section discusses the design and implementation of this attention pattern.\\n\\n# 3.1 Attention Pattern\\n\\nSliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input, similar to CNNs (Wu et al., 2019). Given a fixed window size w, each token attends to 21w tokens on each side (Fig. 2b). The computation complexity of this pattern is O(n × w).\\n\\nFigure 2: Comparing the full self-attention pattern and the configuration of attention patterns in our Longformer.\\n---\\nwhich scales linearly with input sequence length n. simpler than existing task specific approaches that use complex architecture to combine information across smaller input chunks.\\n\\n# Linear Projections for Global Attention\\n\\nRecall that given the linear projections Q, K, V, the Transformer model (Vaswani et al., 2017) computes attention scores as follows:\\n\\nAttention(Q, K, V) = softmax( QKT)V (1)√dk\\n\\nWe use two sets of projections, Qs, Ks, Vs to compute attention scores of sliding window attention, and Qg, Kg, Vg to compute attention scores for the global attention. The additional projections provide flexibility to model the different types of attention, which we show is critical for best performance on downstream tasks. Qg, Kg, Vg are all initialized with values that match Qs, Ks, Vs.\\n\\n# 3.2 Implementation\\n\\nIn regular transformers, attention scores are computed as in Eqn. 1. The expensive operation is the matrix multiplication QKT because both Q and K have n (sequence length) projections. For Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QKT. As shown in Fig. 1, this results in a linear increase in memory usage compared to quadratic increase for full self-attention. However, implementing it requires a form of banded matrix multiplication that is not supported in existing deep learning libraries like PyTorch/Tensorflow. Fig. 1 compares the performance of three different ways of implementing it: loop is a memory efficient PyTorch implementation that supports dilation but is unusably slow and only used for testing; chunks only supports the non-dilated case and is used for the pretraining/finetuning setting; and cuda is our fully functioning highly optimized custom CUDA kernel implemented using TVM (Chen et al., 2018) and used for the language modeling experiments (see Appendix A for more details).\\n\\n# 4 Autoregressive Language Modeling\\n\\nAutoregressive or left-to-right language modeling is loosely defined as estimating the probability distribution of an existing token/character given its previous tokens/characters in an input sequence. This task is considered one of the fundamental tasks in natural language and recent prior work on modeling long sequences using transformers has relied.\\n---\\n# 4.1 Attention Pattern\\n\\nFor autoregressive language modeling we use our dilated sliding window attention. Following Sukhbaatar et al. (2019) we use differing window sizes across the layers. In particular, we use small window sizes for the lower layers and increase window sizes as we move to higher layers. This allows the top layers to learn higher-level representation of the entire sequence while having the lower layers capture local information. In addition, it provides balance between efficiency (smaller window sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements).\\n\\nWe do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. For the higher layers, we use a small amount of increasing dilation only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacrificing local context.\\n\\n# 4.2 Experiment Setup\\n\\nTo compare to prior work we focus on character-level LM (text8 and enwik8; Mahoney, 2009).\\n\\nTraining Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. However, we found that the model needs a large number of gradient updates to learn the local context first, before learning to utilize longer context. To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. In particular, in the first phase we start with a short sequence length and window size, then on each subsequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with starting sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix B for detailed configurations of each phase, and for all other hyperparameters).\\n\\n|Model|#Param|Dev|Test|\\n|---|---|---|---|\\n|T12 (Al-Rfou et al., 2018)|44M|-|1.18|\\n|Adaptive (Sukhbaatar et al., 2019)|38M|1.05|1.11|\\n|BP-Transformer (Ye et al., 2019)|39M|-|1.11|\\n|Our Longformer|41M|1.04|1.10|\\n\\n|Model|#Param|Test BPC|\\n|---|---|---|\\n|Transformer-XL (18 layers)|88M|1.03|\\n|Sparse (Child et al., 2019)|≈100M|0.99|\\n|Transformer-XL (24 layers)|277M|0.99|\\n|Adaptive (Sukhbaatar et al., 2019)|209M|0.98|\\n|Compressive (Rae et al., 2020)|277M|0.97|\\n|Routing (Roy et al., 2020)|≈223M|0.99|\\n|Our Longformer|102M|0.99|\\n\\nEvaluation We evaluate with sequences of length 32,256. Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence.\\n\\n# 4.2.1 Results\\n\\nTab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets. We achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively, demonstrating the effectiveness of our model. For large models, given how expensive these experiments are, and following recent work (Kitaev et al., 2020; Rae et al., 2020), we are only evaluating on enwik8. Tab. 3 shows that Longformer outperforms the comparable Transformer-XL model, matches the performance of the comparable Sparse Transformer (Child et al., 2019), and matches or slightly underperforms recent models that have more than twice the number of parameters. It is worth noting that Adaptive Span (Sukhbaatar et al., 2019) and Compressive Transformer (Rae et al., 2020) are not good fit for the pretraining-finetuning paradigm as discussed in §2.\\n---\\n|Model|Dev BPC|Model|base|large|\\n|---|---|---|---|---|\\n|Decreasing w (from 512 to 32)|1.24|RoBERTa (seqlen: 512)|1.846|1.496|\\n|Fixed w (= 230)|1.23|Longformer (seqlen: 4,096)|10.299|8.738|\\n|Increasing w (from 32 to 512)|1.21|+ copy position embeddings|1.957|1.597|\\n|No Dilation|1.21|+ 2K gradient updates|1.753|1.414|\\n|Dilation on 2 heads|1.20|+ 65K gradient updates|1.705|1.358|\\n| | |Longformer (train extra pos. embed. only)|1.850|1.504|\\n\\nTable 4: Top: changing window size across layers. Bottom: with/without dilation (@ 150K steps on phase1)\\n\\nTable 5: MLM BPC for RoBERTa and various pretrained Longformer configurations.\\n\\n# 4.2.2 Ablation Study\\n\\nTo show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each configuration for 150K steps4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set.\\n\\nThe top of Tab. 4 demonstrates the impact of different ways of configuring the window sizes per layer. We observe that increasing the window size from the bottom to the top layer leads to the best performance, arranging them in the reverse way leads to worse performance, and using a fixed window size (the average of window sizes of the other configuration) leads to a performance that is in between. The bottom of Tab. 4 shows the impact of adding dilation. Adding some dilation to two heads leads to some improvement compared with no dilation at all.\\n\\n# 5 Pretraining and Finetuning\\n\\nCurrent state-of-the-art systems for many NLP tasks finetune a pretrained model with task supervision (e.g. BERT). One of our main motivations is to develop such a model suitable for long document tasks. To do so, we pretrained Longformer on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)5.\\n\\nWe pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, we continue pretraining from the RoBERTa (Liu et al., 2019) released checkpoint, while only making the minimal changes necessary to support Longformer’s attention mechanism. Note that our attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture.\\n\\nWe use sliding window attention with window size of 512, therefore using the same amount of computation as RoBERTa6. RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa’s pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT’s attention heads shows a strong learned bias to attending to local context, including the previous or next token (Clark et al., 2019). Using the copy initialization preserves this local structure everywhere except at the partition boundaries. Despite its simplicity, we found this to be a very effective (see Tab. 5), allowing Longformer pretraining to rapidly converge with a small number of gradient updates.\\n\\nWe pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\\n\\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.8464 One caveat is that the ordering of end performance will not agree with that at step 150K. However, this approximation saves the huge cost of running every experiment to completion.5 Sequences up to 16K are possible on current GPUs. Retraining such model from scratch might be needed to improve performance.\\n---\\n|Wordpieces|WH|TQA|HQA|ON|HY| | | |\\n|---|---|---|---|---|---|---|---|---|\\n|avg.| |1,535|6,589|1,316|506|300|705| |\\n|95th pctl.| |3,627|17,126|1,889|1,147| |705|1,975|\\n\\nTable 6: Average and 95th percentile of context length of datasets in wordpieces. WH: WikiHop, TQA: TriviaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper-partisan news\\n\\nBPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Training for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large.\\n\\nFrozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents. This configuration has a BPC of 1.850 (down from 1.957 at initialization), but higher than 1.705 where all the weights are trainable.\\n\\n# 6 Tasks\\n\\nWe apply Longformer to multiple long document tasks, including QA, coreference resolution and classification. Tab. 6 shows the evaluation datasets have contexts significantly longer than 512 wordpieces. Our primary goal is to evaluate whether our attention mechanism can act as a replacement for the standard self-attention mechanism in BERT style models, and to perform controlled trials against a strong baseline. We are also interested in evaluating whether we can replace complicated task specific models necessitated by BERT’s limited context with simpler models that just concatenate all available context into a single sequence.\\n\\nOur baseline is a RoBERTa based model that breaks the context into the longest possible segment, passes each individually through RoBERTa, and concatenates the activations for further processing. For QA tasks, we also concatenate the question to each segment so that RoBERTa can condition its contextual representations of the context on the question. The Longformer variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention. The global attention uses additional linear projections (§3.1).\\n\\n# 6.1 Question answering\\n\\nWe used three datasets: WikiHop (Welbl et al., 2018), TriviaQA (Joshi et al., 2017, Wikipedia setting), and HotpotQA, (Yang et al., 2018, distractor setting).7\\n\\nFor WikiHop and TriviaQA we follow the simple QA model of BERT (Devlin et al., 2019), and concatenate question and documents into one long sequence, run it through Longformer, then have a dataset-specific prediction layer. WikiHop uses a classification layer for the candidate while TriviaQA uses the loss function of Clark and Gardner (2017) to predict answer span. We include global attention to question tokens and answer candidates for WikiHop and to question tokens for TriviaQA.\\n\\nHotpotQA is a multihop QA dataset that involves extracting answer spans and evidence sentences from 10 Wikipedia paragraphs, 2 of which are relevant and the rest are distractors. We use a two-stage model that first selects the most relevant paragraphs then passes them to a second stage for answer extraction. Both stages concatenate question and context into one sequence, run it through Longformer, then use task-specific prediction layers. We train the models in a multi-task way to predict relevant paragraphs, evidence sentences, answer spans and question types (yes/no/span) jointly. Note that this model is simpler than recent SOTA models that include complex task-specific architectures (e.g., (Tu et al., 2019; Chen et al., 2019; Tu et al., 2020; Groeneveld et al., 2020)). See Appendix D for further details about the models and hyperparameters.\\n\\n# 6.2 Coreference Resolution\\n\\nWe use OntoNotes (Pradhan et al., 2012), and the model from Joshi et al. (2019), a modification of\\n\\n7We use the full version of TriviaQA and HotpotQA, not the simplified versions in MRQA (Fisch et al., 2019).\\n---\\n# 6.3 Document Classification\\n\\nWe evaluate on IMDB (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019) datasets. IMDB is a standard sentiment classification dataset consisting of movie reviews. While most documents in this dataset are short, about 13.6% of them are larger than 512 wordpieces (Tab. 6). Documents in Hyperpartisan are relatively long, and it is small with only 645 documents making it a good test for Longformer’s ability to adapt to limited data. We use global attention on the [CLS] token.\\n\\n|Model|WikiHop|TriviaQA|HotpotQA|\\n|---|---|---|---|\\n|Current∗ SOTA|78.3|73.3|74.2|\\n|Longformer-large|81.9|77.3|73.2|\\n\\nTable 8: Leaderboard results of Longformer-large at time of submission (May 2020). All numbers are F1 scores.\\n\\nWe also evaluate the performance of Longformer-large on long context QA tasks. Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we report mean F1 across five seeds.\\n\\n# 6.4 Results\\n\\nLongformer-large for QA Main Result. Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we report mean F1 across five seeds.\\n\\nfound that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions.\\n---\\n|Model|ans. supp.|joint| |\\n|---|---|---|---|\\n|TAP 2 (ensemble) (Glaß et al., 2019)|79.8|86.7|70.7|\\n|SAE (Tu et al., 2019)|79.6|86.7|71.4|\\n|Quark (dev) (Groeneveld et al., 2020)|81.2|87.0|72.3|\\n|C2F Reader (Shao et al., 2020)|81.2|87.6|72.8|\\n|Longformer-large|81.3|88.3|73.2|\\n|ETC-large† (Ainslie et al., 2020)|81.2|89.1|73.6|\\n|GSAN-large†|81.6|88.7|73.9|\\n|HGN-large (Fang et al., 2020)|82.2|88.5|74.2|\\n\\nTable 9: HotpotQA results in distractor setting test set. Quark’s test results are not available. All numbers are F1 scores. † shows contemporaneous leaderboard submissions.\\n\\n|Model|Accuracy / ∆|\\n|---|---|\\n|Longformer (seqlen: 4,096)|73.8|\\n|RoBERTa-base (seqlen: 512)|72.4 / -1.4|\\n|Longformer (seqlen: 4,096, 15 epochs)| |\\n|Longformer (seqlen: 512, attention: n2)|75.0 / +1.2|\\n| |71.7 / -2.1|\\n|Longformer (seqlen: 2,048)|73.1 / -0.7|\\n|Longformer (no MLM pretraining)|73.2 / -0.6|\\n|Longformer (no linear proj.)|72.2 / -1.6|\\n|Longformer (no linear proj. no global atten.)|65.5 / -8.3|\\n|Longformer (pretrain extra position embed. only)|73.5 / -0.3|\\n\\nTable 10: WikiHop development set ablations\\n\\n# 6.5 Ablations on WikiHop\\n\\nTab. 10 presents an ablation study for WikiHop on the development set. All results use Longformer-base, fine-tuned for five epochs with identical hyperparameters except where noted. Longformer benefits from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training. In addition, when configured as in RoBERTa-base (seqlen: 512, and n2 attention) Longformer performs slightly worse than RoBERTa-base, confirming that performance gains are not due to additional pretraining. Performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as WikiHop.\\n\\n9At submission time, May 2020. Later, BigBird (Zaheer et al., 2020) improved leaderboard results on these datasets. There are confounding factors such as using 16X more compute in BigBird’s pretraining compared with Longformer, potentially affecting the performance.\\n\\n# 7 Longformer-Encoder-Decoder (LED)\\n\\nThe original Transformer (Vaswani et al., 2017) consisted of an encoder-decoder architecture, intended for sequence-to-sequence tasks (Sutskever et al., 2014), such as summarization and translation. While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoder-decoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can’t efficiently scale to seq2seq tasks with longer inputs.\\n\\nTo facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART’s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART’s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART’s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks.\\n\\nWe evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED’s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first &lt;s&gt; token. The decoder uses full attention to the entire encoder and previously decoded locations. As standard in seq2seq models, LED is trained using teacher forcing on gold training summaries and uses beam search at inference.\\n\\nTab. 11 demonstrates the results of LED-large 16K on the arXiv summarization task. This model is merely initialized from BART, with no additional\\n---\\n# Table 11: Summarization results of Longformer-Encoder-Decoder (LED) on the arXiv dataset.\\n\\n| |R-1|R-2|R-L|\\n|---|---|---|---|\\n|Discourse-aware (2018)|35.80|11.05|31.80|\\n|Extr-Abst-TLM (2020)|41.62|14.69|38.03|\\n|Dancer (2020)|42.70|16.54|38.44|\\n|Pegasus (2020)|44.21|16.95|38.83|\\n|LED-large (seqlen: 4,096) (ours)|44.40|17.94|39.76|\\n|BigBird (seqlen: 4,096) (2020)|46.63|19.02|41.77|\\n|LED-large (seqlen: 16,384) (ours)|46.63|19.62|41.83|\\n\\nMetrics from left to right are ROUGE-1, ROUGE-2 and ROUGE-L.\\n\\nWe observe that LED achieves state-of-the-art results on arXiv, slightly outperforming BigBird (Zaheer et al., 2020). Note that the BigBird summarization model supports sequence length of 4K tokens but starts from and continues pre-training Pegasus (Zhang et al., 2020), a model specifically designed and pre-trained for summarization. With no pre-training or task-specific initialization, but with ability to process longer inputs, LED can slightly outperform BigBird. Further improvements should be possible through pre-training of LED. Fig. 3 further illustrates the importance of sequence length showing the ability to process longer input significantly improves the results.\\n\\n# 8 Conclusion and Future Work\\n\\nWe present Longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level NLP tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. Longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. Longformer achieves state-of-the-art results on the character-level language modeling tasks of text8.\\n\\n# References\\n\\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Václav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268–284, Online. Association for Computational Linguistics.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. In AAAI.\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open-domain questions. In ACL.\\n\\nJifan Chen, Shih-Ting Lin, and Greg Durrett. 2019. Multi-hop question answering via reasoning chains. arXiv preprint, abs/1910.02610.\\n\\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. TVM: An automated end-to-end optimizing compiler for deep learning. In OSDI.\\n\\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint, abs/1604.06174.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint, abs/1904.10509.\\n\\nChristopher Clark and Matt Gardner. 2017. Simple and effective multi-paragraph reading comprehension. In ACL.\\n---\\n# References\\n\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT’s attention. arXiv preprint, abs/1906.04341.\\n\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In NAACL-HLT 2018.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In NeurIPS.\\n\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT.\\n\\nYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2020. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8823–8838, Online. Association for Computational Linguistics.\\n\\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In MRQA workshop at EMNLP.\\n\\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of academic articles. ArXiv, abs/2004.06190.\\n\\nMichael Glaß, Alfio Massimiliano Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, Gaudani Bhargav, Dinesh Garg, and Avirup Sil. 2019. Span selection pre-training for question answering. arXiv preprint, abs/1909.04120.\\n\\nScott Gray, Alec Radford, and Diederik P. Kingma. 2017. GPU kernels for block-sparse weights.\\n\\nDirk Groeneveld, Tushar Khot, Mausam, and Ashish Sabhwaral. 2020. A simple yet strong pipeline for HotpotQA. arXiv preprint, abs/2004.06753.\\n\\nAnkit Gupta and Jonathan Berant. 2020. GMAT: Global memory augmentation for transformers. ArXiv, abs/2006.03274.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL.\\n\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\\n\\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and Daniel Weld. 2019. BERT for coreference resolution: Baselines and analysis. In EMNLP-IJCNLP.\\n\\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David Corney, Benno Stein, and Martin Potthast. 2019. SemEval-2019 task 4: Hyperpartisan news detection. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 829–839, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\\n\\nThomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. ICLR.\\n\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In ICLR.\\n\\nOlga V. Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In EMNLP/IJCNLP.\\n\\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to-fine inference. In NAACL.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint, abs/1907.11692.\\n\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nMatt Mahoney. 2009. Large text compression benchmark.\\n\\nA¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. In SSW.\\n\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible\\n---\\n# References\\n\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL.\\n\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 1–40, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong Wang, and Jie Tang. 2019. Blockwise self-attention for long document understanding. arXiv preprint, abs/1911.02972.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\\n\\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In ICLR.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.\\n\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers. arXiv preprint, abs/2003.05997.\\n\\nNan Shao, Yiming Cui, Ting Liu, Shijin Wang, and Guoping Hu. 2020. Is graph structure necessary for multi-hop reasoning? arXiv preprint, abs/2004.03096.\\n\\nSandeep Subramanian, Raymond Li, Jonathan Pilault, and C. Pal. 2020. On extractive and abstractive neural document summarization with transformer language models. In EMNLP.\\n\\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In ACL.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.\\n\\nTrieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. arXiv preprint, abs/1806.02847.\\n\\nMing Tu, Jinke Huang, Xiaodong He, and Bowen Zhou. 2020. Graph sequential network for reasoning over sequences. In NeurIPS Graph Representation Learning workshop.\\n\\nMing Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bufang Zhou. 2019. Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents. arXiv preprint, abs/1911.00484.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.\\n\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL, 6:287–302.\\n\\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. arXiv preprint, abs/1901.10430.\\n\\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint, abs/1904.12848.\\n\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Li-Wei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. arXiv preprint, abs/2002.04745.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP.\\n\\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. BP-Transformer: Modelling long-range context via binary partitioning. arXiv preprint, abs/1911.04070.\\n\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti, S. Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang, and A. Ahmed. 2020. Big bird: Transformers for longer sequences. ArXiv, abs/2007.14062.\\n\\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In NeurIPS.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. ICML.\\n\\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV, pages 19–27.\\n---\\n# A Implementation Details\\n\\nImplementing Longformer’s dilated sliding window attention requires a form of banded matrix multiplication (matrix multiplication where the output is all zero except certain diagonals) that is not directly supported in existing deep learning libraries like PyTorch/Tensorflow. Fig. 1 compares the runtime and memory of three different ways of implementing it.\\n\\nLongformer-loop is a naive implementation that computes each diagonal separately in a loop. It is memory efficient because it only computes the non-zero values, but it is unusably slow. We only use it for testing because it is easy to implement but don’t use it to run experiments.\\n\\nLongformer-chunks only supports the non-dilated case. It chunks Q and K into overlapping blocks of size w and overlap of size 2 1w, multiplies the blocks, then mask out the diagonals. This is very compute efficient because it uses a single matrix multiplication operation from PyTorch, but it consumes 2x the amount of memory a perfectly optimized implementation should consume because it computes some of the zero values. Because of the compute efficiency, this implementation is most suitable for the pretrain/finetune case. We didn’t find the increase in memory to be a problem for this setting.\\n\\nLongformer-cuda is a custom CUDA kernel that we implement using TVM (Chen et al., 2018). It is a fully functioning implementation of our attention (not limited as Longformer-chunks), it is the most memory efficient, and it is as fast as the highly optimized full self-attention.10 We mainly use this implementation for the autoregressive language modeling experiments because of the memory efficiency (allows the longest sequences) and the support of dilation (needed for character-LM experiments).\\n\\n# Tensor Virtual Machine (TVM)\\n\\nWe build our custom CUDA kernel using TVM (Chen et al., 2018), a deep learning compiler stack that compiles high level description of a function into optimized device-specific code. Using TVM, we describe our banded matrix multiplication in high-level python constructs, then TVM generates the corresponding CUDA code and compiles it for GPUs.\\n\\n10It is worth noting that theoretically, a perfectly optimized Longformer-cuda should be faster than the n2 computation. However, achieving this level of performance requires special knowledge of low-level GPU programming, similar to implementing a highly optimized matrix multiplication. Our current implementation is sufficiently fast and practical to use.\\n\\n# B Character LM Hyperparameters\\n\\nWe evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019).\\n\\nWe employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyper-parameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention.\\n\\nWe ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8. We experimented with absolute position embeddings and learned position embeddings, dropout values of [0.1, 0.2] (small model) and [0.1, 0.4] (large model), pre-layernorm and post-layernorm (Xiong et al., 2020), learning rate (LR) of phase1 of values [2.5e-5, 5e-4, 1e-4] constant and cosine LR schedules, and different configurations for dilation (on all heads, on 2 heads, no dilation). Number of gradient updates/phase reported in Tab. 12 is determined by running each phase until the validation BPC stops.\\n\\n11https://github.com/kimiyoung/transformer-xl\\n\\n12https://github.com/NVIDIA/apex\\n\\n13We found that using fp16 in attention operation results in floating point overflow and NaNs in later stages of training.\\n---\\n# C Pretraining Data\\n\\nIn order to allow the model to learn long dependencies in pretraining, we compiled a corpus of long documents. Some of these data sources were also included in the original RoBERTa pretraining including the Books corpus (Zhu et al., 2015) plus English Wikipedia. We additionally included one third of a subset of the Realnews dataset (Zellers et al., 2019) with documents longer than 1,200 tokens as well as one third of the Stories (Trinh and Le, 2018) corpus. Our goal was to include a mix of long and short documents to both allow the model to learn longer dependencies while not to forget information from the original RoBERTa pretraining. The statistics of the pretraining data is shown in Tab. 13.\\n\\n# D Task specific model details\\n\\nAll the QA and classification models are implemented using PyTorch-Lightning. We use the official train/dev/test splits of all datasets except for the Hyperpartisan news which we randomly split into 80/10/10 for train/dev/test.\\n\\n# WikiHop\\n\\nInstances in WikiHop consist of: a question, answer candidates (ranging from two candidates to 79 candidates), supporting contexts (ranging from three paragraphs to 63 paragraphs), and the correct answer. The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision.\\n\\nTo prepare the data for input to Longformer and RoBERTa, we first tokenize the question, answer candidates, and support contexts using RoBERTa’s wordpiece tokenizer. Then we concatenate the question and answer candidates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent]. The contexts are also concatenated using RoBERTa’s document delimiter tokens as separators:  context1  ...  contextM . The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning.\\n\\nAfter preparing the input data, we compute activations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the first 4,096 length sequence due to memory requirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence.\\n\\nFor prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate.\\n\\nTraining used the Adam optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training. We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining.\\n\\nIn general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs.\\n\\n# TriviaQA\\n\\nTriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching.\\n\\nSimilar to WikiHop, we tokenize the question and the document using RoBERTa’s tokenizer, then form the input as [s] question [/s]\\n---\\n# Hyperparameters for the best performing model for character-level language modeling\\n\\n|Param|Value|\\n|---|---|\\n|Position Embeddings|Relative and Sinusoidal as in Dai et al. (2019)|\\n|Small model config|12 layers, 8 heads, 512 hidden size as in Dai et al. (2019)|\\n|Large model config|30 layers, 8 heads, 512 hidden size as in Child et al. (2019)|\\n|Optimizer|AdamW|\\n|Dropout|0.2 (small model), 0.4 (large model)|\\n|Gradient clipping|0.25|\\n|Weight Decay|0.01|\\n|Layernorm Location|pre-layernorm (Xiong et al., 2020)|\\n|Activation|GeLU|\\n|Number of phases|5|\\n|Phase 1 window sizes|32 (bottom layer) - 8,192 (top layer)|\\n|Phase 5 window sizes|512 (bottom layer) - (top layer)|\\n|Phase 1 sequence length|2,048|\\n|Phase 5 sequence length|23,040 (gpu memory limit)|\\n|Phase 1 LR|0.00025|\\n|Phase 5 LR|0.000015625|\\n|Batch size per phase|32, 32, 16, 16, 16|\\n|#Steps per phase (small)|430K, 50k, 50k, 35k, 5k|\\n|#Steps per phase (large)|350K, 25k, 10k, 5k, 5k|\\n|Warmup|10% of the phase steps with maximum 10K steps|\\n|LR scheduler|constant throughout each phase|\\n|Dilation (small model)|0 (layers 0-5), 1 (layers 6-7), 2 (layers 8-9), 3 (layers 10-11)|\\n|Dilation (large model)|0 (layers 0-14), 1 (layers 15-19), 2 (layers 20-24), 3 (layers 25-29)|\\n|Dilation heads|2 heads only|\\n\\n# Pretraining data\\n\\n|Source|Tokens|Avg doc len|\\n|---|---|---|\\n|Books (Zhu et al., 2015)|0.5B|95.9K|\\n|English Wikipedia|2.1B|506|\\n|Realnews (Zellers et al., 2019)|1.8B|1.7K|\\n|Stories (Trinh and Le, 2018)|2.1B|7.8K|\\n\\nWe truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens.\\n\\nFor prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them.\\n\\nHyperparameters of the best configuration are listed in Tab. 14. All other hyperparameters are similar to RoBERTa’s. For hyperparameter search, we only tuned LR for the RoBERTa baseline and tried rates [3e-5, 5e-5, 1e-4], then used the best, which is 3e-5, for all subsequent experiments with no further tuning. We trained the Longformer-large with the best configuration once and submitted its output to the leaderboard. We ran our experiments on 32GB V100 GPUs. Small model takes 1 day to train on 4 GPUs, while large model takes 1 day on 8 GPUs.\\n\\n# HotpotQA\\n\\nHotpotQA dataset involves answering questions from a set of 10 paragraphs from 10 different Wikipedia articles where 2 paragraphs are relevant to the question and the rest are distractors. It includes 2 tasks of answer span extraction and evidence sentence identification. Our model for HotpotQA combines both answer span extraction and evidence extraction in one joint model. We found a higher performance using a two-stage Longformer model with similar setup that first identifies relevant paragraphs and then does find the final answer span and evidence. This is largely because removing the distracting paragraphs first reduces the noise for the final evidence and span detection as also found to be important by recent state-of-the-art methods in this dataset (Fang et al., 2020). Similar to Wikihop and TriviaQA, to prepare the data for input to Longformer, we concatenate question and then all the 10 paragraphs in one long context. We particularly use the following input format with special tokens: “[CLS] [q] question [/q] 〈t〉 title1 〈/t〉 sent1,1 [s] sent1,2 [s] ...\\n\\nThe final dev performance of the two stage model improves over a single stage model by about 4.2 points on joint-F1 metric.\\n---\\n# title2\\n\\nsent2,1 [s] sent2,2\\n\\n...” where [q], [/q], 〈t〉, 〈/t〉, [s], [p] are special tokens representing, question start and end, paragraph title start and end, and sentence, respectively. The special tokens were added to the Longformer vocabulary and randomly initialized before task finetuning. For Longformer, we use global attention to question tokens, paragraph title start tokens as well as sentence tokens. The model includes additional feedforward layers on top of paragraph title start tokens for prediction of relevant paragraphs, as well as sentence tokens for predicting evidence sentences. After training the first stage model, we predict relevant paragraph scores for both training and development set. We then keep up to 5 paragraphs whose raw score is higher than a pre-specified threshold (-3.0), and remove the other paragraphs from the context. We then train the second stage model on the resulting shortened context. For answer span extraction we use BERT’s QA model (Devlin et al., 2019) with addition of a question type (yes/no/span) classification head over the first special token ([CLS]). For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\\n\\nAt inference time for evidence extraction, we use a constrained decoding strategy similar to Groenveld et al. (2020) that ensures that the evidence sentences come from exactly two paragraphs which is the setup of this dataset. We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with linear warmup (1000 steps) and linear decay. We used minimal hyperparameter tuning using LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 5 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperparameters are reported in Tab 14.\\n\\n# Coreference model details\\n\\nThe coreference model is a straightforward adaptation of the coarse-to-fine BERT based model from Joshi et al. (2019). After preprocessing each document with the RoBERTa wordpiece tokenizer, it splits each document into non-overlapping segments up to the maximum sequence length, then concatenates the activations for the coarse-to-fine clustering stage that forms coreference clusters. The maximum sequence length was 384 for RoBERTa-base, chosen after three trials from [256, 384, 512] using the default hyperparameters in the original implementation. For Longformer-base the sequence length was 4,096. Similar to the original implementation, different learning rates were used for the pretrained RoBERTa parameters and the randomly initialized task parameters. Using a larger learning rate in the task parameters allows the optimizer to adjust them farther from their randomly initialized values without destroying the information in the pretrained RoBERTa parameters.\\n\\nHyperparameter searches were minimal and consisted of grid searches of RoBERTa LR in [1e-5, 2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for both RoBERTa and Longformer for a fair comparison. The best configuration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hyperparameters were the same as in the original implementation. Training takes about 10 hours on a single GPU.\\n\\nOur implementation is a superhack that involves PyTorch and Tensorflow sharing a single process and GPU. To avoid re-implementing the complicated coarse-to-fine logic from Tensorflow in PyTorch (that involves a highly optimized custom GPU kernel originally released by Lee et al. (2018)), we devised a system where the lower transformer portion of the model passes activations and gradients back and forth between PyTorch and Tensorflow. The input tensors are first run through the transformer in PyTorch, the activations are collected from the top layer, transferred from GPU to CPU then from CPU to Tensorflow and back to GPU to run the coarse-to-fine clustering and compute the loss. Then gradients are back propagated.\\n\\n|Param|WikiHop|TriviaQA|HotpotQA|\\n|---|---|---|---|\\n|Epochs|15|5|5|\\n|LR|3e-5|3e-5|5e-5|\\n|Warmup steps|200|1000|1000|\\n|Batch size|32|32|32|\\n|Optimizer|Adam|Adam|Adam|\\n\\nTable 14: Hyperparameters of the QA models. All models use a similar scheduler with linear warmup and decay.\\n---\\n# Text classification\\n\\nFor classification, following BERT, we used a simple binary cross entropy loss on top of a first [CLS] token with addition of global attention to [CLS]. We used Adam optimizer with batch sizes of 32 and linear warmup and decay with warmup steps equal to 0.1 of the total training steps. For both IMDB and Hyperpartisan news we did grid search of LRs [3e-5, 5e-5] and epochs [10, 15, 20] and found the model with [3e-5] and epochs 15 to work best. Experiments were done on a single RTX8000 GPU.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLuaAvSNGlLE"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "\n",
        "def get_page_nodes(docs, separator=\"\\n---\\n\"):\n",
        "    \"\"\"Split each document into page node, by separator.\"\"\"\n",
        "    nodes = []\n",
        "    for doc in docs:\n",
        "        doc_chunks = doc.text.split(separator)\n",
        "        for doc_chunk in doc_chunks:\n",
        "            node = TextNode(\n",
        "                text=doc_chunk,\n",
        "                metadata=deepcopy(doc.metadata),\n",
        "            )\n",
        "            nodes.append(node)\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_T49L5rDEPZ",
        "outputId": "5274a706-2a75-43d9-da6a-9ec37ce95365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error while downloading images from the parsed result: string indices must be integers\n"
          ]
        }
      ],
      "source": [
        "image_dicts = parser.get_images(\n",
        "    documents[0].text,\n",
        "    download_path=\"dellxps_images\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJlYK602-Ez9"
      },
      "source": [
        "#### Web page loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KcC1-35r_X7P"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TuLlu0y0_Ypu"
      },
      "outputs": [],
      "source": [
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/Large_language_model\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpWMN8tZ_9pQ",
        "outputId": "325df25a-7515-4d2e-dba4-9bf7ab1b2c60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='https://en.wikipedia.org/wiki/Large_language_model', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nmove to sidebar hide\\n\\nNavigation\\n\\n  * [Main page](/wiki/Main_Page \"Visit the main page \\\\[z\\\\]\")\\n  * [Contents](/wiki/Wikipedia:Contents \"Guides to browsing Wikipedia\")\\n  * [Current events](/wiki/Portal:Current_events \"Articles related to current events\")\\n  * [Random article](/wiki/Special:Random \"Visit a randomly selected article \\\\[x\\\\]\")\\n  * [About Wikipedia](/wiki/Wikipedia:About \"Learn about Wikipedia and how it works\")\\n  * [Contact us](//en.wikipedia.org/wiki/Wikipedia:Contact_us \"How to contact Wikipedia\")\\n\\nContribute\\n\\n  * [Help](/wiki/Help:Contents \"Guidance on how to use and edit Wikipedia\")\\n  * [Learn to edit](/wiki/Help:Introduction \"Learn how to edit Wikipedia\")\\n  * [Community portal](/wiki/Wikipedia:Community_portal \"The hub for editors\")\\n  * [Recent changes](/wiki/Special:RecentChanges \"A list of recent changes to Wikipedia \\\\[r\\\\]\")\\n  * [Upload file](/wiki/Wikipedia:File_upload_wizard \"Add images or other media for use on Wikipedia\")\\n\\n[ ![](/static/images/icons/wikipedia.png)\\n![Wikipedia](/static/images/mobile/copyright/wikipedia-wordmark-en.svg) ![The\\nFree Encyclopedia](/static/images/mobile/copyright/wikipedia-tagline-en.svg)\\n](/wiki/Main_Page)\\n\\n[ Search ](/wiki/Special:Search \"Search Wikipedia \\\\[f\\\\]\")\\n\\nSearch\\n\\n  * [Donate](https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en \"Support us by donating to the Wikimedia Foundation\")\\n\\nAppearance\\n\\n  * [Create account](/w/index.php?title=Special:CreateAccount&returnto=Large+language+model \"You are encouraged to create an account and log in; however, it is not mandatory\")\\n  * [Log in](/w/index.php?title=Special:UserLogin&returnto=Large+language+model \"You\\'re encouraged to log in; however, it\\'s not mandatory. \\\\[o\\\\]\")\\n\\nPersonal tools\\n\\n  * [ Create account](/w/index.php?title=Special:CreateAccount&returnto=Large+language+model \"You are encouraged to create an account and log in; however, it is not mandatory\")\\n  * [ Log in](/w/index.php?title=Special:UserLogin&returnto=Large+language+model \"You\\'re encouraged to log in; however, it\\'s not mandatory. \\\\[o\\\\]\")\\n\\nPages for logged out editors [learn more](/wiki/Help:Introduction)\\n\\n  * [Contributions](/wiki/Special:MyContributions \"A list of edits made from this IP address \\\\[y\\\\]\")\\n  * [Talk](/wiki/Special:MyTalk \"Discussion about edits from this IP address \\\\[n\\\\]\")\\n\\n## Contents\\n\\nmove to sidebar hide\\n\\n  * (Top)\\n  * 1 History\\n  * 2 Dataset preprocessing Toggle Dataset preprocessing subsection\\n    * 2.1 Tokenization\\n      * 2.1.1 BPE\\n      * 2.1.2 Problems\\n    * 2.2 Dataset cleaning\\n    * 2.3 Synthetic data\\n  * 3 Training and architecture Toggle Training and architecture subsection\\n    * 3.1 Reinforcement learning from human feedback (RLHF)\\n    * 3.2 Instruction tuning\\n    * 3.3 Mixture of experts\\n    * 3.4 Prompt engineering, attention mechanism, and context window\\n    * 3.5 Infrastructure\\n  * 4 Training cost\\n  * 5 Tool use\\n  * 6 Agency\\n  * 7 Compression\\n  * 8 Multimodality\\n  * 9 Properties Toggle Properties subsection\\n    * 9.1 Scaling laws\\n    * 9.2 Emergent abilities\\n  * 10 Interpretation Toggle Interpretation subsection\\n    * 10.1 Understanding and intelligence\\n  * 11 Evaluation Toggle Evaluation subsection\\n    * 11.1 Perplexity\\n      * 11.1.1 BPW, BPC, and BPT\\n    * 11.2 Task-specific datasets and benchmarks\\n      * 11.2.1 Adversarially constructed evaluations\\n  * 12 Wider impact Toggle Wider impact subsection\\n    * 12.1 Memorization and copyright\\n    * 12.2 Security\\n    * 12.3 Algorithmic bias\\n      * 12.3.1 Stereotyping\\n      * 12.3.2 Political bias\\n  * 13 List of Large Language Models\\n  * 14 See also\\n  * 15 Notes\\n  * 16 References\\n  * 17 Further reading\\n\\nToggle the table of contents\\n\\n# Large language model\\n\\n42 languages\\n\\n  * [Afrikaans](https://af.wikipedia.org/wiki/Groot_taalmodel \"Groot taalmodel – Afrikaans\")\\n  * [العربية](https://ar.wikipedia.org/wiki/%D9%86%D9%85%D9%88%D8%B0%D8%AC_%D9%84%D8%BA%D9%88%D9%8A_%D9%83%D8%A8%D9%8A%D8%B1 \"نموذج لغوي كبير – Arabic\")\\n  * [Azərbaycanca](https://az.wikipedia.org/wiki/B%C3%B6y%C3%BCk_dil_modeli \"Böyük dil modeli – Azerbaijani\")\\n  * [閩南語 / Bân-lâm-gú](https://zh-min-nan.wikipedia.org/wiki/T%C5%8Da-h%C3%AAng_g%C3%AD-gi%C3%A2n_b%C3%B4%CD%98-h%C3%AAng \"Tōa-hêng gí-giân bô͘-hêng – Minnan\")\\n  * [Boarisch](https://bar.wikipedia.org/wiki/Large_language_model \"Large language model – Bavarian\")\\n  * [Bosanski](https://bs.wikipedia.org/wiki/Veliki_jezi%C4%8Dki_modeli \"Veliki jezički modeli – Bosnian\")\\n  * [Català](https://ca.wikipedia.org/wiki/Model_de_llenguatge_extens \"Model de llenguatge extens – Catalan\")\\n  * [Čeština](https://cs.wikipedia.org/wiki/Velk%C3%BD_jazykov%C3%BD_model \"Velký jazykový model – Czech\")\\n  * [Deutsch](https://de.wikipedia.org/wiki/Large_Language_Model \"Large Language Model – German\")\\n  * [Ελληνικά](https://el.wikipedia.org/wiki/%CE%9C%CE%B5%CE%B3%CE%AC%CE%BB%CE%BF_%CE%B3%CE%BB%CF%89%CF%83%CF%83%CE%B9%CE%BA%CF%8C_%CE%BC%CE%BF%CE%BD%CF%84%CE%AD%CE%BB%CE%BF \"Μεγάλο γλωσσικό μοντέλο – Greek\")\\n  * [Español](https://es.wikipedia.org/wiki/Modelo_extenso_de_lenguaje \"Modelo extenso de lenguaje – Spanish\")\\n  * [Euskara](https://eu.wikipedia.org/wiki/Hizkuntza_Eredu_Handiak_\\\\(LLM\\\\) \"Hizkuntza Eredu Handiak \\\\(LLM\\\\) – Basque\")\\n  * [فارسی](https://fa.wikipedia.org/wiki/%D9%85%D8%AF%D9%84_%D8%B2%D8%A8%D8%A7%D9%86%DB%8C_%D8%A8%D8%B2%D8%B1%DA%AF \"مدل زبانی بزرگ – Persian\")\\n  * [Français](https://fr.wikipedia.org/wiki/Grand_mod%C3%A8le_de_langage \"Grand modèle de langage – French\")\\n  * [Gaeilge](https://ga.wikipedia.org/wiki/Samhail_teanga_mh%C3%B3r \"Samhail teanga mhór – Irish\")\\n  * [Galego](https://gl.wikipedia.org/wiki/Modelo_de_linguaxe_de_grande_escala \"Modelo de linguaxe de grande escala – Galician\")\\n  * [한국어](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8 \"대형 언어 모델 – Korean\")\\n  * [हिन्दी](https://hi.wikipedia.org/wiki/%E0%A4%AC%E0%A4%A1%E0%A4%BC%E0%A5%87_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE_%E0%A4%AE%E0%A5%89%E0%A4%A1%E0%A4%B2 \"बड़े भाषा मॉडल – Hindi\")\\n  * [Bahasa Indonesia](https://id.wikipedia.org/wiki/Model_bahasa_besar \"Model bahasa besar – Indonesian\")\\n  * [IsiZulu](https://zu.wikipedia.org/wiki/UNongo_lolimi_olukhulu \"UNongo lolimi olukhulu – Zulu\")\\n  * [Italiano](https://it.wikipedia.org/wiki/Modello_linguistico_di_grandi_dimensioni \"Modello linguistico di grandi dimensioni – Italian\")\\n  * [עברית](https://he.wikipedia.org/wiki/%D7%9E%D7%95%D7%93%D7%9C_%D7%A9%D7%A4%D7%94_%D7%92%D7%93%D7%95%D7%9C \"מודל שפה גדול – Hebrew\")\\n  * [Magyar](https://hu.wikipedia.org/wiki/Nagy_nyelvi_modell \"Nagy nyelvi modell – Hungarian\")\\n  * [Македонски](https://mk.wikipedia.org/wiki/%D0%93%D0%BE%D0%BB%D0%B5%D0%BC_%D1%98%D0%B0%D0%B7%D0%B8%D1%87%D0%B5%D0%BD_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB \"Голем јазичен модел – Macedonian\")\\n  * [Nederlands](https://nl.wikipedia.org/wiki/Groot_taalmodel \"Groot taalmodel – Dutch\")\\n  * [日本語](https://ja.wikipedia.org/wiki/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB \"大規模言語モデル – Japanese\")\\n  * [Polski](https://pl.wikipedia.org/wiki/Du%C5%BCy_model_j%C4%99zykowy \"Duży model językowy – Polish\")\\n  * [Português](https://pt.wikipedia.org/wiki/Modelos_de_linguagem_de_grande_escala \"Modelos de linguagem de grande escala – Portuguese\")\\n  * [Qaraqalpaqsha](https://kaa.wikipedia.org/wiki/%C3%9Alken_til_modeli \"Úlken til modeli – Kara-Kalpak\")\\n  * [Runa Simi](https://qu.wikipedia.org/wiki/Hatun_simi_wallpama \"Hatun simi wallpama – Quechua\")\\n  * [Русский](https://ru.wikipedia.org/wiki/%D0%91%D0%BE%D0%BB%D1%8C%D1%88%D0%B0%D1%8F_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C \"Большая языковая модель – Russian\")\\n  * [Shqip](https://sq.wikipedia.org/wiki/Modeli_i_gjuh%C3%ABs_s%C3%AB_madhe \"Modeli i gjuhës së madhe – Albanian\")\\n  * [Slovenščina](https://sl.wikipedia.org/wiki/Obse%C5%BEni_jezikovni_model \"Obsežni jezikovni model – Slovenian\")\\n  * [کوردی](https://ckb.wikipedia.org/wiki/%D9%85%DB%86%D8%AF%DB%8E%D9%84%DB%8C_%D8%B2%D9%85%D8%A7%D9%86%DB%8C_%DA%AF%DB%95%D9%88%D8%B1%DB%95 \"مۆدێلی زمانی گەورە – Central Kurdish\")\\n  * [Српски / srpski](https://sr.wikipedia.org/wiki/Veliki_jezi%C4%8Dki_modeli \"Veliki jezički modeli – Serbian\")\\n  * [Tagalog](https://tl.wikipedia.org/wiki/Malaking_modelong_pangwika \"Malaking modelong pangwika – Tagalog\")\\n  * [Türkçe](https://tr.wikipedia.org/wiki/Geni%C5%9F_dil_modeli \"Geniş dil modeli – Turkish\")\\n  * [Українська](https://uk.wikipedia.org/wiki/%D0%92%D0%B5%D0%BB%D0%B8%D0%BA%D0%B0_%D0%BC%D0%BE%D0%B2%D0%BD%D0%B0_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C \"Велика мовна модель – Ukrainian\")\\n  * [ئۇيغۇرچە / Uyghurche](https://ug.wikipedia.org/wiki/%DA%86%D9%88%DA%AD_%D8%AA%D9%89%D9%84_%D9%85%D9%88%D8%AF%D9%89%D9%84%D9%89 \"چوڭ تىل مودىلى – Uyghur\")\\n  * [Tiếng Việt](https://vi.wikipedia.org/wiki/M%C3%B4_h%C3%ACnh_ng%C3%B4n_ng%E1%BB%AF_l%E1%BB%9Bn \"Mô hình ngôn ngữ lớn – Vietnamese\")\\n  * [粵語](https://zh-yue.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B \"大型語言模型 – Cantonese\")\\n  * [中文](https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B \"大型语言模型 – Chinese\")\\n\\n[Edit\\nlinks](https://www.wikidata.org/wiki/Special:EntityPage/Q115305900#sitelinks-\\nwikipedia \"Edit interlanguage links\")\\n\\n  * [Article](/wiki/Large_language_model \"View the content page \\\\[c\\\\]\")\\n  * [Talk](/wiki/Talk:Large_language_model \"Discuss improvements to the content page \\\\[t\\\\]\")\\n\\nEnglish\\n\\n  * [Read](/wiki/Large_language_model)\\n  * [Edit](/w/index.php?title=Large_language_model&action=edit \"Edit this page \\\\[e\\\\]\")\\n  * [View history](/w/index.php?title=Large_language_model&action=history \"Past revisions of this page \\\\[h\\\\]\")\\n\\nTools\\n\\nTools\\n\\nmove to sidebar hide\\n\\nActions\\n\\n  * [Read](/wiki/Large_language_model)\\n  * [Edit](/w/index.php?title=Large_language_model&action=edit \"Edit this page \\\\[e\\\\]\")\\n  * [View history](/w/index.php?title=Large_language_model&action=history)\\n\\nGeneral\\n\\n  * [What links here](/wiki/Special:WhatLinksHere/Large_language_model \"List of all English Wikipedia pages containing links to this page \\\\[j\\\\]\")\\n  * [Related changes](/wiki/Special:RecentChangesLinked/Large_language_model \"Recent changes in pages linked from this page \\\\[k\\\\]\")\\n  * [Upload file](/wiki/Wikipedia:File_Upload_Wizard \"Upload files \\\\[u\\\\]\")\\n  * [Special pages](/wiki/Special:SpecialPages \"A list of all special pages \\\\[q\\\\]\")\\n  * [Permanent link](/w/index.php?title=Large_language_model&oldid=1251988652 \"Permanent link to this revision of this page\")\\n  * [Page information](/w/index.php?title=Large_language_model&action=info \"More information about this page\")\\n  * [Cite this page](/w/index.php?title=Special:CiteThisPage&page=Large_language_model&id=1251988652&wpFormIdentifier=titleform \"Information on how to cite this page\")\\n  * [Get shortened URL](/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model)\\n  * [Download QR code](/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model)\\n  * [Wikidata item](https://www.wikidata.org/wiki/Special:EntityPage/Q115305900 \"Structured data on this page hosted by Wikidata \\\\[g\\\\]\")\\n\\nPrint/export\\n\\n  * [Download as PDF](/w/index.php?title=Special:DownloadAsPdf&page=Large_language_model&action=show-download-screen \"Download this page as a PDF file\")\\n  * [Printable version](/w/index.php?title=Large_language_model&printable=yes \"Printable version of this page \\\\[p\\\\]\")\\n\\nIn other projects\\n\\n  * [Wikimedia Commons](https://commons.wikimedia.org/wiki/Category:Large_language_models)\\n\\nAppearance\\n\\nmove to sidebar hide\\n\\nFrom Wikipedia, the free encyclopedia\\n\\nType of artificial neural network\\n\\nNot to be confused with [Logic learning machine](/wiki/Logic_learning_machine\\n\"Logic learning machine\").\\n\\nPart of a series on  \\n---  \\n[Machine learning](/wiki/Machine_learning \"Machine learning\")  \\nand [data mining](/wiki/Data_mining \"Data mining\")  \\nParadigms\\n\\n  * [Supervised learning](/wiki/Supervised_learning \"Supervised learning\")\\n  * [Unsupervised learning](/wiki/Unsupervised_learning \"Unsupervised learning\")\\n  * [Semi-supervised learning](/wiki/Semi-supervised_learning \"Semi-supervised learning\")\\n  * [Self-supervised learning](/wiki/Self-supervised_learning \"Self-supervised learning\")\\n  * [Reinforcement learning](/wiki/Reinforcement_learning \"Reinforcement learning\")\\n  * [Meta-learning](/wiki/Meta-learning_\\\\(computer_science\\\\) \"Meta-learning \\\\(computer science\\\\)\")\\n  * [Online learning](/wiki/Online_machine_learning \"Online machine learning\")\\n  * [Batch learning](/wiki/Batch_learning \"Batch learning\")\\n  * [Curriculum learning](/wiki/Curriculum_learning \"Curriculum learning\")\\n  * [Rule-based learning](/wiki/Rule-based_machine_learning \"Rule-based machine learning\")\\n  * [Neuro-symbolic AI](/wiki/Neuro-symbolic_AI \"Neuro-symbolic AI\")\\n  * [Neuromorphic engineering](/wiki/Neuromorphic_engineering \"Neuromorphic engineering\")\\n  * [Quantum machine learning](/wiki/Quantum_machine_learning \"Quantum machine learning\")\\n\\n  \\nProblems\\n\\n  * [Classification](/wiki/Statistical_classification \"Statistical classification\")\\n  * [Generative modeling](/wiki/Generative_model \"Generative model\")\\n  * [Regression](/wiki/Regression_analysis \"Regression analysis\")\\n  * [Clustering](/wiki/Cluster_analysis \"Cluster analysis\")\\n  * [Dimensionality reduction](/wiki/Dimensionality_reduction \"Dimensionality reduction\")\\n  * [Density estimation](/wiki/Density_estimation \"Density estimation\")\\n  * [Anomaly detection](/wiki/Anomaly_detection \"Anomaly detection\")\\n  * [Data cleaning](/wiki/Data_cleaning \"Data cleaning\")\\n  * [AutoML](/wiki/Automated_machine_learning \"Automated machine learning\")\\n  * [Association rules](/wiki/Association_rule_learning \"Association rule learning\")\\n  * [Semantic analysis](/wiki/Semantic_analysis_\\\\(machine_learning\\\\) \"Semantic analysis \\\\(machine learning\\\\)\")\\n  * [Structured prediction](/wiki/Structured_prediction \"Structured prediction\")\\n  * [Feature engineering](/wiki/Feature_engineering \"Feature engineering\")\\n  * [Feature learning](/wiki/Feature_learning \"Feature learning\")\\n  * [Learning to rank](/wiki/Learning_to_rank \"Learning to rank\")\\n  * [Grammar induction](/wiki/Grammar_induction \"Grammar induction\")\\n  * [Ontology learning](/wiki/Ontology_learning \"Ontology learning\")\\n  * [Multimodal learning](/wiki/Multimodal_learning \"Multimodal learning\")\\n\\n  \\n[Supervised learning](/wiki/Supervised_learning \"Supervised learning\")  \\n(**[classification](/wiki/Statistical_classification \"Statistical\\nclassification\")** • **[regression](/wiki/Regression_analysis \"Regression\\nanalysis\")**)\\n\\n  * [Apprenticeship learning](/wiki/Apprenticeship_learning \"Apprenticeship learning\")\\n  * [Decision trees](/wiki/Decision_tree_learning \"Decision tree learning\")\\n  * [Ensembles](/wiki/Ensemble_learning \"Ensemble learning\")\\n    * [Bagging](/wiki/Bootstrap_aggregating \"Bootstrap aggregating\")\\n    * [Boosting](/wiki/Boosting_\\\\(machine_learning\\\\) \"Boosting \\\\(machine learning\\\\)\")\\n    * [Random forest](/wiki/Random_forest \"Random forest\")\\n  * [_k_ -NN](/wiki/K-nearest_neighbors_algorithm \"K-nearest neighbors algorithm\")\\n  * [Linear regression](/wiki/Linear_regression \"Linear regression\")\\n  * [Naive Bayes](/wiki/Naive_Bayes_classifier \"Naive Bayes classifier\")\\n  * [Artificial neural networks](/wiki/Artificial_neural_network \"Artificial neural network\")\\n  * [Logistic regression](/wiki/Logistic_regression \"Logistic regression\")\\n  * [Perceptron](/wiki/Perceptron \"Perceptron\")\\n  * [Relevance vector machine (RVM)](/wiki/Relevance_vector_machine \"Relevance vector machine\")\\n  * [Support vector machine (SVM)](/wiki/Support_vector_machine \"Support vector machine\")\\n\\n  \\n[Clustering](/wiki/Cluster_analysis \"Cluster analysis\")\\n\\n  * [BIRCH](/wiki/BIRCH \"BIRCH\")\\n  * [CURE](/wiki/CURE_algorithm \"CURE algorithm\")\\n  * [Hierarchical](/wiki/Hierarchical_clustering \"Hierarchical clustering\")\\n  * [_k_ -means](/wiki/K-means_clustering \"K-means clustering\")\\n  * [Fuzzy](/wiki/Fuzzy_clustering \"Fuzzy clustering\")\\n  * [Expectation–maximization (EM)](/wiki/Expectation%E2%80%93maximization_algorithm \"Expectation–maximization algorithm\")\\n  *   \\n[DBSCAN](/wiki/DBSCAN \"DBSCAN\")\\n\\n  * [OPTICS](/wiki/OPTICS_algorithm \"OPTICS algorithm\")\\n  * [Mean shift](/wiki/Mean_shift \"Mean shift\")\\n\\n  \\n[Dimensionality reduction](/wiki/Dimensionality_reduction \"Dimensionality\\nreduction\")\\n\\n  * [Factor analysis](/wiki/Factor_analysis \"Factor analysis\")\\n  * [CCA](/wiki/Canonical_correlation \"Canonical correlation\")\\n  * [ICA](/wiki/Independent_component_analysis \"Independent component analysis\")\\n  * [LDA](/wiki/Linear_discriminant_analysis \"Linear discriminant analysis\")\\n  * [NMF](/wiki/Non-negative_matrix_factorization \"Non-negative matrix factorization\")\\n  * [PCA](/wiki/Principal_component_analysis \"Principal component analysis\")\\n  * [PGD](/wiki/Proper_generalized_decomposition \"Proper generalized decomposition\")\\n  * [t-SNE](/wiki/T-distributed_stochastic_neighbor_embedding \"T-distributed stochastic neighbor embedding\")\\n  * [SDL](/wiki/Sparse_dictionary_learning \"Sparse dictionary learning\")\\n\\n  \\n[Structured prediction](/wiki/Structured_prediction \"Structured prediction\")\\n\\n  * [Graphical models](/wiki/Graphical_model \"Graphical model\")\\n    * [Bayes net](/wiki/Bayesian_network \"Bayesian network\")\\n    * [Conditional random field](/wiki/Conditional_random_field \"Conditional random field\")\\n    * [Hidden Markov](/wiki/Hidden_Markov_model \"Hidden Markov model\")\\n\\n  \\n[Anomaly detection](/wiki/Anomaly_detection \"Anomaly detection\")\\n\\n  * [RANSAC](/wiki/Random_sample_consensus \"Random sample consensus\")\\n  * [_k_ -NN](/wiki/K-nearest_neighbors_algorithm \"K-nearest neighbors algorithm\")\\n  * [Local outlier factor](/wiki/Local_outlier_factor \"Local outlier factor\")\\n  * [Isolation forest](/wiki/Isolation_forest \"Isolation forest\")\\n\\n  \\n[Artificial neural network](/wiki/Artificial_neural_network \"Artificial neural\\nnetwork\")\\n\\n  * [Autoencoder](/wiki/Autoencoder \"Autoencoder\")\\n  * [Deep learning](/wiki/Deep_learning \"Deep learning\")\\n  * [Feedforward neural network](/wiki/Feedforward_neural_network \"Feedforward neural network\")\\n  * [Recurrent neural network](/wiki/Recurrent_neural_network \"Recurrent neural network\")\\n    * [LSTM](/wiki/Long_short-term_memory \"Long short-term memory\")\\n    * [GRU](/wiki/Gated_recurrent_unit \"Gated recurrent unit\")\\n    * [ESN](/wiki/Echo_state_network \"Echo state network\")\\n    * [reservoir computing](/wiki/Reservoir_computing \"Reservoir computing\")\\n  * [Boltzmann machine](/wiki/Boltzmann_machine \"Boltzmann machine\")\\n    * [Restricted](/wiki/Restricted_Boltzmann_machine \"Restricted Boltzmann machine\")\\n  * [GAN](/wiki/Generative_adversarial_network \"Generative adversarial network\")\\n  * [Diffusion model](/wiki/Diffusion_model \"Diffusion model\")\\n  * [SOM](/wiki/Self-organizing_map \"Self-organizing map\")\\n  * [Convolutional neural network](/wiki/Convolutional_neural_network \"Convolutional neural network\")\\n    * [U-Net](/wiki/U-Net \"U-Net\")\\n    * [LeNet](/wiki/LeNet \"LeNet\")\\n    * [AlexNet](/wiki/AlexNet \"AlexNet\")\\n    * [DeepDream](/wiki/DeepDream \"DeepDream\")\\n  * [Neural radiance field](/wiki/Neural_radiance_field \"Neural radiance field\")\\n  * [Transformer](/wiki/Transformer_\\\\(machine_learning_model\\\\) \"Transformer \\\\(machine learning model\\\\)\")\\n    * [Vision](/wiki/Vision_transformer \"Vision transformer\")\\n  * [Mamba](/wiki/Mamba_\\\\(deep_learning_architecture\\\\) \"Mamba \\\\(deep learning architecture\\\\)\")\\n  * [Spiking neural network](/wiki/Spiking_neural_network \"Spiking neural network\")\\n  * [Memtransistor](/wiki/Memtransistor \"Memtransistor\")\\n  * [Electrochemical RAM](/wiki/Electrochemical_RAM \"Electrochemical RAM\") (ECRAM)\\n\\n  \\n[Reinforcement learning](/wiki/Reinforcement_learning \"Reinforcement\\nlearning\")\\n\\n  * [Q-learning](/wiki/Q-learning \"Q-learning\")\\n  * [SARSA](/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action \"State–action–reward–state–action\")\\n  * [Temporal difference (TD)](/wiki/Temporal_difference_learning \"Temporal difference learning\")\\n  * [Multi-agent](/wiki/Multi-agent_reinforcement_learning \"Multi-agent reinforcement learning\")\\n    * [Self-play](/wiki/Self-play_\\\\(reinforcement_learning_technique\\\\) \"Self-play \\\\(reinforcement learning technique\\\\)\")\\n\\n  \\nLearning with humans\\n\\n  * [Active learning](/wiki/Active_learning_\\\\(machine_learning\\\\) \"Active learning \\\\(machine learning\\\\)\")\\n  * [Crowdsourcing](/wiki/Crowdsourcing \"Crowdsourcing\")\\n  * [Human-in-the-loop](/wiki/Human-in-the-loop \"Human-in-the-loop\")\\n  * [RLHF](/wiki/Reinforcement_learning_from_human_feedback \"Reinforcement learning from human feedback\")\\n\\n  \\nModel diagnostics\\n\\n  * [Coefficient of determination](/wiki/Coefficient_of_determination \"Coefficient of determination\")\\n  * [Confusion matrix](/wiki/Confusion_matrix \"Confusion matrix\")\\n  * [Learning curve](/wiki/Learning_curve_\\\\(machine_learning\\\\) \"Learning curve \\\\(machine learning\\\\)\")\\n  * [ROC curve](/wiki/Receiver_operating_characteristic \"Receiver operating characteristic\")\\n\\n  \\nMathematical foundations\\n\\n  * [Kernel machines](/wiki/Kernel_machines \"Kernel machines\")\\n  * [Bias–variance tradeoff](/wiki/Bias%E2%80%93variance_tradeoff \"Bias–variance tradeoff\")\\n  * [Computational learning theory](/wiki/Computational_learning_theory \"Computational learning theory\")\\n  * [Empirical risk minimization](/wiki/Empirical_risk_minimization \"Empirical risk minimization\")\\n  * [Occam learning](/wiki/Occam_learning \"Occam learning\")\\n  * [PAC learning](/wiki/Probably_approximately_correct_learning \"Probably approximately correct learning\")\\n  * [Statistical learning](/wiki/Statistical_learning_theory \"Statistical learning theory\")\\n  * [VC theory](/wiki/Vapnik%E2%80%93Chervonenkis_theory \"Vapnik–Chervonenkis theory\")\\n\\n  \\nJournals and conferences\\n\\n  * [ECML PKDD](/wiki/ECML_PKDD \"ECML PKDD\")\\n  * [NeurIPS](/wiki/Conference_on_Neural_Information_Processing_Systems \"Conference on Neural Information Processing Systems\")\\n  * [ICML](/wiki/International_Conference_on_Machine_Learning \"International Conference on Machine Learning\")\\n  * [ICLR](/wiki/International_Conference_on_Learning_Representations \"International Conference on Learning Representations\")\\n  * [IJCAI](/wiki/International_Joint_Conference_on_Artificial_Intelligence \"International Joint Conference on Artificial Intelligence\")\\n  * [ML](/wiki/Machine_Learning_\\\\(journal\\\\) \"Machine Learning \\\\(journal\\\\)\")\\n  * [JMLR](/wiki/Journal_of_Machine_Learning_Research \"Journal of Machine Learning Research\")\\n\\n  \\nRelated articles\\n\\n  * [Glossary of artificial intelligence](/wiki/Glossary_of_artificial_intelligence \"Glossary of artificial intelligence\")\\n  * [List of datasets for machine-learning research](/wiki/List_of_datasets_for_machine-learning_research \"List of datasets for machine-learning research\")\\n    * [List of datasets in computer vision and image processing](/wiki/List_of_datasets_in_computer_vision_and_image_processing \"List of datasets in computer vision and image processing\")\\n  * [Outline of machine learning](/wiki/Outline_of_machine_learning \"Outline of machine learning\")\\n\\n  \\n  \\n  * [v](/wiki/Template:Machine_learning \"Template:Machine learning\")\\n  * [t](/wiki/Template_talk:Machine_learning \"Template talk:Machine learning\")\\n  * [e](/wiki/Special:EditPage/Template:Machine_learning \"Special:EditPage/Template:Machine learning\")\\n\\n  \\n  \\nA **large language model** (**LLM**) is a type of computational\\n[model](/wiki/Model#Conceptual_model \"Model\") designed for [natural language\\nprocessing](/wiki/Natural_language_processing \"Natural language processing\")\\ntasks such as language [generation](/wiki/Generative_artificial_intelligence\\n\"Generative artificial intelligence\"). As [language\\nmodels](/wiki/Language_model \"Language model\"), LLMs acquire these abilities\\nby [learning statistical relationships](/wiki/Machine_learning \"Machine\\nlearning\") from vast amounts of text during a [self-supervised](/wiki/Self-\\nsupervised_learning \"Self-supervised learning\") and [semi-\\nsupervised](/wiki/Semi-supervised_learning \"Semi-supervised learning\")\\ntraining process.[1]\\n\\nThe largest and most capable LLMs, as of August\\n2024[[update]](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit),\\nare [artificial neural networks](/wiki/Artificial_neural_network \"Artificial\\nneural network\") built with a decoder-only [transformer-based\\narchitecture](/wiki/Transformer_\\\\(deep_learning_architecture\\\\) \"Transformer\\n\\\\(deep learning architecture\\\\)\"), enabling efficient processing and generation\\nof large-scale text data. Modern models can be [fine-tuned](/wiki/Fine-\\ntuning_\\\\(deep_learning\\\\) \"Fine-tuning \\\\(deep learning\\\\)\") for specific tasks,\\nor be guided by [prompt engineering](/wiki/Prompt_engineering \"Prompt\\nengineering\").[2] These models acquire [predictive\\npower](/wiki/Predictive_learning \"Predictive learning\") regarding\\n[syntax](/wiki/Syntax \"Syntax\"), [semantics](/wiki/Semantics \"Semantics\"), and\\n[ontologies](/wiki/Ontology_\\\\(information_science\\\\) \"Ontology \\\\(information\\nscience\\\\)\")[3] inherent in human language corpora, but they also inherit\\ninaccuracies and [biases](/wiki/Algorithmic_bias \"Algorithmic bias\") present\\nin the [data](/wiki/Training,_validation,_and_test_data_sets \"Training,\\nvalidation, and test data sets\") on which they are trained.[4]\\n\\n## History\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=1 \"Edit\\nsection: History\")]\\n\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Trends_in_AI_training_FLOP_over_time_%282010-2025%29.svg/220px-\\nTrends_in_AI_training_FLOP_over_time_%282010-2025%29.svg.png)](/wiki/File:Trends_in_AI_training_FLOP_over_time_\\\\(2010-2025\\\\).svg)The\\ntraining compute of notable large models in FLOPs vs publication date over the\\nperiod 2010-2024. For overall notable models (top left), frontier models (top\\nright), top language models (bottom left) and top models within leading\\ncompanies (bottom right). The majority of these models are language models.\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/0/06/Large-\\nscale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg/220px-\\nLarge-\\nscale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg.png)](/wiki/File:Large-\\nscale_AI_training_compute_\\\\(FLOP\\\\)_vs_Publication_date_\\\\(2017-2024\\\\).svg)The\\ntraining compute of notable large AI models in FLOPs vs publication date over\\nthe period 2017-2024. The majority of large models are language models or\\nmultimodal models with language capacity.\\n\\nBefore 2017, there were a few language models that were large as compared to\\ncapacities then available. In the 1990s, the [IBM alignment\\nmodels](/wiki/IBM_alignment_models \"IBM alignment models\") pioneered\\nstatistical language modelling. A smoothed n-gram model in 2001 trained on 0.3\\nbillion words achieved then-SOTA (state of the art) perplexity.[5] In the\\n2000s, as Internet use became prevalent, some researchers constructed\\nInternet-scale language datasets (\"web as corpus\"[6]), upon which they trained\\nstatistical language models.[7][8] In 2009, in most language processing tasks,\\nstatistical language models dominated over symbolic language models, as they\\ncan usefully ingest large datasets.[9]\\n\\nAfter neural networks became dominant in image processing around 2012,[10]\\nthey were applied to language modelling as well. Google converted its\\ntranslation service to [Neural Machine\\nTranslation](/wiki/Google_Neural_Machine_Translation \"Google Neural Machine\\nTranslation\") in 2016. As it was before Transformers, it was done by seq2seq\\ndeep LSTM networks.\\n\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-\\nmodel-architecture.png/290px-The-Transformer-model-\\narchitecture.png)](/wiki/File:The-Transformer-model-architecture.png)An\\nillustration of main components of the transformer model from the original\\npaper, where layers were normalized after (instead of before) multiheaded\\nattention\\n\\nAt the 2017 [NeurIPS](/wiki/NeurIPS \"NeurIPS\") conference, Google researchers\\nintroduced the [transformer architecture](/wiki/Transformer_architecture\\n\"Transformer architecture\") in their landmark paper \"[Attention Is All You\\nNeed](/wiki/Attention_Is_All_You_Need \"Attention Is All You Need\")\". This\\npaper\\'s goal was to improve upon 2014 [Seq2seq](/wiki/Seq2seq \"Seq2seq\")\\ntechnology,[11] and was based mainly on the\\n[attention](/wiki/Attention_\\\\(machine_learning\\\\) \"Attention \\\\(machine\\nlearning\\\\)\") mechanism developed by Bahdanau et al. in 2014.[12] The following\\nyear in 2018, [BERT](/wiki/BERT_\\\\(language_model\\\\) \"BERT \\\\(language model\\\\)\")\\nwas introduced and quickly became \"ubiquitous\".[13] Though the original\\ntransformer has both encoder and decoder blocks, BERT is an encoder-only\\nmodel.\\n\\nAlthough decoder-only [GPT-1](/wiki/GPT-1 \"GPT-1\") was introduced in 2018, it\\nwas [GPT-2](/wiki/GPT-2 \"GPT-2\") in 2019 that caught widespread attention\\nbecause [OpenAI](/wiki/OpenAI \"OpenAI\") at first deemed it too powerful to\\nrelease publicly, out of fear of malicious use.[14] [GPT-3](/wiki/GPT-3\\n\"GPT-3\") in 2020 went a step further and as of\\n2024[[update]](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit)\\nis available only via [API](/wiki/Web_API \"Web API\") with no offering of\\ndownloading the model to execute locally. But it was the 2022 consumer-facing\\nbrowser-based [ChatGPT](/wiki/ChatGPT \"ChatGPT\") that captured the\\nimaginations of the general population and caused some media hype and online\\nbuzz.[15] The 2023 [GPT-4](/wiki/GPT-4 \"GPT-4\") was praised for its increased\\naccuracy and as a \"holy grail\" for its [multimodal](/wiki/Multimodal_learning\\n\"Multimodal learning\") capabilities.[16] OpenAI did not reveal high-level\\narchitecture and the number of\\n[parameters](/wiki/Parameter#Artificial_Intelligence \"Parameter\") of GPT-4.\\n\\nCompeting language models have for the most part been attempting to equal the\\nGPT series, at least in terms of number of parameters.[17]\\n\\nSince 2022, [source-available](/wiki/Source-available_software \"Source-\\navailable software\") models have been gaining popularity, especially at first\\nwith [BLOOM](/wiki/BLOOM_\\\\(language_model\\\\) \"BLOOM \\\\(language model\\\\)\") and\\n[LLaMA](/wiki/LLaMA \"LLaMA\"), though both have restrictions on the field of\\nuse. [Mistral AI](/wiki/Mistral_AI \"Mistral AI\")\\'s models Mistral 7B and\\nMixtral 8x7b have the more permissive [Apache License](/wiki/Apache_License\\n\"Apache License\"). As of June\\n2024[[update]](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit),\\nThe Instruction fine tuned variant of the Llama 3 70 billion parameter model\\nis the most powerful open LLM according to the LMSYS Chatbot Arena\\nLeaderboard, being more powerful than GPT-3.5 but not as powerful as\\nGPT-4.[18]\\n\\nAs of 2024, the largest and most capable models are all based on the\\nTransformer architecture. Some recent implementations are based on other\\narchitectures, such as [recurrent neural\\nnetwork](/wiki/Recurrent_neural_network \"Recurrent neural network\") variants\\nand [Mamba](/wiki/Mamba_\\\\(deep_learning_architecture\\\\) \"Mamba \\\\(deep learning\\narchitecture\\\\)\") (a [state space](/wiki/State-space_representation \"State-\\nspace representation\") model).[19][20][21]\\n\\n## Dataset preprocessing\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=2 \"Edit\\nsection: Dataset preprocessing\")]\\n\\nSee also: [List of datasets for machine-learning research §\\nInternet](/wiki/List_of_datasets_for_machine-learning_research#Internet \"List\\nof datasets for machine-learning research\")\\n\\n### Tokenization\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=3 \"Edit\\nsection: Tokenization\")]\\n\\nBecause [machine learning](/wiki/Machine_learning \"Machine learning\")\\nalgorithms process numbers rather than text, the text must be converted to\\nnumbers. In the first step, a vocabulary is decided upon, then integer indices\\nare arbitrarily but uniquely assigned to each vocabulary entry, and finally,\\nan [embedding](/wiki/Word_embedding \"Word embedding\") is associated to the\\ninteger index. Algorithms include [byte-pair\\nencoding](/wiki/Byte_pair_encoding \"Byte pair encoding\") (BPE) and\\n[WordPiece](/wiki/BERT_\\\\(language_model\\\\)#Design \"BERT \\\\(language model\\\\)\").\\nThere are also special tokens serving as [control\\ncharacters](/wiki/Control_character \"Control character\"), such as `[MASK]` for\\nmasked-out token (as used in [BERT](/wiki/BERT_\\\\(language_model\\\\) \"BERT\\n\\\\(language model\\\\)\")), and `[UNK]` (\"unknown\") for characters not appearing in\\nthe vocabulary. Also, some special symbols are used to denote special text\\nformatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and\\nGPT. \"##\" denotes continuation of a preceding word in BERT.[22]\\n\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split `tokenizer:\\ntexts -> series of numerical \"tokens\"` as\\n\\ntoken  | izer  | :  |  texts  |  -> | series  |  of  |  numerical  |  \"  | t  | ok  | ens  | \"   \\n---|---|---|---|---|---|---|---|---|---|---|---|---  \\n  \\nTokenization also [compresses](/wiki/Data_compression \"Data compression\") the\\ndatasets. Because LLMs generally require input to be an\\n[array](/wiki/Array_\\\\(data_structure\\\\) \"Array \\\\(data structure\\\\)\") that is not\\n[jagged](/wiki/Jagged_array \"Jagged array\"), the shorter texts must be\\n\"padded\" until they match the length of the longest one. How many tokens are,\\non average, needed per word depends on the language of the dataset.[23][24]\\n\\n#### BPE\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=4 \"Edit\\nsection: BPE\")]\\n\\nMain article: [Byte pair encoding](/wiki/Byte_pair_encoding \"Byte pair\\nencoding\")\\n\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first\\nstep, all unique characters (including blanks and [punctuation\\nmarks](/wiki/Punctuation_mark \"Punctuation mark\")) are treated as an initial\\nset of [_n_ -grams](/wiki/N-gram \"N-gram\") (i.e. initial set of uni-grams).\\nSuccessively the most frequent pair of adjacent characters is merged into a\\nbi-gram and all instances of the pair are replaced by it. All occurrences of\\nadjacent pairs of (previously merged) _n_ -grams that most frequently occur\\ntogether are then again merged into even lengthier _n_ -gram, until a\\nvocabulary of prescribed size is obtained (in case of [GPT-3](/wiki/GPT-3\\n\"GPT-3\"), the size is 50257).[25] After a tokenizer is trained, any text can\\nbe tokenized by it, as long as it does not contain characters not appearing in\\nthe initial-set of uni-grams.[26]\\n\\n#### Problems\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=5 \"Edit\\nsection: Problems\")]\\n\\nA token vocabulary based on the frequencies extracted from mainly English\\ncorpora uses as few tokens as possible for an average English word. An average\\nword in another language encoded by such an English-optimized tokenizer is\\nhowever split into suboptimal amount of tokens. GPT-2 tokenizer can use up to\\n15 times more tokens per word for some languages, for example for the [Shan\\nlanguage](/wiki/Shan_language \"Shan language\") from [Myanmar](/wiki/Myanmar\\n\"Myanmar\"). Even more widespread languages such as Portuguese and German have\\n\"a premium of 50%\" compared to English.[27]\\n\\nGreedy tokenization also causes subtle problems with text completion.[28]\\n\\n### Dataset cleaning\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=6 \"Edit\\nsection: Dataset cleaning\")]\\n\\nMain article: [Data cleansing](/wiki/Data_cleansing \"Data cleansing\")\\n\\nIn the context of training LLMs, datasets are typically cleaned by removing\\ntoxic passages from the dataset, discarding low-quality data, and de-\\nduplication.[29] Cleaned datasets can increase training efficiency and lead to\\nimproved downstream performance.[30][31] A trained LLM can be used to clean\\ndatasets for training a further LLM.[32]\\n\\nWith the increasing proportion of LLM-generated content on the web, data\\ncleaning in the future may include filtering out such content. LLM-generated\\ncontent can pose a problem if the content is similar to human text (making\\nfiltering difficult) but of lower quality (degrading performance of models\\ntrained on it).[33]\\n\\n### Synthetic data\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=7 \"Edit\\nsection: Synthetic data\")]\\n\\nMain article: [Synthetic data](/wiki/Synthetic_data \"Synthetic data\")\\n\\nTraining of largest language models might need more linguistic data than\\nnaturally available, or that the naturally occurring data is of insufficient\\nquality. In these cases, synthetic data might be used. Microsoft\\'s\\n[Phi](/w/index.php?title=Phi_\\\\(LLM\\\\)&action=edit&redlink=1 \"Phi \\\\(LLM\\\\) \\\\(page\\ndoes not exist\\\\)\") series of LLMs is trained on textbook-like data generated\\nby another LLM.[34]\\n\\n## Training and architecture\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=8 \"Edit\\nsection: Training and architecture\")]\\n\\nSee also: [Fine-tuning (machine learning)](/wiki/Fine-\\ntuning_\\\\(machine_learning\\\\) \"Fine-tuning \\\\(machine learning\\\\)\")\\n\\n### Reinforcement learning from human feedback (RLHF)\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=9 \"Edit\\nsection: Reinforcement learning from human feedback \\\\(RLHF\\\\)\")]\\n\\nMain article: [Reinforcement learning from human\\nfeedback](/wiki/Reinforcement_learning_from_human_feedback \"Reinforcement\\nlearning from human feedback\")\\n\\nReinforcement learning from human feedback (RLHF) through algorithms, such as\\n[proximal policy optimization](/wiki/Proximal_Policy_Optimization \"Proximal\\nPolicy Optimization\"), is used to further fine-tune a model based on a dataset\\nof human preferences.[35]\\n\\n### Instruction tuning\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=10 \"Edit\\nsection: Instruction tuning\")]\\n\\nUsing \"self-instruct\" approaches, LLMs have been able to\\n[bootstrap](/wiki/Bootstrapping \"Bootstrapping\") correct responses, replacing\\nany naive responses, starting from human-generated corrections of a few cases.\\nFor example, in the instruction \"Write an essay about the main themes\\nrepresented in _Hamlet_ ,\" an initial naive completion might be \"If you submit\\nthe essay after March 17, your grade will be reduced by 10% for each day of\\ndelay,\" based on the frequency of this textual sequence in the corpus.[36]\\n\\n### Mixture of experts\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=11 \"Edit\\nsection: Mixture of experts\")]\\n\\nMain article: [Mixture of experts](/wiki/Mixture_of_experts \"Mixture of\\nexperts\")\\n\\nThe largest LLM may be too expensive to train and use directly. For such\\nmodels, [mixture of experts](/wiki/Mixture_of_experts \"Mixture of experts\")\\n(MoE) can be applied, a line of research pursued by Google researchers since\\n2017 to train models reaching up to 1 trillion parameters.[37][38][39]\\n\\n### Prompt engineering, attention mechanism, and context window\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=12 \"Edit\\nsection: Prompt engineering, attention mechanism, and context window\")]\\n\\nSee also: [Prompt engineering](/wiki/Prompt_engineering \"Prompt engineering\")\\nand [Attention (machine learning)](/wiki/Attention_\\\\(machine_learning\\\\)\\n\"Attention \\\\(machine learning\\\\)\")\\n\\nMost results previously achievable only by (costly) fine-tuning, can be\\nachieved through [prompt engineering](/wiki/Prompt_engineering \"Prompt\\nengineering\"), although limited to the scope of a single conversation (more\\nprecisely, limited to the scope of a context window).[40]\\n\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Multiple_attention_heads.png/290px-\\nMultiple_attention_heads.png)](/wiki/File:Multiple_attention_heads.png) When\\neach head calculates, according to its own criteria, how much other tokens are\\nrelevant for the \"it_\" token, note that the second attention head, represented\\nby the second column, is focusing most on the first two rows, i.e. the tokens\\n\"The\" and \"animal\", while the third column is focusing most on the bottom two\\nrows, i.e. on \"tired\", which has been tokenized into two tokens.[41]\\n\\nIn order to find out which tokens are relevant to each other within the scope\\nof the context window, the attention mechanism calculates \"soft\" weights for\\neach token, more precisely for its embedding, by using multiple attention\\nheads, each with its own \"relevance\" for calculating its own soft weights. For\\nexample, the small (i.e. 117M parameter sized) [GPT-2](/wiki/GPT-2 \"GPT-2\")\\nmodel has had twelve attention heads and a context window of only 1k\\ntokens.[42] In its medium version it has 345M parameters and contains 24\\nlayers, each with 12 attention heads. For the training with gradient descent a\\nbatch size of 512 was utilized.[26]\\n\\nThe largest models, such as Google\\'s [Gemini\\n1.5](/wiki/Gemini_\\\\(language_model\\\\) \"Gemini \\\\(language model\\\\)\"), presented\\nin February 2024, can have a context window sized up to 1 million (context\\nwindow of 10 million was also \"successfully tested\").[43] Other models with\\nlarge context windows includes Anthropic\\'s Claude 2.1, with a context window\\nof up to 200k tokens.[44] Note that this maximum refers to the number of input\\ntokens and that the maximum number of output tokens differs from the input and\\nis often smaller. For example, the GPT-4 Turbo model has a maximum output of\\n4096 tokens.[45]\\n\\nLength of a conversation that the model can take into account when generating\\nits next answer is limited by the size of a context window, as well. If the\\nlength of a conversation, for example with [ChatGPT](/wiki/ChatGPT \"ChatGPT\"),\\nis longer than its context window, only the parts inside the context window\\nare taken into account when generating the next answer, or the model needs to\\napply some algorithm to summarize the too distant parts of conversation.\\n\\nThe shortcomings of making a context window larger include higher\\ncomputational cost and possibly diluting the focus on local context, while\\nmaking it smaller can cause a model to miss an important long-range\\ndependency. Balancing them are a matter of experimentation and domain-specific\\nconsiderations.\\n\\nA model may be pre-trained either to predict how the segment continues, or\\nwhat is missing in the segment, given a segment from its training dataset.[46]\\nIt can be either\\n\\n  * autoregressive (i.e. predicting how the segment continues, the way [GPTs](/wiki/Generative_pretrained_transformer \"Generative pretrained transformer\") do it): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\\n  * \"[masked](/wiki/Cloze_test \"Cloze test\")\" (i.e. filling in the parts missing from the segment, the way \"BERT\"[47] does it): for example, given a segment \"I like to `[__] [__]` cream\", the model predicts that \"eat\" and \"ice\" are missing.\\n\\nModels may be trained on auxiliary tasks which test their understanding of the\\ndata distribution, such as Next Sentence Prediction (NSP), in which pairs of\\nsentences are presented and the model must predict whether they appear\\nconsecutively in the training corpus.[47] During training,\\n[regularization](/wiki/Regularization_\\\\(mathematics\\\\) \"Regularization\\n\\\\(mathematics\\\\)\") loss is also used to stabilize training. However\\nregularization loss is usually not used during\\n[testing](/wiki/Training,_validation,_and_test_data_sets \"Training,\\nvalidation, and test data sets\") and evaluation.\\n\\n### Infrastructure\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=13 \"Edit\\nsection: Infrastructure\")]\\n\\nSubstantial infrastructure is necessary for training the largest\\nmodels.[48][49][50]\\n\\n## Training cost\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=14 \"Edit\\nsection: Training cost\")]\\n\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg/330px-\\nEstimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg)](/wiki/File:Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg)\\n\\nAdvances in software and hardware have reduced the cost substantially since\\n2020, such that in 2023 training of a 12-billion-parameter LLM computational\\ncost is 72,300 [A100-GPU](/wiki/Ampere_\\\\(microarchitecture\\\\) \"Ampere\\n\\\\(microarchitecture\\\\)\")-hours, while in 2020 the cost of training a\\n1.5-billion-parameter LLM (which was two orders of magnitude smaller than the\\nstate of the art in 2020) was between $80 thousand and $1.6\\nmillion.[51][52][53] Since 2020, large sums were invested in increasingly\\nlarge models. For example, training of the GPT-2 (i.e. a 1.5-billion-\\nparameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a\\n540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG\\n530B (in 2021) cost around $11 million.[54]\\n\\nFor Transformer-based LLM, training cost is much higher than inference cost.\\nIt costs 6 [FLOPs](/wiki/FLOPS \"FLOPS\") per parameter to train on one token,\\nwhereas it costs 1 to 2 FLOPs per parameter to infer on one token.[55]\\n\\n## Tool use\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=15 \"Edit\\nsection: Tool use\")]\\n\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at\\nleast not without the use of external tools or additional software. An example\\nof such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that\\nthe LLM has not already encountered a continuation of this calculation in its\\ntraining corpus.\\n[_[dubious](/wiki/Wikipedia:Accuracy_dispute#Disputed_statement\\n\"Wikipedia:Accuracy dispute\") -\\n[discuss](/wiki/Talk:Large_language_model#Dubious \"Talk:Large language\\nmodel\")_] In such cases, the LLM needs to resort to running program code that\\ncalculates the result, which can then be included in its response.\\n[_[dubious](/wiki/Wikipedia:Accuracy_dispute#Disputed_statement\\n\"Wikipedia:Accuracy dispute\") -\\n[discuss](/wiki/Talk:Large_language_model#Dubious \"Talk:Large language\\nmodel\")_]: Another example is \\'What is the time now? It is \\', where a separate\\nprogram interpreter would need to execute a code to get system time on the\\ncomputer, so LLM could include it in its reply.[56][57] This basic strategy\\ncan be sophisticated with multiple attempts of generated programs, and other\\nsampling strategies.[58]\\n\\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-\\nuse. If the number of tools is finite, then finetuning may be done just once.\\nIf the number of tools can grow arbitrarily, as with online [API](/wiki/API\\n\"API\") services, then the LLM can be fine-tuned to be able to read API\\ndocumentation and call API correctly.[59][60]\\n\\nA simpler form of tool use is [retrieval-augmented\\ngeneration](/wiki/Retrieval-augmented_generation \"Retrieval-augmented\\ngeneration\"): the augmentation of an LLM with [document\\nretrieval](/wiki/Document_retrieval \"Document retrieval\"). Given a query, a\\ndocument retriever is called to retrieve the most relevant documents. This is\\nusually done by encoding the query and the documents into vectors, then\\nfinding the documents with vectors (usually stored in a [vector\\ndatabase](/wiki/Vector_database \"Vector database\")) most similar to the vector\\nof the query. The LLM then generates an output based on both the query and\\ncontext included from the retrieved documents.[61]\\n\\n## Agency\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=16 \"Edit\\nsection: Agency\")]\\n\\nAn LLM is a language model, which is not an agent as it has no goal, but it\\ncan be used as a component of an [intelligent agent](/wiki/Intelligent_agent\\n\"Intelligent agent\").[62] Researchers have described several methods for such\\nintegrations.[_[citation needed](/wiki/Wikipedia:Citation_needed\\n\"Wikipedia:Citation needed\")_]\\n\\nThe [ReAct pattern](/w/index.php?title=ReAct_pattern&action=edit&redlink=1\\n\"ReAct pattern \\\\(page does not exist\\\\)\"), a portmanteau of \"Reason + Act\",\\nconstructs an [agent](/wiki/Intelligent_agent \"Intelligent agent\") out of an\\nLLM, using the LLM as a planner. The LLM is prompted to \"think out loud\".\\nSpecifically, the language model is prompted with a textual description of the\\nenvironment, a goal, a list of possible actions, and a record of the actions\\nand observations so far. It generates one or more thoughts before generating\\nan action, which is then executed in the environment.[63] The linguistic\\ndescription of the environment given to the LLM planner can even be the LaTeX\\ncode of a paper describing the environment.[64]\\n\\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first\\nconnected to the visual world via image descriptions, then it is prompted to\\nproduce plans for complex tasks and behaviors based on its pretrained\\nknowledge and environmental feedback it receives.[65]\\n\\nThe Reflexion method[66] constructs an agent that learns over multiple\\nepisodes. At the end of each episode, the LLM is given the record of the\\nepisode, and prompted to think up \"lessons learned\", which would help it\\nperform better at a subsequent episode. These \"lessons learned\" are given to\\nthe agent in the subsequent episodes.[_[citation\\nneeded](/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_]\\n\\n[Monte Carlo tree search](/wiki/Monte_Carlo_tree_search \"Monte Carlo tree\\nsearch\") can use an LLM as rollout heuristic. When a programmatic world model\\nis not available, an LLM can also be prompted with a description of the\\nenvironment to act as world model.[67]\\n\\nFor open-ended exploration, an LLM can be used to score observations for their\\n\"interestingness\", which can be used as a reward signal to guide a normal\\n(non-LLM) reinforcement learning agent.[68] Alternatively, it can [propose\\nincreasingly difficult tasks](/wiki/Zone_of_proximal_development \"Zone of\\nproximal development\") for [curriculum learning](/wiki/Curriculum_learning\\n\"Curriculum learning\").[69] Instead of outputting individual actions, an LLM\\nplanner can also construct \"skills\", or\\n[functions](/wiki/Function_\\\\(computer_programming\\\\) \"Function \\\\(computer\\nprogramming\\\\)\") for complex action sequences. The skills can be stored and\\nlater invoked, allowing increasing levels of abstraction in planning.[69]\\n\\nLLM-powered agents can keep a long-term memory of its previous contexts, and\\nthe memory can be retrieved in the same way as Retrieval Augmented Generation.\\nMultiple such agents can interact socially.[70]\\n\\n## Compression\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=17 \"Edit\\nsection: Compression\")]\\n\\nTypically, LLMs are trained with single- or half-precision floating point\\nnumbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one\\nbillion parameters require 2 gigabytes. The largest models typically have 100\\nbillion parameters, requiring 200 gigabytes to load, which places them outside\\nthe range of most consumer electronics.[71]\\n\\n_Post-training[quantization](/wiki/Quantization_\\\\(signal_processing\\\\)\\n\"Quantization \\\\(signal processing\\\\)\")_[72] aims to decrease the space\\nrequirement by lowering precision of the parameters of a trained model, while\\npreserving most of its performance.[73][74] The simplest form of quantization\\nsimply truncates all numbers to a given number of bits. It can be improved by\\nusing a different quantization [codebook](/wiki/Block_cipher \"Block cipher\")\\nper layer. Further improvement can be done by applying [different\\nprecisions](/wiki/Mixed-precision_arithmetic \"Mixed-precision arithmetic\") to\\ndifferent parameters, with higher precision for particularly important\\nparameters (\"outlier weights\").[75] See [76] for a visual guide.\\n\\nWhile quantized models are typically frozen, and only pre-quantized models are\\nfine-tuned, quantized models can still be fine-tuned.[77]\\n\\n## Multimodality\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=18 \"Edit\\nsection: Multimodality\")]\\n\\nSee also: [Multimodal learning](/wiki/Multimodal_learning \"Multimodal\\nlearning\")\\n\\nMultimodality means \"having several modalities\", and a\\n[\"modality\"](/wiki/Modality_\\\\(human%E2%80%93computer_interaction\\\\) \"Modality\\n\\\\(human–computer interaction\\\\)\") refers to a type of input or output, such as\\nvideo, image, audio, text, [proprioception](/wiki/Proprioception\\n\"Proprioception\"), etc.[78] There have been many AI models trained\\nspecifically to ingest one modality and output another modality, such as\\n[AlexNet](/wiki/AlexNet \"AlexNet\") for image to label,[79] [visual question\\nanswering](/wiki/Visual_question_answering \"Visual question answering\") for\\nimage-text to text,[80] and [speech recognition](/wiki/Speech_recognition\\n\"Speech recognition\") for speech to text.\\n\\nA common method to create multimodal models out of an LLM is to \"tokenize\" the\\noutput of a trained encoder. Concretely, one can construct an LLM that can\\nunderstand images as follows: take a trained LLM, and take a trained image\\nencoder  E {\\\\displaystyle E} ![{\\\\\\\\displaystyle\\nE}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b).\\nMake a small multilayered perceptron  f {\\\\displaystyle f} ![{\\\\\\\\displaystyle\\nf}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61),\\nso that for any image  y {\\\\displaystyle y} ![{\\\\\\\\displaystyle\\ny}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d),\\nthe post-processed vector  f ( E ( y ) ) {\\\\displaystyle f(E(y))}\\n![{\\\\\\\\displaystyle\\nf\\\\(E\\\\(y\\\\)\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8d41d0ec0611a795f65ea14a43b8016462703a8e)\\nhas the same dimensions as an encoded token. That is an \"image token\". Then,\\none can interleave text tokens and image tokens. The compound model is then\\nfine-tuned on an image-text dataset. This basic construction can be applied\\nwith more sophistication to improve the model. The image encoder may be frozen\\nto improve stability.[81]\\n\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning\\na pair of pretrained language model and image encoder to perform better on\\nvisual question answering than models trained from scratch.[82] [Google\\nPaLM](/wiki/Pathways_Language_Model \"Pathways Language Model\") model was fine-\\ntuned into a multimodal model PaLM-E using the tokenization method, and\\napplied to robotic control.[83] [LLaMA](/wiki/LLaMA \"LLaMA\") models have also\\nbeen turned multimodal using the tokenization method, to allow image\\ninputs,[84] and video inputs.[85]\\n\\n[GPT-4](/wiki/GPT-4 \"GPT-4\") can use both text and image as inputs[86]\\n(although the vision component was not released to the public until\\nGPT-4V[87]); [Google DeepMind](/wiki/Google_DeepMind \"Google DeepMind\")\\'s\\n[Gemini](/wiki/Gemini_\\\\(language_model\\\\) \"Gemini \\\\(language model\\\\)\") is also\\nmultimodal.[88] Mistral introduced its own multimodel Pixtral 12B model in\\nSeptember 2024.[89]\\n\\n## Properties\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=19 \"Edit\\nsection: Properties\")]\\n\\n### Scaling laws\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=20 \"Edit\\nsection: Scaling laws\")]\\n\\nMain article: [Neural scaling law](/wiki/Neural_scaling_law \"Neural scaling\\nlaw\")\\n\\nThe following four hyper-parameters characterize an LLM:\\n\\n  * cost of (pre-)training ( C {\\\\displaystyle C} ![{\\\\\\\\displaystyle C}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029)),\\n  * size of the [artificial neural network](/wiki/Artificial_neural_network \"Artificial neural network\") itself, such as number of parameters  N {\\\\displaystyle N} ![{\\\\\\\\displaystyle N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3) (i.e. amount of neurons in its layers, amount of weights between them and biases),\\n  * size of its (pre-)training dataset (i.e. number of tokens in corpus,  D {\\\\displaystyle D} ![{\\\\\\\\displaystyle D}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6)),\\n  * performance after (pre-)training.\\n\\nThey are related by simple [statistical laws](/wiki/Empirical_statistical_laws\\n\"Empirical statistical laws\"), called \"scaling laws\". One particular scaling\\nlaw (\"[Chinchilla scaling](/wiki/Chinchilla_AI \"Chinchilla AI\")\") for LLM\\nautoregressively trained for one epoch, with a [log-log](/wiki/Log-log_plot\\n\"Log-log plot\") [learning rate](/wiki/Learning_rate \"Learning rate\") schedule,\\nstates that:[90] { C = C 0 N D L = A N α + B D β + L 0 {\\\\displaystyle\\n{\\\\begin{cases}C=C_{0}ND\\\\\\\\\\\\\\\\[6pt]L={\\\\frac {A}{N^{\\\\alpha }}}+{\\\\frac {B}{D^{\\\\beta\\n}}}+L_{0}\\\\end{cases}}} ![{\\\\\\\\displaystyle\\n{\\\\\\\\begin{cases}C=C_{0}ND\\\\\\\\\\\\\\\\\\\\[6pt\\\\]L={\\\\\\\\frac {A}{N^{\\\\\\\\alpha }}}+{\\\\\\\\frac\\n{B}{D^{\\\\\\\\beta\\n}}}+L_{0}\\\\\\\\end{cases}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/39435f4ecd5e00c0714a4f7f71cc0b91f5973cdd)\\nwhere the variables are\\n\\n  * C {\\\\displaystyle C} ![{\\\\\\\\displaystyle C}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029) is the cost of training the model, in [FLOPs](/wiki/FLOPS \"FLOPS\").\\n  * N {\\\\displaystyle N} ![{\\\\\\\\displaystyle N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3) is the number of parameters in the model.\\n  * D {\\\\displaystyle D} ![{\\\\\\\\displaystyle D}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6) is the number of tokens in the training set.\\n  * L {\\\\displaystyle L} ![{\\\\\\\\displaystyle L}](https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8) is the average negative log-likelihood loss per token ([nats](/wiki/Nat_\\\\(unit\\\\) \"Nat \\\\(unit\\\\)\")/token), achieved by the trained LLM on the test dataset.\\n\\nand the statistical hyper-parameters are\\n\\n  * C 0 = 6 {\\\\displaystyle C_{0}=6} ![{\\\\\\\\displaystyle C_{0}=6}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b05c98b1743f05e046a3f3bb0a966fa898e431e2), meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[55]\\n  * α = 0.34 , β = 0.28 , A = 406.4 , B = 410.7 , L 0 = 1.69 {\\\\displaystyle \\\\alpha =0.34,\\\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69} ![{\\\\\\\\displaystyle \\\\\\\\alpha =0.34,\\\\\\\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}](https://wikimedia.org/api/rest_v1/media/math/render/svg/848b6d78d881ed6da8d6b60e8d788bc799525401)\\n\\n### Emergent abilities\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=21 \"Edit\\nsection: Emergent abilities\")]\\n\\n[![](//upload.wikimedia.org/wikipedia/commons/thumb/5/57/LLM_emergent_benchmarks.png/220px-\\nLLM_emergent_benchmarks.png)](/wiki/File:LLM_emergent_benchmarks.png)At\\npoint(s) referred to as\\n[breaks](/wiki/Neural_scaling_law#Broken_Neural_Scaling_Laws_\\\\(BNSL\\\\) \"Neural\\nscaling law\"),[91] the lines change their slopes, appearing on a linear-log\\nplot as a series of linear segments connected by arcs.\\n\\nPerformance of bigger models on various tasks, when plotted on a log-log\\nscale, appears as a linear extrapolation of performance achieved by smaller\\nmodels. However, this linearity may be punctuated by\\n\"[break(s)](/wiki/Neural_scaling_law#Broken_Neural_Scaling_Laws_\\\\(BNSL\\\\)\\n\"Neural scaling law\")\"[91] in the scaling law, where the slope of the line\\nchanges abruptly, and where larger models acquire \"emergent\\nabilities\".[40][92] They arise from the complex interaction of the model\\'s\\ncomponents and are not explicitly programmed or designed.[93]\\n\\nThe most intriguing among emergent abilities is [in-context\\nlearning](/wiki/In-context_learning \"In-context learning\") from example\\ndemonstrations.[94] In-context learning is involved in tasks, such as:\\n\\n  * reported arithmetics, decoding the [International Phonetic Alphabet](/wiki/International_Phonetic_Alphabet \"International Phonetic Alphabet\"), unscrambling a word\\'s letters, disambiguate word in context,[40][95][96] converting spatial words, [cardinal directions](/wiki/Cardinal_direction \"Cardinal direction\") (for example, replying \"northeast\" upon [0, 0, 1; 0, 0, 0; 0, 0, 0]), color terms represented in text.[97]\\n  * [chain-of-thought prompting](/wiki/Chain-of-thought_prompting \"Chain-of-thought prompting\"): Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.[98]\\n  * identifying offensive content in paragraphs of [Hinglish](/wiki/Hinglish \"Hinglish\") (a combination of Hindi and English), and generating a similar English equivalent of [Kiswahili](/wiki/Kiswahili \"Kiswahili\") proverbs.[99]\\n\\nSchaeffer _et. al._ argue that the emergent abilities are not unpredictably\\nacquired, but predictably acquired according to a [smooth scaling\\nlaw](/wiki/Neural_scaling_law \"Neural scaling law\"). The authors considered a\\ntoy statistical model of an LLM solving multiple-choice questions, and showed\\nthat this statistical model, modified to account for other types of tasks,\\napplies to these tasks as well.[100]\\n\\nLet  x {\\\\displaystyle x} ![{\\\\\\\\displaystyle\\nx}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4)\\nbe the number of parameter count, and  y {\\\\displaystyle y} ![{\\\\\\\\displaystyle\\ny}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d)\\nbe the performance of the model.\\n\\n  * When  y = average Pr ( correct token ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{correct token}})} ![{\\\\\\\\displaystyle y={\\\\\\\\text{average }}\\\\\\\\Pr\\\\({\\\\\\\\text{correct token}}\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/25f87a1a04b7eb97aca02ae9170ae7f05e308bd4), then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} ![{\\\\\\\\displaystyle \\\\(\\\\\\\\log x,y\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1dccdbdb2af7f930d3fff961d7f76540706bbaf8) is an exponential curve (before it hits the plateau at one), which looks like emergence.\\n  * When  y = average log \\u2061 ( Pr ( correct token ) ) {\\\\displaystyle y={\\\\text{average }}\\\\log(\\\\Pr({\\\\text{correct token}}))} ![{\\\\\\\\displaystyle y={\\\\\\\\text{average }}\\\\\\\\log\\\\(\\\\\\\\Pr\\\\({\\\\\\\\text{correct token}}\\\\)\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c22c18197c1091afcb5ed896ba90b8429af1c861), then the  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} ![{\\\\\\\\displaystyle \\\\(\\\\\\\\log x,y\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1dccdbdb2af7f930d3fff961d7f76540706bbaf8) plot is a straight line (before it hits the plateau at zero), which does not look like emergence.\\n  * When  y = average Pr ( the most likely token is correct ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{the most likely token is correct}})} ![{\\\\\\\\displaystyle y={\\\\\\\\text{average }}\\\\\\\\Pr\\\\({\\\\\\\\text{the most likely token is correct}}\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6028c3484d3fbd36ffdc2cad41ff60ba9f8c1e7a), then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} ![{\\\\\\\\displaystyle \\\\(\\\\\\\\log x,y\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1dccdbdb2af7f930d3fff961d7f76540706bbaf8) is a step-function, which looks like emergence.\\n\\n## Interpretation\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=22 \"Edit\\nsection: Interpretation\")]\\n\\nLarge language models by themselves are \"[black boxes](/wiki/Black_box \"Black\\nbox\")\", and it is not clear how they can perform linguistic tasks. There are\\nseveral methods for understanding how LLM work.\\n\\nMechanistic interpretability aims to [reverse-\\nengineer](/wiki/Reverse_engineering \"Reverse engineering\") LLM by discovering\\nsymbolic algorithms that approximate the inference performed by LLM. One\\nexample is Othello-GPT, where a small Transformer is trained to predict legal\\n[Othello](/wiki/Reversi \"Reversi\") moves. It is found that there is a linear\\nrepresentation of Othello board, and modifying the representation changes the\\npredicted legal Othello moves in the correct way.[101][102] In another\\nexample, a small Transformer is trained on [Karel\\nprograms](/wiki/Karel_\\\\(programming_language\\\\) \"Karel \\\\(programming\\nlanguage\\\\)\"). Similar to the Othello-GPT example, there is a linear\\nrepresentation of Karel program semantics, and modifying the representation\\nchanges output in the correct way. The model also generates correct programs\\nthat are on average shorter than those in the training set.[103]\\n\\nIn another example, the authors trained small transformers on [modular\\narithmetic addition](/wiki/Modular_arithmetic \"Modular arithmetic\"). The\\nresulting models were reverse-engineered, and it turned out they used\\n[discrete Fourier transform](/wiki/Discrete_Fourier_transform \"Discrete\\nFourier transform\").[104]\\n\\n### Understanding and intelligence\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=23 \"Edit\\nsection: Understanding and intelligence\")]\\n\\nSee also: [Mind–body dualism](/wiki/Mind%E2%80%93body_dualism \"Mind–body\\ndualism\")\\n\\nNLP researchers were evenly split when asked, in a 2022 survey, whether\\n(untuned) LLMs \"could (ever) understand natural language in some nontrivial\\nsense\".[105] Proponents of \"LLM understanding\" believe that some LLM\\nabilities, such as mathematical reasoning, imply an ability to\\n[\"understand\"](/wiki/Natural_language_understanding \"Natural language\\nunderstanding\") certain concepts. A Microsoft team argued in 2023 that GPT-4\\n\"can solve novel and difficult tasks that span mathematics, coding, vision,\\nmedicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed\\nas an early (yet still incomplete) version of an [artificial general\\nintelligence](/wiki/Artificial_general_intelligence \"Artificial general\\nintelligence\") system\": \"Can one reasonably say that a system that passes\\nexams for software engineering candidates is not _really_\\nintelligent?\"[106][107] Some researchers characterize LLMs as \"alien\\nintelligence\".[108][109] For example, Conjecture CEO Connor Leahy considers\\nuntuned LLMs to be like inscrutable alien \"[Shoggoths](/wiki/Shoggoth\\n\"Shoggoth\")\", and believes that RLHF tuning creates a \"smiling facade\"\\nobscuring the inner workings of the LLM: \"If you don\\'t push it too far, the\\nsmiley face stays on. But then you give it [an unexpected] prompt, and\\nsuddenly you see this massive underbelly of insanity, of weird thought\\nprocesses and clearly non-human understanding.\"[110][111]\\n\\nIn contrast, some proponents of the \"LLMs lack understanding\" school believe\\nthat existing LLMs are \"simply remixing and recombining existing\\nwriting\",[109] a phenomenon known as [stochastic\\nparrot](/wiki/Stochastic_parrot \"Stochastic parrot\"), or they point to the\\ndeficits existing LLMs continue to have in prediction skills, reasoning\\nskills, agency, and explainability.[105] For example, GPT-4 has natural\\ndeficits in planning and in real-time learning.[107] Generative LLMs have been\\nobserved to confidently assert claims of fact which do not seem to be\\n[justified](/wiki/Justification_\\\\(epistemology\\\\) \"Justification\\n\\\\(epistemology\\\\)\") by their [training data](/wiki/Training_data \"Training\\ndata\"), a phenomenon which has been termed\\n\"[hallucination](/wiki/Hallucination_\\\\(artificial_intelligence\\\\)\\n\"Hallucination \\\\(artificial intelligence\\\\)\")\".[112] Specifically,\\nhallucinations in the context of LLMs correspond to the generation of text or\\nresponses that seem syntactically sound, fluent, and natural but are factually\\nincorrect, nonsensical, or unfaithful to the provided source input.[113]\\nNeuroscientist [Terrence Sejnowski](/wiki/Terrence_Sejnowski \"Terrence\\nSejnowski\") has argued that \"The diverging opinions of experts on the\\nintelligence of LLMs suggests that our old ideas based on natural intelligence\\nare inadequate\".[105]\\n\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main\\naspects – the first is how to model thought and language in a computer system,\\nand the second is how to enable the computer system to generate human like\\nlanguage.[105] These aspects of language as a model of\\n[cognition](/wiki/Cognition \"Cognition\") have been developed in the field of\\n[cognitive linguistics](/wiki/Cognitive_linguistics \"Cognitive linguistics\").\\nAmerican linguist [George Lakoff](/wiki/George_Lakoff \"George Lakoff\")\\npresented Neural Theory of Language (NTL)[114] as a [computational\\nbasis](/wiki/Cognitive_linguistics#Computational_approaches \"Cognitive\\nlinguistics\") for using language as a model of learning tasks and\\nunderstanding. [The NTL\\nModel](https://www.icsi.berkeley.edu/icsi/projects/ai/ntl) outlines how\\nspecific neural structures of the human brain shape the nature of thought and\\nlanguage and in turn what are the computational properties of such neural\\nsystems that can be applied to model thought and language in a computer\\nsystem. After a framework for modeling language in a computer systems was\\nestablished, the focus shifted to establishing frameworks for computer systems\\nto generate language with acceptable grammar. In his 2014 book titled _[The\\nLanguage Myth: Why Language Is Not An Instinct](/wiki/The_Language_Myth \"The\\nLanguage Myth\")_ , British cognitive linguist and digital communication\\ntechnologist [Vyvyan Evans](/wiki/Vyvyan_Evans \"Vyvyan Evans\") mapped out the\\nrole of [probabilistic context-free grammar](/wiki/Probabilistic_context-\\nfree_grammar \"Probabilistic context-free grammar\") (PCFG) in enabling [NLP to\\nmodel cognitive patterns](/wiki/Natural_language_processing#Cognition \"Natural\\nlanguage processing\") and generate human like language.[115][116]\\n\\n## Evaluation\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=24 \"Edit\\nsection: Evaluation\")]\\n\\n### Perplexity\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=25 \"Edit\\nsection: Perplexity\")]\\n\\nThe most commonly used measure of a language model\\'s performance is its\\n[perplexity](/wiki/Perplexity \"Perplexity\") on a given text corpus. Perplexity\\nis a measure of how well a model is able to predict the contents of a dataset;\\nthe higher the likelihood the model assigns to the dataset, the lower the\\nperplexity. Mathematically, perplexity is defined as the exponential of the\\naverage negative log likelihood per token: log \\u2061 ( Perplexity ) = − 1 N ∑ i =\\n1 N log \\u2061 ( Pr ( token i ∣ context for token i ) ) {\\\\displaystyle\\n\\\\log({\\\\text{Perplexity}})=-{\\\\frac {1}{N}}\\\\sum\\n_{i=1}^{N}\\\\log(\\\\Pr({\\\\text{token}}_{i}\\\\mid {\\\\text{context for token}}_{i}))}\\n![{\\\\\\\\displaystyle \\\\\\\\log\\\\({\\\\\\\\text{Perplexity}}\\\\)=-{\\\\\\\\frac {1}{N}}\\\\\\\\sum\\n_{i=1}^{N}\\\\\\\\log\\\\(\\\\\\\\Pr\\\\({\\\\\\\\text{token}}_{i}\\\\\\\\mid {\\\\\\\\text{context for\\ntoken}}_{i}\\\\)\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/556393708767666076b9723412bc8519284449a5)here\\nN {\\\\displaystyle N} ![{\\\\\\\\displaystyle\\nN}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3)\\nis the number of tokens in the text corpus, and \"context for token  i\\n{\\\\displaystyle i} ![{\\\\\\\\displaystyle\\ni}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\"\\ndepends on the specific type of LLM used. If the LLM is autoregressive, then\\n\"context for token  i {\\\\displaystyle i} ![{\\\\\\\\displaystyle\\ni}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\"\\nis the segment of text appearing before token  i {\\\\displaystyle i}\\n![{\\\\\\\\displaystyle\\ni}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20).\\nIf the LLM is masked, then \"context for token  i {\\\\displaystyle i}\\n![{\\\\\\\\displaystyle\\ni}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\"\\nis the segment of text surrounding token  i {\\\\displaystyle i}\\n![{\\\\\\\\displaystyle\\ni}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20).\\n\\nBecause language models may [overfit](/wiki/Overfit \"Overfit\") to their\\ntraining data, models are usually evaluated by their perplexity on a [test\\nset](/wiki/Test_set \"Test set\") of unseen data.[47] This presents particular\\nchallenges for the evaluation of large language models. As they are trained on\\nincreasingly large corpora of text largely scraped from the web, it becomes\\nincreasingly likely that models\\' training data inadvertently includes portions\\nof any given test set.[2]\\n\\n#### BPW, BPC, and BPT\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=26 \"Edit\\nsection: BPW, BPC, and BPT\")]\\n\\nIn [information theory](/wiki/Information_theory \"Information theory\"), the\\nconcept of [entropy](/wiki/Entropy_\\\\(information_theory\\\\) \"Entropy\\n\\\\(information theory\\\\)\") is intricately linked to perplexity, a relationship\\nnotably established by [Claude Shannon](/wiki/Claude_Shannon \"Claude\\nShannon\").[117] This relationship is mathematically expressed as  Entropy =\\nlog 2 \\u2061 ( Perplexity ) {\\\\displaystyle {\\\\text{Entropy}}=\\\\log\\n_{2}({\\\\text{Perplexity}})} ![{\\\\\\\\displaystyle {\\\\\\\\text{Entropy}}=\\\\\\\\log\\n_{2}\\\\({\\\\\\\\text{Perplexity}}\\\\)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/462f40a6811ee57670d1735c452d04be85a82c57).\\n\\nEntropy, in this context, is commonly quantified in terms of bits per word\\n(BPW) or bits per character (BPC), which hinges on whether the language model\\nutilizes word-based or character-based tokenization.\\n\\nNotably, in the case of larger language models that predominantly employ sub-\\nword tokenization, bits per token (BPT) emerges as a seemingly more\\nappropriate measure. However, due to the variance in tokenization methods\\nacross different Large Language Models (LLMs), BPT does not serve as a\\nreliable metric for comparative analysis among diverse models. To convert BPT\\ninto BPW, one can multiply it by the average number of tokens per word.\\n\\nIn the evaluation and comparison of language models, [cross-\\nentropy](/wiki/Cross-entropy \"Cross-entropy\") is generally the preferred\\nmetric over entropy. The underlying principle is that a lower BPW is\\nindicative of a model\\'s enhanced capability for compression. This, in turn,\\nreflects the model\\'s proficiency in making accurate predictions.\\n\\n### Task-specific datasets and benchmarks\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=27 \"Edit\\nsection: Task-specific datasets and benchmarks\")]\\n\\nA large number of testing datasets and\\n[benchmarks](/wiki/Benchmark_\\\\(computing\\\\) \"Benchmark \\\\(computing\\\\)\") have\\nalso been developed to evaluate the capabilities of language models on more\\nspecific downstream tasks. Tests may be designed to evaluate a variety of\\ncapabilities, including general knowledge, [commonsense\\nreasoning](/wiki/Commonsense_reasoning \"Commonsense reasoning\"), and\\nmathematical problem-solving.\\n\\nOne broad category of evaluation dataset is question answering datasets,\\nconsisting of pairs of questions and correct answers, for example, (\"Have the\\nSan Jose Sharks won the Stanley Cup?\", \"No\").[118] A question answering task\\nis considered \"open book\" if the model\\'s prompt includes text from which the\\nexpected answer can be derived (for example, the previous question could be\\nadjoined with some text which includes the sentence \"The Sharks have advanced\\nto the Stanley Cup finals once, losing to the Pittsburgh Penguins in\\n2016.\"[118]). Otherwise, the task is considered \"closed book\", and the model\\nmust draw on knowledge retained during training.[119] Some examples of\\ncommonly used question answering datasets include TruthfulQA, Web Questions,\\nTriviaQA, and SQuAD.[119]\\n\\nEvaluation datasets may also take the form of text completion, having the\\nmodel select the most likely word or sentence to complete a prompt, for\\nexample: \"Alice was friends with Bob. Alice went to visit her friend,\\n____\".[2]\\n\\nSome composite benchmarks have also been developed which combine a diversity\\nof different evaluation datasets and tasks. Examples include GLUE, SuperGLUE,\\n[MMLU](/wiki/MMLU \"MMLU\"), BIG-bench, and HELM.[117][119] OpenAI has released\\ntools for running composite benchmarks, but noted that the eval results are\\nsensitive to the prompting method.[120][121] Some public datasets contain\\nquestions that are mislabeled, ambiguous, unanswerable, or otherwise of low-\\nquality, which can be cleaned to give more reliable benchmark scores.[122]\\n\\nIt was previously standard to report results on a heldout portion of an\\nevaluation dataset after doing supervised fine-tuning on the remainder. It is\\nnow more common to evaluate a pre-trained model directly through prompting\\ntechniques, though researchers vary in the details of how they formulate\\nprompts for particular tasks, particularly with respect to how many examples\\nof solved tasks are adjoined to the prompt (i.e. the value of _n_ in _n_ -shot\\nprompting).\\n\\n#### Adversarially constructed evaluations\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=28 \"Edit\\nsection: Adversarially constructed evaluations\")]\\n\\nBecause of the rapid pace of improvement of large language models, evaluation\\nbenchmarks have suffered from short lifespans, with state of the art models\\nquickly \"saturating\" existing benchmarks, exceeding the performance of human\\nannotators, leading to efforts to replace or augment the benchmark with more\\nchallenging tasks.[123] In addition, there are cases of \"shortcut learning\"\\nwherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical\\ncorrelations in superficial test question wording in order to guess the\\ncorrect responses, without necessarily understanding the actual question being\\nasked.[105]\\n\\nSome datasets have been constructed adversarially, focusing on particular\\nproblems on which extant language models seem to have unusually poor\\nperformance compared to humans. One example is the TruthfulQA dataset, a\\nquestion answering dataset consisting of 817 questions which language models\\nare susceptible to answering incorrectly by mimicking falsehoods to which they\\nwere repeatedly exposed during training. For example, an LLM may answer \"No\"\\nto the question \"Can you teach an old dog new tricks?\" because of its exposure\\nto the English idiom _[you can\\'t teach an old dog new\\ntricks](https://en.wiktionary.org/wiki/you_can%27t_teach_an_old_dog_new_tricks\\n\"wikt:you can\\'t teach an old dog new tricks\")_ , even though this is not\\nliterally true.[124]\\n\\nAnother example of an adversarial evaluation dataset is Swag and its\\nsuccessor, HellaSwag, collections of problems in which one of multiple options\\nmust be selected to complete a text passage. The incorrect completions were\\ngenerated by sampling from a language model and filtering with a set of\\nclassifiers. The resulting problems are trivial for humans but at the time the\\ndatasets were created state of the art language models had poor accuracy on\\nthem. For example:\\n\\n> We see a fitness center sign. We then see a man talking to the camera and\\n> sitting and laying on a exercise ball. The man...  \\n> a) demonstrates how to increase efficient exercise work by running up and\\n> down balls.  \\n> b) moves all his arms and legs and builds up a lot of muscle.  \\n> c) then plays the ball and we see a graphics and hedge trimming\\n> demonstration.  \\n> d) performs sit ups while on the ball and talking.[125]\\n\\n[BERT](/wiki/BERT_\\\\(language_model\\\\) \"BERT \\\\(language model\\\\)\") selects b) as\\nthe most likely completion, though the correct answer is d).[125]\\n\\n## Wider impact\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=29 \"Edit\\nsection: Wider impact\")]\\n\\nIn 2023, _[Nature Biomedical Engineering](/wiki/Nature_Biomedical_Engineering\\n\"Nature Biomedical Engineering\")_ wrote that \"it is no longer possible to\\naccurately distinguish\" human-written text from text created by large language\\nmodels, and that \"It is all but certain that general-purpose large language\\nmodels will rapidly proliferate... It is a rather safe bet that they will\\nchange many industries over time.\"[126] [Goldman Sachs](/wiki/Goldman_Sachs\\n\"Goldman Sachs\") suggested in 2023 that generative language AI could increase\\nglobal GDP by 7% in the next ten years, and could expose to automation 300\\nmillion jobs globally.[127][128]\\n\\n### Memorization and copyright\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=30 \"Edit\\nsection: Memorization and copyright\")]\\n\\nFurther information: [Artificial intelligence and\\ncopyright](/wiki/Artificial_intelligence_and_copyright \"Artificial\\nintelligence and copyright\")\\n\\nMemorization is an emergent behavior in LLMs in which long strings of text are\\noccasionally output verbatim from training data, contrary to typical behavior\\nof traditional artificial neural nets. Evaluations of controlled LLM output\\nmeasure the amount memorized from training data (focused on GPT-2-series\\nmodels) as variously over 1% for exact duplicates[129] or up to about 7%.[130]\\n\\n### Security\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=31 \"Edit\\nsection: Security\")]\\n\\nSome commenters expressed concern over accidental or deliberate creation of\\nmisinformation, or other forms of misuse.[131] For example, the availability\\nof large language models could reduce the skill-level required to commit\\nbioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM\\ncreators should exclude from their training data papers on creating or\\nenhancing pathogens.[132]\\n\\nA study by researchers at Google and several universities, including [Cornell\\nUniversity](/wiki/Cornell_University \"Cornell University\") and [University of\\nCalifornia, Berkeley](/wiki/University_of_California,_Berkeley \"University of\\nCalifornia, Berkeley\"), showed that there are potential security risks in\\nlanguage models such as [ChatGPT](/wiki/ChatGPT \"ChatGPT\"). In their study,\\nthey examined and confirmed the possibility that questioners could get, from\\nChatGPT, the training data that the AI model used. For example, when asking\\nChatGPT 3.5 turbo to repeat the word \"poem\" forever, the AI model will say\\n\"poem\" hundreds of times and then diverge, deviating from the standard\\ndialogue style and spitting out nonsense phrases, thus spitting out the\\ntraining data as it is. The researchers have seen more than 10,000 examples of\\nthe AI model exposing their training data in a similar method. The researchers\\nsaid that it was hard to tell if the AI model was actually safe or not.[133]\\n\\nThe potential presence of \"sleeper agents\" within LLM models is another\\nemerging security concern. These are hidden functionalities built into the\\nmodel that remain dormant until triggered by a specific event or condition.\\nUpon activation, the LLM deviates from its expected behavior to make insecure\\nactions.[134]\\n\\nLarge language model (LLM) applications accessible to the public, like ChatGPT\\nor Claude, typically incorporate safety measures designed to filter out\\nharmful content. However, implementing these controls effectively has proven\\nchallenging. For instance, research by Kang et al. [135] demonstrated a method\\nfor circumventing LLM safety systems. Similarly, Wang [136] illustrated how a\\npotential criminal could potentially bypass ChatGPT 4o\\'s safety controls to\\nobtain information on establishing a drug trafficking operation.\\n\\n### Algorithmic bias\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=32 \"Edit\\nsection: Algorithmic bias\")]\\n\\nMain article: [Algorithmic bias](/wiki/Algorithmic_bias \"Algorithmic bias\")\\n\\nWhile LLMs have shown remarkable capabilities in generating human-like text,\\nthey are susceptible to inheriting and amplifying biases present in their\\ntraining data. This can manifest in skewed representations or unfair treatment\\nof different demographics, such as those based on race, gender, language, and\\ncultural groups.[137] Since English data is overrepresented in current large\\nlanguage models\\' training data, it may also downplay non-English views.[138]\\n\\n#### Stereotyping\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=33 \"Edit\\nsection: Stereotyping\")]\\n\\nAI models can reinforce a wide range of stereotypes, including those based on\\ngender, ethnicity, age, nationality, religion, or occupation. This can lead to\\noutputs that unfairly generalize or caricature groups of people, sometimes in\\nharmful or derogatory ways.[139]\\n\\nNotably, gender bias refers to the tendency of these models to produce outputs\\nthat are unfairly prejudiced towards one gender over another. This bias\\ntypically arises from the data on which these models are trained. Large\\nlanguage models often assign roles and characteristics based on traditional\\ngender norms.[137] For example, it might associate nurses or secretaries\\npredominantly with women and engineers or CEOs with men.[140]\\n\\n#### Political bias\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=34 \"Edit\\nsection: Political bias\")]\\n\\nPolitical bias refers to the tendency of algorithms to systematically favor\\ncertain political viewpoints, ideologies, or outcomes over others. Language\\nmodels may also exhibit political biases. Since the training data includes a\\nwide range of political opinions and coverage, the models might generate\\nresponses that lean towards particular political ideologies or viewpoints,\\ndepending on the prevalence of those views in the data.[141]\\n\\n## List of Large Language Models\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=35 \"Edit\\nsection: List of Large Language Models\")]\\n\\nSee also: [List of chatbots](/wiki/List_of_chatbots \"List of chatbots\")\\n\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day =\\n8.64E19 FLOP. Also, only the largest model\\'s cost is written.\\n\\nName | Release date[a] | Developer | Number of parameters (billion) [b] | Corpus size  | Training cost (petaFLOP-day) | License[c] | Notes   \\n---|---|---|---|---|---|---|---  \\n[GPT-1](/wiki/GPT-1 \"GPT-1\") | June 2018 | [OpenAI](/wiki/OpenAI \"OpenAI\") | 0.117 |  | 1[142] | MIT[143] | First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 [GPUs](/wiki/Graphics_processing_unit \"Graphics processing unit\").   \\n[BERT](/wiki/BERT_\\\\(language_model\\\\) \"BERT \\\\(language model\\\\)\") | October 2018 | [Google](/wiki/Google \"Google\") | 0.340[144] | 3.3 billion words[144] | 9[145] | Apache 2.0[146] | An early and influential language model.[4] [Encoder-only](/wiki/Transformer_\\\\(deep_learning_architecture\\\\)#encoder-only \"Transformer \\\\(deep learning architecture\\\\)\") and thus not built to be prompted or generative.[147] Training took 4 days on 64 TPUv2 chips.[148]  \\n[T5](/wiki/T5_\\\\(language_model\\\\) \"T5 \\\\(language model\\\\)\") | October 2019 | Google  | 11[149] | 34 billion tokens[149] |  | Apache 2.0[150] | Base model for many Google projects, such as Imagen.[151]  \\n[XLNet](/wiki/XLNet \"XLNet\") | June 2019 | [Google](/wiki/Google \"Google\") | 0.340[152] | 33 billion words  | 330 | Apache 2.0[153] | An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[154]  \\n[GPT-2](/wiki/GPT-2 \"GPT-2\") | February 2019 | [OpenAI](/wiki/OpenAI \"OpenAI\") | 1.5[155] | 40GB[156] (~10 billion tokens)[157] | 28[158] | MIT[159] | Trained on 32 TPUv3 chips for 1 week.[158]  \\n[GPT-3](/wiki/GPT-3 \"GPT-3\") | May 2020 | OpenAI | 175[51] | 300 billion tokens[157] | 3640[160] | proprietary  | A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called [ChatGPT](/wiki/ChatGPT \"ChatGPT\") in 2022.[161]  \\nGPT-Neo | March 2021 | [EleutherAI](/wiki/EleutherAI \"EleutherAI\") | 2.7[162] | 825 GiB[163] |  | MIT[164] | The first of [a series of free GPT-3 alternatives](/wiki/EleutherAI#GPT_models \"EleutherAI\") released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[164]  \\n[GPT-J](/wiki/GPT-J \"GPT-J\") | June 2021 | [EleutherAI](/wiki/EleutherAI \"EleutherAI\") | 6[165] | 825 GiB[163] | 200[166] | Apache 2.0  | GPT-3-style language model   \\nMegatron-Turing NLG | October 2021[167] | [Microsoft](/wiki/Microsoft \"Microsoft\") and [Nvidia](/wiki/Nvidia \"Nvidia\") | 530[168] | 338.6 billion tokens[168] | 38000[169] | Restricted web access  | Trained for 3 months on over 2000 A100 GPUs on the NVIDIA [Selene Supercomputer](/wiki/Selene_\\\\(supercomputer\\\\) \"Selene \\\\(supercomputer\\\\)\"), for over 3 million GPU-hours.[169]  \\nErnie 3.0 Titan | December 2021 | [Baidu](/wiki/Baidu \"Baidu\") | 260[170] | 4 Tb  |  | Proprietary  | Chinese-language LLM. [Ernie Bot](/wiki/Ernie_Bot \"Ernie Bot\") is based on this model.   \\n[Claude](/wiki/Claude_\\\\(language_model\\\\) \"Claude \\\\(language model\\\\)\")[171] | December 2021 | [Anthropic](/wiki/Anthropic \"Anthropic\") | 52[172] | 400 billion tokens[172] |  | beta  | Fine-tuned for desirable behavior in conversations.[173]  \\nGLaM (Generalist Language Model) | December 2021 | Google | 1200[39] | 1.6 trillion tokens[39] | 5600[39] | Proprietary  | Sparse [mixture of experts](/wiki/Mixture_of_experts \"Mixture of experts\") model, making it more expensive to train but cheaper to run inference compared to GPT-3.   \\nGopher | December 2021 | [DeepMind](/wiki/DeepMind \"DeepMind\") | 280[174] | 300 billion tokens[175] | 5833[176] | Proprietary  | Later developed into the Chinchilla model.   \\n[LaMDA](/wiki/LaMDA \"LaMDA\") (Language Models for Dialog Applications) | January 2022 | Google | 137[177] | 1.56T words,[177] 168 billion tokens[175] | 4110[178] | Proprietary  | Specialized for response generation in conversations.   \\nGPT-NeoX | February 2022 | [EleutherAI](/wiki/EleutherAI \"EleutherAI\") | 20[179] | 825 GiB[163] | 740[166] | Apache 2.0  | based on the Megatron architecture   \\n[Chinchilla](/wiki/Chinchilla_AI \"Chinchilla AI\") | March 2022 | [DeepMind](/wiki/DeepMind \"DeepMind\") | 70[180] | 1.4 trillion tokens[180][175] | 6805[176] | Proprietary  | Reduced-parameter model trained on more data. Used in the [Sparrow](/wiki/Sparrow_\\\\(bot\\\\) \"Sparrow \\\\(bot\\\\)\") bot. Often cited for its [neural scaling law](/wiki/Neural_scaling_law \"Neural scaling law\").   \\n[PaLM](/wiki/PaLM \"PaLM\") (Pathways Language Model) | April 2022 | Google | 540[181] | 768 billion tokens[180] | 29,250[176] | Proprietary  | Trained for ~60 days on ~6000 [TPU v4](/wiki/Tensor_Processing_Unit \"Tensor Processing Unit\") chips.[176] As of October 2024[[update]](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit), it is the largest dense Transformer published.   \\nOPT (Open Pretrained Transformer) | May 2022 | [Meta](/wiki/Meta_Platforms \"Meta Platforms\") | 175[182] | 180 billion tokens[183] | 310[166] | Non-commercial research[d] | GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[184]  \\nYaLM 100B | June 2022 | [Yandex](/wiki/Yandex \"Yandex\") | 100[185] | 1.7TB[185] |  | Apache 2.0 | English-Russian model based on Microsoft\\'s Megatron-LM.   \\nMinerva | June 2022 | Google | 540[186] | 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[186] |  | Proprietary  | For solving \"mathematical and scientific questions using step-by-step reasoning\".[187] Initialized from PaLM models, then finetuned on mathematical and scientific data.   \\n[BLOOM](/wiki/BLOOM_\\\\(language_model\\\\) \"BLOOM \\\\(language model\\\\)\") | July 2022 | Large collaboration led by [Hugging Face](/wiki/Hugging_Face \"Hugging Face\") | 175[188] | 350 billion tokens (1.6TB)[189] |  | Responsible AI  | Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)   \\nGalactica | November 2022 | [Meta](/wiki/Meta_Platforms \"Meta Platforms\") | 120 | 106 billion tokens[190] | unknown | CC-BY-NC-4.0  | Trained on scientific text and modalities.   \\nAlexaTM (Teacher Models) | November 2022 | [Amazon](/wiki/Amazon_\\\\(company\\\\) \"Amazon \\\\(company\\\\)\") | 20[191] | 1.3 trillion[192] |  | proprietary[193] | bidirectional sequence-to-sequence architecture   \\n[Neuro-sama](/wiki/Neuro-sama \"Neuro-sama\") | December 2022 | Independent | Unknown | Unknown  |  | privately-owned  | A language model designed for live-streaming on [Twitch](/wiki/Twitch_\\\\(service\\\\) \"Twitch \\\\(service\\\\)\").   \\n[LLaMA](/wiki/LLaMA \"LLaMA\") (Large Language Model Meta AI) | February 2023 | [Meta AI](/wiki/Meta_AI \"Meta AI\") | 65[194] | 1.4 trillion[194] | 6300[195] | Non-commercial research[e] | Corpus has 20 languages. \"Overtrained\" (compared to [Chinchilla scaling law](/wiki/Chinchilla_\\\\(language_model\\\\) \"Chinchilla \\\\(language model\\\\)\")) for better performance with fewer parameters.[194]  \\n[GPT-4](/wiki/GPT-4 \"GPT-4\") | March 2023 | OpenAI | Unknown[f] (According to rumors: 1760)[197] | Unknown  | 240,000[198] | proprietary  | Available for ChatGPT Plus users and used in [several products](/wiki/GPT-4#Usage \"GPT-4\").   \\nChameleon | June 2024 | [Meta AI](/wiki/Meta_AI \"Meta AI\") | 34[199] | 4.4 trillion |  |   \\nCerebras-GPT  | March 2023 | [Cerebras](/wiki/Cerebras \"Cerebras\") | 13[200] |  | 270[166] | Apache 2.0  | Trained with [Chinchilla formula](/wiki/Chinchilla_\\\\(language_model\\\\) \"Chinchilla \\\\(language model\\\\)\").   \\nFalcon | March 2023 | [Technology Innovation Institute](/wiki/Technology_Innovation_Institute \"Technology Innovation Institute\") | 40[201] | 1 trillion tokens, from RefinedWeb (filtered web text corpus)[202] plus some \"curated corpora\".[203] | 2800[195] | Apache 2.0[204] |   \\nBloombergGPT | March 2023 | [Bloomberg L.P.](/wiki/Bloomberg_L.P. \"Bloomberg L.P.\") | 50 | 363 billion token dataset based on Bloomberg\\'s data sources, plus 345 billion tokens from general purpose datasets[205] |  | Proprietary  | Trained on financial data from proprietary sources, for financial tasks.   \\n[PanGu-Σ](/wiki/Huawei_PanGu \"Huawei PanGu\") | March 2023 | [Huawei](/wiki/Huawei \"Huawei\") | 1085 | 329 billion tokens[206] |  | Proprietary  |   \\nOpenAssistant[207] | March 2023 | [LAION](/wiki/LAION \"LAION\") | 17 | 1.5 trillion tokens  |  | Apache 2.0  | Trained on crowdsourced open data   \\nJurassic-2[208] | March 2023 | [AI21 Labs](/wiki/AI21_Labs \"AI21 Labs\") | Unknown  | Unknown  |  | Proprietary  | Multilingual[209]  \\n[PaLM 2](/wiki/PaLM \"PaLM\") (Pathways Language Model 2) | May 2023 | Google | 340[210] | 3.6 trillion tokens[210] | 85,000[195] | Proprietary  | Was used in [Bard chatbot](/wiki/Bard_\\\\(chatbot\\\\) \"Bard \\\\(chatbot\\\\)\").[211]  \\nLlama 2 | July 2023 | Meta AI | 70[212] | 2 trillion tokens[212] | 21,000 | Llama 2 license  | 1.7 million A100-hours.[213]  \\n[Claude 2](/wiki/Claude_\\\\(language_model\\\\) \"Claude \\\\(language model\\\\)\") | July 2023 | Anthropic  | Unknown  | Unknown  | Unknown | Proprietary  | Used in Claude chatbot.[214]  \\n[Granite 13b](/wiki/IBM_Granite \"IBM Granite\") | July 2023 | [IBM](/wiki/IBM \"IBM\") | Unknown  | Unknown  | Unknown | Proprietary  | Used in [IBM Watsonx](/wiki/IBM_Watsonx \"IBM Watsonx\").[215]  \\nMistral 7B | September 2023 | [Mistral AI](/wiki/Mistral_AI \"Mistral AI\") | 7.3[216] | Unknown  |  | Apache 2.0  |   \\n[Claude 2.1](/wiki/Claude_\\\\(language_model\\\\) \"Claude \\\\(language model\\\\)\") | November 2023 | Anthropic  | Unknown  | Unknown  | Unknown | Proprietary  | Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[217]  \\nGrok-1[218] | November 2023 | [x.AI](/wiki/X.AI \"X.AI\") | 314  | Unknown  | Unknown | Apache 2.0  | Used in [Grok](/wiki/Grok_\\\\(chatbot\\\\) \"Grok \\\\(chatbot\\\\)\") chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).[219]  \\n[Gemini 1.0](/wiki/Gemini_\\\\(language_model\\\\) \"Gemini \\\\(language model\\\\)\") | December 2023 | [Google DeepMind](/wiki/Google_DeepMind \"Google DeepMind\") | Unknown  | Unknown  | Unknown | Proprietary  | Multimodal model, comes in three sizes. Used in [the chatbot of the same name](/wiki/Gemini_\\\\(chatbot\\\\) \"Gemini \\\\(chatbot\\\\)\").[220]  \\nMixtral 8x7B  | December 2023 | [Mistral AI](/wiki/Mistral_AI \"Mistral AI\") | 46.7  | Unknown  | Unknown | Apache 2.0  | Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[221] [Mixture of experts](/wiki/Mixture_of_experts \"Mixture of experts\") model, with 12.9 billion parameters activated per token.[222]  \\nMixtral 8x22B  | April 2024 | [Mistral AI](/wiki/Mistral_AI \"Mistral AI\") | 141  | Unknown  | Unknown | Apache 2.0  | [223]  \\n[Phi-2](/w/index.php?title=Phi_\\\\(LLM\\\\)&action=edit&redlink=1 \"Phi \\\\(LLM\\\\) \\\\(page does not exist\\\\)\") | December 2023 | Microsoft  | 2.7  | 1.4T tokens  | 419[224] | MIT  | Trained on real and synthetic \"textbook-quality\" data, for 14 days on 96 A100 GPUs.[224]  \\n[Gemini 1.5](/wiki/Gemini_\\\\(language_model\\\\) \"Gemini \\\\(language model\\\\)\") | February 2024 | [Google DeepMind](/wiki/Google_DeepMind \"Google DeepMind\") | Unknown  | Unknown  | Unknown | Proprietary  | Multimodal model, based on a [Mixture-of-Experts](/wiki/Mixture_of_experts \"Mixture of experts\") (MoE) architecture. Context window above 1 million tokens.[225]  \\n[Gemini Ultra](/wiki/Gemini_\\\\(language_model\\\\) \"Gemini \\\\(language model\\\\)\") | February 2024 | [Google DeepMind](/wiki/Google_DeepMind \"Google DeepMind\") | Unknown  | Unknown  | 580,000[226] |  |   \\nGemma | February 2024 | [Google DeepMind](/wiki/Google_DeepMind \"Google DeepMind\") | 7 | 6T tokens | Unknown | Gemma Terms of Use[227] |   \\n[Claude 3](/wiki/Claude_\\\\(language_model\\\\) \"Claude \\\\(language model\\\\)\") | March 2024  | Anthropic  | Unknown  | Unknown  | Unknown  | Proprietary  | Includes three models, Haiku, Sonnet, and Opus.[228]  \\n[Nova](https://rubiks.ai/nova/release/) | October 2024  | [Rubik\\'s AI](https://rubiks.ai/) | Unknown  | Unknown  | Unknown  | Proprietary  | Includes three models, Nova-Instant, Nova-Air, and Nova-Pro.   \\n[DBRX](/wiki/DBRX \"DBRX\") | March 2024  | [Databricks](/wiki/Databricks \"Databricks\") and [Mosaic ML](/wiki/Mosaic_ML \"Mosaic ML\") | 136 | 12T Tokens  |  | Databricks Open Model License  | Training cost 10 million USD.   \\nFugaku-LLM  | May 2024  | [Fujitsu](/wiki/Fujitsu \"Fujitsu\"), [Tokyo Institute of Technology](/wiki/Tokyo_Institute_of_Technology \"Tokyo Institute of Technology\"), etc.  | 13 | 380B Tokens  |  |  | The largest model ever trained on CPU-only, on the [Fugaku](/wiki/Fugaku_\\\\(supercomputer\\\\) \"Fugaku \\\\(supercomputer\\\\)\").[229]  \\n[Phi-3](/w/index.php?title=Phi_\\\\(LLM\\\\)&action=edit&redlink=1 \"Phi \\\\(LLM\\\\) \\\\(page does not exist\\\\)\") | April 2024 | Microsoft  | 14[230] | 4.8T Tokens  |  | MIT  | Microsoft markets them as \"small language model\".[231]  \\n[Granite Code Models](/wiki/IBM_Granite \"IBM Granite\") | May 2024 | [IBM](/wiki/IBM \"IBM\") | Unknown  | Unknown  | Unknown | Apache 2.0  |   \\nQwen2  | June 2024 | [Alibaba Cloud](/wiki/Alibaba_Cloud \"Alibaba Cloud\") | 72[232] | 3T Tokens  |  |  | Multiple sizes, the smallest being 0.5B.   \\nNemotron-4  | June 2024  | [Nvidia](/wiki/Nvidia \"Nvidia\") | 340 | 9T Tokens  | 200,000 | NVIDIA Open Model License  | Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[233][234]  \\nLlama 3.1  | July 2024  | Meta AI  | 405  | 15.6T tokens  | 440,000 | Llama 3 license  | 405B version took 31 million hours on [H100](/wiki/Hopper_\\\\(microarchitecture\\\\) \"Hopper \\\\(microarchitecture\\\\)\")-80GB, at 3.8E25 FLOPs.[235][236]  \\n  \\n## See also\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=36 \"Edit\\nsection: See also\")]\\n\\n  * [Foundation models](/wiki/Foundation_models \"Foundation models\")\\n\\n## Notes\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=37 \"Edit\\nsection: Notes\")]\\n\\n  1. **^** This is the date that documentation describing the model\\'s architecture was first released.\\n  2. **^** In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\n  3. **^** This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\n  4. **^** The smaller models including 66B are publicly available, while the 175B model is available on request.\\n  5. **^** Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\n  6. **^** As stated in Technical report: \"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\"[196]\\n\\n## References\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=38 \"Edit\\nsection: References\")]\\n\\n  1. **^** [\"Better Language Models and Their Implications\"](https://openai.com/blog/better-language-models/). _OpenAI_. 2019-02-14. [Archived](https://web.archive.org/web/20201219132206/https://openai.com/blog/better-language-models/) from the original on 2020-12-19. Retrieved 2019-08-25.\\n  2. ^ _**a**_ _**b**_ _**c**_ Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). [\"Language Models are Few-Shot Learners\"](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) (PDF). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 1877–1901. [Archived](https://web.archive.org/web/20231117204007/https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) (PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\\n  3. **^** Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). [_NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning_](https://2024.eswc-conferences.org/wp-content/uploads/2024/05/77770034.pdf) (PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\\n  4. ^ _**a**_ _**b**_ [Manning, Christopher D.](/wiki/Christopher_D._Manning \"Christopher D. Manning\") (2022). [\"Human Language Understanding & Reasoning\"](https://www.amacad.org/publication/human-language-understanding-reasoning). _Daedalus_. **151** (2): 127–138. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1162/daed_a_01905](https://doi.org/10.1162%2Fdaed_a_01905). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [248377870](https://api.semanticscholar.org/CorpusID:248377870). [Archived](https://web.archive.org/web/20231117205531/https://www.amacad.org/publication/human-language-understanding-reasoning) from the original on 2023-11-17. Retrieved 2023-03-09.\\n  5. **^** Goodman, Joshua (2001-08-09), _A Bit of Progress in Language Modeling_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[cs/0108005](https://arxiv.org/abs/cs/0108005), [Bibcode](/wiki/Bibcode_\\\\(identifier\\\\) \"Bibcode \\\\(identifier\\\\)\"):[2001cs........8005G](https://ui.adsabs.harvard.edu/abs/2001cs........8005G)\\n  6. **^** Kilgarriff, Adam; Grefenstette, Gregory (September 2003). [\"Introduction to the Special Issue on the Web as Corpus\"](https://direct.mit.edu/coli/article/29/3/333-347/1816). _Computational Linguistics_. **29** (3): 333–347. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1162/089120103322711569](https://doi.org/10.1162%2F089120103322711569). [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [0891-2017](https://search.worldcat.org/issn/0891-2017).\\n  7. **^** Banko, Michele; Brill, Eric (2001). [\"Scaling to very very large corpora for natural language disambiguation\"](https://dx.doi.org/10.3115/1073012.1073017). _Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL \\'01_. Morristown, NJ, USA: Association for Computational Linguistics: 26–33. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.3115/1073012.1073017](https://doi.org/10.3115%2F1073012.1073017).\\n  8. **^** Resnik, Philip; Smith, Noah A. (September 2003). [\"The Web as a Parallel Corpus\"](https://direct.mit.edu/coli/article/29/3/349-380/1809). _Computational Linguistics_. **29** (3): 349–380. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1162/089120103322711578](https://doi.org/10.1162%2F089120103322711578). [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [0891-2017](https://search.worldcat.org/issn/0891-2017). [Archived](https://web.archive.org/web/20240607172811/https://direct.mit.edu/coli/article/29/3/349-380/1809) from the original on 2024-06-07. Retrieved 2024-06-07.\\n  9. **^** Halevy, Alon; Norvig, Peter; Pereira, Fernando (March 2009). [\"The Unreasonable Effectiveness of Data\"](https://ieeexplore.ieee.org/document/4804817). _IEEE Intelligent Systems_. **24** (2): 8–12. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1109/MIS.2009.36](https://doi.org/10.1109%2FMIS.2009.36). [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [1541-1672](https://search.worldcat.org/issn/1541-1672).\\n  10. **^** <https://www.mdpi.com/2072-4292/13/22/4712>\\n  11. **^** [Vaswani, Ashish](/wiki/Ashish_Vaswani \"Ashish Vaswani\"); Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; [Gomez, Aidan N](/wiki/Aidan_Gomez \"Aidan Gomez\"); Kaiser, Łukasz; Polosukhin, Illia (2017). [\"Attention is All you Need\"](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (PDF). _Advances in Neural Information Processing Systems_. **30**. Curran Associates, Inc. [Archived](https://web.archive.org/web/20240221141113/https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\\n  12. **^** Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1409.0473](https://arxiv.org/abs/1409.0473) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  13. **^** Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). [\"A Primer in BERTology: What We Know About How BERT Works\"](https://aclanthology.org/2020.tacl-1.54). _Transactions of the Association for Computational Linguistics_. **8** : 842–866. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2002.12327](https://arxiv.org/abs/2002.12327). [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1162/tacl_a_00349](https://doi.org/10.1162%2Ftacl_a_00349). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [211532403](https://api.semanticscholar.org/CorpusID:211532403). [Archived](https://web.archive.org/web/20220403103310/https://aclanthology.org/2020.tacl-1.54/) from the original on 2022-04-03. Retrieved 2024-01-21.\\n  14. **^** Hern, Alex (14 February 2019). [\"New AI fake text generator may be too dangerous to release, say creators\"](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction). _[The Guardian](/wiki/The_Guardian \"The Guardian\")_. [Archived](https://web.archive.org/web/20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction) from the original on 14 February 2019. Retrieved 20 January 2024.\\n  15. **^** [\"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\"](https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months). [Euronews](/wiki/Euronews \"Euronews\"). November 30, 2023. [Archived](https://web.archive.org/web/20240114025250/https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months) from the original on January 14, 2024. Retrieved January 20, 2024.\\n  16. **^** Heaven, Will (March 14, 2023). [\"GPT-4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\"](https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/). [MIT Technology Review](/wiki/MIT_Technology_Review \"MIT Technology Review\"). [Archived](https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/) from the original on March 17, 2023. Retrieved January 20, 2024.\\n  17. **^** [\"Parameters in notable artificial intelligence systems\"](https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest). _ourworldindata.org_. November 30, 2023. Retrieved January 20, 2024.\\n  18. **^** [\"LMSYS Chatbot Arena Leaderboard\"](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard). _huggingface.co_. [Archived](https://web.archive.org/web/20240610162906/https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) from the original on June 10, 2024. Retrieved June 12, 2024.\\n  19. **^** Peng, Bo; et al. (2023). \"RWKV: Reinventing RNNS for the Transformer Era\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.13048](https://arxiv.org/abs/2305.13048) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  20. **^** Merritt, Rick (2022-03-25). [\"What Is a Transformer Model?\"](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/). _NVIDIA Blog_. [Archived](https://web.archive.org/web/20231117203924/https://blogs.nvidia.com/blog/what-is-a-transformer-model/) from the original on 2023-11-17. Retrieved 2023-07-25.\\n  21. **^** Gu, Albert; Dao, Tri (2023-12-01), _Mamba: Linear-Time Sequence Modeling with Selective State Spaces_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2312.00752](https://arxiv.org/abs/2312.00752)\\n  22. **^** Kaushal, Ayush; Mahowald, Kyle (2022-06-06), _What do tokens know about their characters and how do they know it?_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2206.02608](https://arxiv.org/abs/2206.02608)\\n  23. **^** Yennie Jun (2023-05-03). [\"All languages are NOT created (tokenized) equal\"](https://web.archive.org/web/20230817165705/https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized). _Language models cost much more in some languages than others_. Archived from [the original](https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized) on 2023-08-17. Retrieved 2023-08-17. \"In other words, to express the same sentiment, some languages require up to 10 times more tokens.\"\\n  24. **^** Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). [\"Language Model Tokenizers Introduce Unfairness Between Languages\"](https://openreview.net/forum?id=Pj4YYuxTq9). _NeurIPS_. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.15425](https://arxiv.org/abs/2305.15425). [Archived](https://web.archive.org/web/20231215212906/https://openreview.net/forum?id=Pj4YYuxTq9) from the original on December 15, 2023. Retrieved September 16, 2023 - via openreview.net.\\n  25. **^** [\"OpenAI API\"](https://web.archive.org/web/20230423211308/https://platform.openai.com/tokenizer). _platform.openai.com_. Archived from [the original](https://platform.openai.com/) on April 23, 2023. Retrieved 2023-04-30.\\n  26. ^ _**a**_ _**b**_ Paaß, Gerhard; Giesselbach, Sven (2022). [\"Pre-trained Language Models\"](https://link.springer.com/chapter/10.1007/978-3-031-23190-2_2). _Foundation Models for Natural Language Processing_. Artificial Intelligence: Foundations, Theory, and Algorithms. pp. 19–78. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1007/978-3-031-23190-2_2](https://doi.org/10.1007%2F978-3-031-23190-2_2). [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [9783031231902](/wiki/Special:BookSources/9783031231902 \"Special:BookSources/9783031231902\"). [Archived](https://web.archive.org/web/20230803212329/https://link.springer.com/chapter/10.1007/978-3-031-23190-2_2) from the original on 3 August 2023. Retrieved 3 August 2023.\\n  27. **^** Petrov, Aleksandar; Emanuele La Malfa; Torr, Philip H. S.; Bibi, Adel (2023). \"Language Model Tokenizers Introduce Unfairness Between Languages\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.15425](https://arxiv.org/abs/2305.15425) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  28. **^** Lundberg, Scott (2023-12-12). [\"The Art of Prompt Design: Prompt Boundaries and Token Healing\"](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38). _Medium_. Retrieved 2024-08-05.\\n  29. **^** Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). \"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2104.08758](https://arxiv.org/abs/2104.08758) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  30. **^** Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini, Nicholas (May 2022). [\"Deduplicating Training Data Makes Language Models Better\"](https://aclanthology.org/2022.acl-long.577.pdf) (PDF). _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_. 1: Long Papers: 8424–8445. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.18653/v1/2022.acl-long.577](https://doi.org/10.18653%2Fv1%2F2022.acl-long.577).\\n  31. **^** Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11), _Textbooks Are All You Need II: phi-1.5 technical report_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2309.05463](https://arxiv.org/abs/2309.05463)\\n  32. **^** Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). \"Rho-1: Not All Tokens Are What You Need\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2404.07965](https://arxiv.org/abs/2404.07965) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  33. **^** Brown, Tom B.; et al. (2020). \"Language Models are Few-Shot Learners\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2005.14165](https://arxiv.org/abs/2005.14165) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  34. **^** Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2404.14219](https://arxiv.org/abs/2404.14219) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  35. **^** Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \"Training language models to follow instructions with human feedback\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2203.02155](https://arxiv.org/abs/2203.02155) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  36. **^** Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Model with Self Generated Instructions\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2212.10560](https://arxiv.org/abs/2212.10560) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  37. **^** Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1701.06538](https://arxiv.org/abs/1701.06538) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  38. **^** Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2006.16668](https://arxiv.org/abs/2006.16668) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  39. ^ _**a**_ _**b**_ _**c**_ _**d**_ Dai, Andrew M; Du, Nan (December 9, 2021). [\"More Efficient In-Context Learning with GLaM\"](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20230312072042/https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) from the original on 2023-03-12. Retrieved 2023-03-09.\\n  40. ^ _**a**_ _**b**_ _**c**_ Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). [\"Emergent Abilities of Large Language Models\"](https://openreview.net/forum?id=yzkSU5zdwD). _Transactions on Machine Learning Research_. [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [2835-8856](https://search.worldcat.org/issn/2835-8856). [Archived](https://web.archive.org/web/20230322210052/https://openreview.net/forum?id=yzkSU5zdwD) from the original on 22 March 2023. Retrieved 19 March 2023.\\n  41. **^** Allamar, Jay. [\"Illustrated transformer\"](https://jalammar.github.io/illustrated-transformer/). [Archived](https://web.archive.org/web/20230725230033/http://jalammar.github.io/illustrated-transformer/) from the original on 2023-07-25. Retrieved 2023-07-29.\\n  42. **^** Allamar, Jay. [\"The Illustrated GPT-2 (Visualizing Transformer Language Models)\"](https://jalammar.github.io/illustrated-gpt2/). Retrieved 2023-08-01.\\n  43. **^** [\"Our next-generation model: Gemini 1.5\"](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window). _Google_. 15 February 2024. [Archived](https://web.archive.org/web/20240218141522/https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window) from the original on 18 February 2024. Retrieved 18 February 2024.\\n  44. **^** [\"Long context prompting for Claude 2.1\"](https://www.anthropic.com/news/claude-2-1-prompting). December 6, 2023. [Archived](https://web.archive.org/web/20240827053830/https://www.anthropic.com/news/claude-2-1-prompting) from the original on August 27, 2024. Retrieved January 20, 2024.\\n  45. **^** [\"Rate limits\"](https://platform.openai.com/docs/guides/rate-limits). _openai.com_. [Archived](https://web.archive.org/web/20240202003219/https://platform.openai.com/docs/guides/rate-limits) from the original on February 2, 2024. Retrieved January 20, 2024.\\n  46. **^** Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). [\"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\"](https://www.researchgate.net/publication/338931711). _Proceedings of the Australasian Computer Science Week Multiconference_. pp. 1–4. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2104.10810](https://arxiv.org/abs/2104.10810). [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1145/3373017.3373028](https://doi.org/10.1145%2F3373017.3373028). [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [9781450376976](/wiki/Special:BookSources/9781450376976 \"Special:BookSources/9781450376976\"). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [211040895](https://api.semanticscholar.org/CorpusID:211040895).\\n  47. ^ _**a**_ _**b**_ _**c**_ Jurafsky, Dan; Martin, James H. (7 January 2023). [_Speech and Language Processing_](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf) (PDF) (3rd edition draft ed.). [Archived](https://web.archive.org/web/20230323210221/https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf) (PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\\n  48. **^** [\"From bare metal to a 70B model: infrastructure set-up and scripts\"](https://imbue.com/research/70b-infrastructure/). _imbue.com_. [Archived](https://web.archive.org/web/20240726203419/https://imbue.com/research/70b-infrastructure/) from the original on 2024-07-26. Retrieved 2024-07-24.\\n  49. **^** [\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\"](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles). _GitHub_. [Archived](https://web.archive.org/web/20240124035658/https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles) from the original on 2024-01-24. Retrieved 2024-07-24.\\n  50. **^** Albrecht, Josh (2024-07-23). [\"State of the Art: Training >70B LLMs on 10,000 H100 clusters\"](https://www.latent.space/p/llm-training-2024). _www.latent.space_. Retrieved 2024-07-24.\\n  51. ^ _**a**_ _**b**_ Wiggers, Kyle (28 April 2022). [\"The emerging types of language models and why they matter\"](https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/). _TechCrunch_. [Archived](https://web.archive.org/web/20230316072443/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/) from the original on 16 March 2023. Retrieved 9 March 2023.\\n  52. **^** Sharir, Or; Peleg, Barak; Shoham, Yoav (2020). \"The Cost of Training NLP Models: A Concise Overview\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2004.08900](https://arxiv.org/abs/2004.08900) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  53. **^** Biderman, Stella; Schoelkopf, Hailey; Anthony, Quentin; Bradley, Herbie; Khan, Mohammad Aflah; Purohit, Shivanshu; Prashanth, USVSN Sai (April 2023). \"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.01373](https://arxiv.org/abs/2304.01373) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  54. **^** Maslej, Nestor; Fattorini, Loredana; Brynjolfsson, Erik; Etchemendy, John; Ligett, Katrina; Lyons, Terah; Manyika, James; Ngo, Helen; Niebles, Juan Carlos (2023-10-05), _Artificial Intelligence Index Report 2023_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2310.03715](https://arxiv.org/abs/2310.03715)\\n  55. ^ _**a**_ _**b**_ Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2001.08361](https://arxiv.org/abs/2001.08361) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  56. **^** Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-11-01). \"PAL: Program-aided Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2211.10435](https://arxiv.org/abs/2211.10435) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  57. **^** [\"PAL: Program-aided Language Models\"](https://reasonwithpal.com/). _reasonwithpal.com_. [Archived](https://web.archive.org/web/20230612162208/https://reasonwithpal.com/) from the original on 2023-06-12. Retrieved 2023-06-12.\\n  58. **^** Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \"ART: Automatic multi-step reasoning and tool-use for large language models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.09014](https://arxiv.org/abs/2303.09014) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  59. **^** Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.16434](https://arxiv.org/abs/2303.16434) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  60. **^** Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \"Gorilla: Large Language Model Connected with Massive APIs\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.15334](https://arxiv.org/abs/2305.15334) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  61. **^** Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). [\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 9459–9474. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2005.11401](https://arxiv.org/abs/2005.11401). [Archived](https://web.archive.org/web/20230612171229/https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html) from the original on 2023-06-12. Retrieved 2023-06-12.\\n  62. **^** Huang, Wenlong; Abbeel, Pieter; Pathak, Deepak; Mordatch, Igor (2022-06-28). [\"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\"](https://proceedings.mlr.press/v162/huang22a.html). _Proceedings of the 39th International Conference on Machine Learning_. PMLR: 9118–9147. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2201.07207](https://arxiv.org/abs/2201.07207).\\n  63. **^** Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct: Synergizing Reasoning and Acting in Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2210.03629](https://arxiv.org/abs/2210.03629) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  64. **^** Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \"SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.15486](https://arxiv.org/abs/2305.15486) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  65. **^** Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2302.01560](https://arxiv.org/abs/2302.01560) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  66. **^** Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). \"Reflexion: Language Agents with Verbal Reinforcement Learning\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.11366](https://arxiv.org/abs/2303.11366) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  67. **^** Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). \"Reasoning with Language Model is Planning with World Model\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.14992](https://arxiv.org/abs/2305.14992) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  68. **^** Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \"OMNI: Open-endedness via Models of human Notions of Interestingness\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2306.01711](https://arxiv.org/abs/2306.01711) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  69. ^ _**a**_ _**b**_ [\"Voyager | An Open-Ended Embodied Agent with Large Language Models\"](https://voyager.minedojo.org/). _voyager.minedojo.org_. [Archived](https://web.archive.org/web/20230608225054/https://voyager.minedojo.org/) from the original on 2023-06-08. Retrieved 2023-06-09.\\n  70. **^** Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). \"Generative Agents: Interactive Simulacra of Human Behavior\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.03442](https://arxiv.org/abs/2304.03442) [[cs.HC](https://arxiv.org/archive/cs.HC)].\\n  71. **^** Mann, Tobias. [\"How to run an LLM locally on your PC in less than 10 minutes\"](https://www.theregister.com/2024/03/17/ai_pc_local_llm/). _www.theregister.com_. Retrieved 2024-05-17.\\n  72. **^** Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). [\"Up or Down? Adaptive Rounding for Post-Training Quantization\"](https://proceedings.mlr.press/v119/nagel20a.html). _Proceedings of the 37th International Conference on Machine Learning_. PMLR: 7197–7206. [Archived](https://web.archive.org/web/20230614080854/https://proceedings.mlr.press/v119/nagel20a.html) from the original on 2023-06-14. Retrieved 2023-06-14.\\n  73. **^** Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \"Model compression via distillation and quantization\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1802.05668](https://arxiv.org/abs/1802.05668) [[cs.NE](https://arxiv.org/archive/cs.NE)].\\n  74. **^** Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2210.17323](https://arxiv.org/abs/2210.17323) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  75. **^** Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2306.03078](https://arxiv.org/abs/2306.03078) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  76. **^** Grootendorst, Maarten. [\"A Visual Guide to Quantization\"](https://web.archive.org/web/20240731003355/https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization). _newsletter.maartengrootendorst.com_. Archived from [the original](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization) on 31 Jul 2024. Retrieved 2024-07-31.\\n  77. **^** Dettmers, Tim; Pagnoni, Artidoro; [Holtzman, Ari](/wiki/Ari_Holtzman \"Ari Holtzman\"); Zettlemoyer, Luke (2023-05-01). \"QLoRA: Efficient Finetuning of Quantized LLMs\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.14314](https://arxiv.org/abs/2305.14314) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  78. **^** Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). [\"Multimodal Neural Language Models\"](https://proceedings.mlr.press/v32/kiros14.html). _Proceedings of the 31st International Conference on Machine Learning_. PMLR: 595–603. [Archived](https://web.archive.org/web/20230702195952/https://proceedings.mlr.press/v32/kiros14.html) from the original on 2023-07-02. Retrieved 2023-07-02.\\n  79. **^** Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). [\"ImageNet Classification with Deep Convolutional Neural Networks\"](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html). _Advances in Neural Information Processing Systems_. **25**. Curran Associates, Inc. [Archived](https://web.archive.org/web/20230702195952/https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) from the original on 2023-07-02. Retrieved 2023-07-02.\\n  80. **^** Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). [\"VQA: Visual Question Answering\"](https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html). _ICCV_ : 2425–2433. [Archived](https://web.archive.org/web/20230702195952/https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html) from the original on 2023-07-02. Retrieved 2023-07-02.\\n  81. **^** Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2301.12597](https://arxiv.org/abs/2301.12597) [[cs.CV](https://arxiv.org/archive/cs.CV)].\\n  82. **^** Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). [\"Flamingo: a Visual Language Model for Few-Shot Learning\"](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html). _Advances in Neural Information Processing Systems_. **35** : 23716–23736. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2204.14198](https://arxiv.org/abs/2204.14198). [Archived](https://web.archive.org/web/20230702195951/https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html) from the original on 2023-07-02. Retrieved 2023-07-02.\\n  83. **^** Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \"PaLM-E: An Embodied Multimodal Language Model\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.03378](https://arxiv.org/abs/2303.03378) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  84. **^** Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \"Visual Instruction Tuning\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.08485](https://arxiv.org/abs/2304.08485) [[cs.CV](https://arxiv.org/archive/cs.CV)].\\n  85. **^** Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2306.02858](https://arxiv.org/abs/2306.02858) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  86. **^** OpenAI (2023-03-27). \"GPT-4 Technical Report\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.08774](https://arxiv.org/abs/2303.08774) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  87. **^** OpenAI (September 25, 2023). [\"GPT-4V(ision) System Card\"](https://cdn.openai.com/papers/GPTV_System_Card.pdf) (PDF).\\n  88. **^** Pichai, Sundar (10 May 2023), [_Google Keynote (Google I/O \\'23)_](https://www.youtube.com/watch?v=cNfINi5CNbY&t=931s), timestamp 15:31, retrieved 2023-07-02\\n  89. **^** Wiggers, Kyle (11 September 2024). [\"Mistral releases Pixtral 12B, its first multimodal model\"](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/?utm_medium=aisecret.us&utm_source=aisecret.us&utm_campaign=aisecret.us). _TechCrunch_. Retrieved 14 September 2024.\\n  90. **^** Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \"Training Compute-Optimal Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2203.15556](https://arxiv.org/abs/2203.15556) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  91. ^ _**a**_ _**b**_ Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \"Broken Neural Scaling Laws\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2210.14891](https://arxiv.org/abs/2210.14891) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  92. **^** [\"137 emergent abilities of large language models\"](https://www.jasonwei.net/blog/emergence). _Jason Wei_. Retrieved 2023-06-24.\\n  93. **^** Bowman, Samuel R. (2023). \"Eight Things to Know about Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.00612](https://arxiv.org/abs/2304.00612) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  94. **^** Hahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.07971](https://arxiv.org/abs/2303.07971) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  95. **^** Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). [\"Proceedings of the 2019 Conference of the North\"](https://aclanthology.org/N19-1128). _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_. Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.18653/v1/N19-1128](https://doi.org/10.18653%2Fv1%2FN19-1128). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [102353817](https://api.semanticscholar.org/CorpusID:102353817). [Archived](https://web.archive.org/web/20230627202732/https://aclanthology.org/N19-1128/) from the original on 2023-06-27. Retrieved 2023-06-27.\\n  96. **^** [\"WiC: The Word-in-Context Dataset\"](https://pilehvar.github.io/wic/). _pilehvar.github.io_. [Archived](https://web.archive.org/web/20230627202725/https://pilehvar.github.io/wic/) from the original on 2023-06-27. Retrieved 2023-06-27.\\n  97. **^** Patel, Roma; Pavlick, Ellie (2021-10-06). [\"Mapping Language Models to Grounded Conceptual Spaces\"](https://openreview.net/forum?id=gJcEM8sxHK). _ICLR_. [Archived](https://web.archive.org/web/20230624191940/https://openreview.net/forum?id=gJcEM8sxHK) from the original on 2023-06-24. Retrieved 2023-06-27.\\n  98. **^** _[A Closer Look at Large Language Models Emergent Abilities](https://www.notion.so/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f) [Archived](https://web.archive.org/web/20230624012329/https://www.notion.so/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f) 2023-06-24 at the [Wayback Machine](/wiki/Wayback_Machine \"Wayback Machine\")_ (Yao Fu, Nov 20, 2022)\\n  99. **^** Ornes, Stephen (March 16, 2023). [\"The Unpredictable Abilities Emerging From Large AI Models\"](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/). _Quanta Magazine_. [Archived](https://web.archive.org/web/20230316203438/https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/) from the original on March 16, 2023. Retrieved March 16, 2023.\\n  100. **^** Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a Mirage?\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.15004](https://arxiv.org/abs/2304.15004) [[cs.AI](https://arxiv.org/archive/cs.AI)].\\n  101. **^** Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2210.13382](https://arxiv.org/abs/2210.13382) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  102. **^** [\"Large Language Model: world models or surface statistics?\"](https://thegradient.pub/othello/). _The Gradient_. 2023-01-21. Retrieved 2023-06-12.\\n  103. **^** Jin, Charles; Rinard, Martin (2023-05-01). \"Evidence of Meaning in Language Models Trained on Programs\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.11169](https://arxiv.org/abs/2305.11169) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  104. **^** Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for grokking via mechanistic interpretability\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2301.05217](https://arxiv.org/abs/2301.05217) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  105. ^ _**a**_ _**b**_ _**c**_ _**d**_ _**e**_ Mitchell, Melanie; Krakauer, David C. (28 March 2023). [\"The debate over understanding in AI\\'s large language models\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812). _Proceedings of the National Academy of Sciences_. **120** (13): e2215907120. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2210.13966](https://arxiv.org/abs/2210.13966). [Bibcode](/wiki/Bibcode_\\\\(identifier\\\\) \"Bibcode \\\\(identifier\\\\)\"):[2023PNAS..12015907M](https://ui.adsabs.harvard.edu/abs/2023PNAS..12015907M). [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1073/pnas.2215907120](https://doi.org/10.1073%2Fpnas.2215907120). [PMC](/wiki/PMC_\\\\(identifier\\\\) \"PMC \\\\(identifier\\\\)\") [10068812](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812). [PMID](/wiki/PMID_\\\\(identifier\\\\) \"PMID \\\\(identifier\\\\)\") [36943882](https://pubmed.ncbi.nlm.nih.gov/36943882).\\n  106. **^** Metz, Cade (16 May 2023). [\"Microsoft Says New A.I. Shows Signs of Human Reasoning\"](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.html). _The New York Times_.\\n  107. ^ _**a**_ _**b**_ Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.12712](https://arxiv.org/abs/2303.12712) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  108. **^** [\"ChatGPT is more like an \\'alien intelligence\\' than a human brain, says futurist\"](https://www.zdnet.com/article/chatgpt-is-more-like-an-alien-intelligence-than-a-human-brain-says-futurist/). _ZDNET_. 2023. [Archived](https://web.archive.org/web/20230612065937/https://www.zdnet.com/article/chatgpt-is-more-like-an-alien-intelligence-than-a-human-brain-says-futurist/) from the original on 12 June 2023. Retrieved 12 June 2023.\\n  109. ^ _**a**_ _**b**_ Newport, Cal (13 April 2023). [\"What Kind of Mind Does ChatGPT Have?\"](https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have). _The New Yorker_. [Archived](https://web.archive.org/web/20230612071443/https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have) from the original on 12 June 2023. Retrieved 12 June 2023.\\n  110. **^** Roose, Kevin (30 May 2023). [\"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\"](https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html) _The New York Times_. [Archived](https://web.archive.org/web/20230530193814/https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html) from the original on 30 May 2023. Retrieved 12 June 2023.\\n  111. **^** [\"The A to Z of Artificial Intelligence\"](https://time.com/6271657/a-to-z-of-artificial-intelligence/). _Time Magazine_. 13 April 2023. [Archived](https://web.archive.org/web/20230616123839/https://time.com/6271657/a-to-z-of-artificial-intelligence/) from the original on 16 June 2023. Retrieved 12 June 2023.\\n  112. **^** Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). [\"Survey of Hallucination in Natural Language Generation\"](https://dl.acm.org/doi/pdf/10.1145/3571730) (pdf). _ACM Computing Surveys_. **55** (12). [Association for Computing Machinery](/wiki/Association_for_Computing_Machinery \"Association for Computing Machinery\"): 1–38. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2202.03629](https://arxiv.org/abs/2202.03629). [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1145/3571730](https://doi.org/10.1145%2F3571730). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [246652372](https://api.semanticscholar.org/CorpusID:246652372). [Archived](https://web.archive.org/web/20230326145635/https://dl.acm.org/doi/pdf/10.1145/3571730) from the original on 26 March 2023. Retrieved 15 January 2023.\\n  113. **^** Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2307.03987](https://arxiv.org/abs/2307.03987) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  114. **^** Lakoff, George (1999). _Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm_. New York Basic Books. pp. 569–583. [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [978-0-465-05674-3](/wiki/Special:BookSources/978-0-465-05674-3 \"Special:BookSources/978-0-465-05674-3\").\\n  115. **^** Evans, Vyvyan. (2014). _The Language Myth_. Cambridge University Press. [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [978-1-107-04396-1](/wiki/Special:BookSources/978-1-107-04396-1 \"Special:BookSources/978-1-107-04396-1\").\\n  116. **^** Friston, Karl J. (2022). _Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference_. The MIT Press. [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [978-0-262-36997-8](/wiki/Special:BookSources/978-0-262-36997-8 \"Special:BookSources/978-0-262-36997-8\").\\n  117. ^ _**a**_ _**b**_ Huyen, Chip (October 18, 2019). [\"Evaluation Metrics for Language Modeling\"](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/). _The Gradient_. Retrieved January 14, 2024.\\n  118. ^ _**a**_ _**b**_ Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1905.10044](https://arxiv.org/abs/1905.10044) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  119. ^ _**a**_ _**b**_ _**c**_ Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \"A Survey of Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.18223](https://arxiv.org/abs/2303.18223) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  120. **^** [_openai/simple-evals_](https://github.com/openai/simple-evals), OpenAI, 2024-05-28, retrieved 2024-05-28\\n  121. **^** [_openai/evals_](https://github.com/openai/evals), OpenAI, 2024-05-28, [archived](https://web.archive.org/web/20240508225708/https://github.com/openai/evals) from the original on 2024-05-08, retrieved 2024-05-28\\n  122. **^** [\"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\"](https://imbue.com/research/70b-evals/). _imbue.com_. [Archived](https://web.archive.org/web/20240726173012/https://imbue.com/research/70b-evals/) from the original on 2024-07-26. Retrieved 2024-07-24.\\n  123. **^** Srivastava, Aarohi; et al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2206.04615](https://arxiv.org/abs/2206.04615) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  124. **^** Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2109.07958](https://arxiv.org/abs/2109.07958) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  125. ^ _**a**_ _**b**_ Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish Your Sentence?\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1905.07830](https://arxiv.org/abs/1905.07830) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  126. **^** \"Prepare for truly useful large language models\". _Nature Biomedical Engineering_. **7** (2): 85–86. 7 March 2023. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1038/s41551-023-01012-6](https://doi.org/10.1038%2Fs41551-023-01012-6). [PMID](/wiki/PMID_\\\\(identifier\\\\) \"PMID \\\\(identifier\\\\)\") [36882584](https://pubmed.ncbi.nlm.nih.gov/36882584). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [257403466](https://api.semanticscholar.org/CorpusID:257403466).\\n  127. **^** [\"Your job is (probably) safe from artificial intelligence\"](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence). _The Economist_. 7 May 2023. [Archived](https://web.archive.org/web/20230617225618/https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence) from the original on 17 June 2023. Retrieved 18 June 2023.\\n  128. **^** [\"Generative AI Could Raise Global GDP by 7%\"](https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html). _Goldman Sachs_. [Archived](https://web.archive.org/web/20230618013836/https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html) from the original on 18 June 2023. Retrieved 18 June 2023.\\n  129. **^** Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). [\"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\"](https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf) (PDF). _Proceedings of the ACM on Management of Data_. **1** (2): 1–18. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1145/3589324](https://doi.org/10.1145%2F3589324). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [259213212](https://api.semanticscholar.org/CorpusID:259213212). [Archived](https://web.archive.org/web/20240827053753/https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf) (PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\\n  130. **^** Peng, Wang & Deng 2023, p. 8.\\n  131. **^** Alba, Davey (1 May 2023). [\"AI chatbots have been used to create dozens of news content farms\"](https://www.japantimes.co.jp/news/2023/05/01/business/tech/ai-fake-news-content-farms/). _The Japan Times_. Retrieved 18 June 2023.\\n  132. **^** [\"Could chatbots help devise the next pandemic virus?\"](https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus). _Science_. 14 June 2023. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1126/science.adj2463](https://doi.org/10.1126%2Fscience.adj2463). [Archived](https://web.archive.org/web/20230618013834/https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus) from the original on 18 June 2023. Retrieved 18 June 2023.\\n  133. **^** Stephen Council (1 Dec 2023). [\"How Googlers cracked an SF rival\\'s tech model with a single word\"](https://www.sfgate.com/tech/article/google-openai-chatgpt-break-model-18525445.php). SFGATE. [Archived](https://web.archive.org/web/20231216160941/https://www.sfgate.com/tech/article/google-openai-chatgpt-break-model-18525445.php) from the original on 16 December 2023.\\n  134. **^** Hubinger, Evan (10 January 2024). \"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2401.05566](https://arxiv.org/abs/2401.05566) [[cs.CR](https://arxiv.org/archive/cs.CR)].\\n  135. **^** Kang, Daniel (2023). \"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2302.05733](https://arxiv.org/abs/2302.05733) [[cs.CR](https://arxiv.org/archive/cs.CR)].\\n  136. **^** Wang, Yongge (20 June 2024). [\"Encryption Based Covert Channel for Large Language Models\"](https://eprint.iacr.org/2024/586.pdf) (PDF). IACR ePrint 2024/586. [Archived](https://web.archive.org/web/20240624191233/https://eprint.iacr.org/2024/586.pdf) (PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\\n  137. ^ _**a**_ _**b**_ Stokel-Walker, Chris (November 22, 2023). [\"ChatGPT Replicates Gender Bias in Recommendation Letters\"](https://www.scientificamerican.com/article/chatgpt-replicates-gender-bias-in-recommendation-letters/). _Scientific American_. [Archived](https://web.archive.org/web/20231229043124/https://www.scientificamerican.com/article/chatgpt-replicates-gender-bias-in-recommendation-letters/) from the original on 2023-12-29. Retrieved 2023-12-29.\\n  138. **^** Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.16281v2](https://arxiv.org/abs/2303.16281v2) [[cs.CY](https://arxiv.org/archive/cs.CY)].\\n  139. **^** Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29), _Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models_ , [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2305.18189](https://arxiv.org/abs/2305.18189)\\n  140. **^** Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). [\"Gender bias and stereotypes in Large Language Models\"](https://dl.acm.org/doi/10.1145/3582269.3615599). _Proceedings of the ACM Collective Intelligence Conference_. CI \\'23. New York, NY, USA: Association for Computing Machinery. pp. 12–24. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1145/3582269.3615599](https://doi.org/10.1145%2F3582269.3615599). [ISBN](/wiki/ISBN_\\\\(identifier\\\\) \"ISBN \\\\(identifier\\\\)\") [979-8-4007-0113-9](/wiki/Special:BookSources/979-8-4007-0113-9 \"Special:BookSources/979-8-4007-0113-9\").\\n  141. **^** Heikkilä, Melissa (August 7, 2023). [\"AI language models are rife with different political biases\"](https://www.technologyreview.com/2023/08/07/1077324/ai-language-models-are-rife-with-political-biases/). _MIT Technology Review_. Retrieved 2023-12-29.\\n  142. **^** [\"Improving language understanding with unsupervised learning\"](https://openai.com/research/language-unsupervised). _openai.com_. June 11, 2018. [Archived](https://web.archive.org/web/20230318210736/https://openai.com/research/language-unsupervised) from the original on 2023-03-18. Retrieved 2023-03-18.\\n  143. **^** [\"finetune-transformer-lm\"](https://github.com/openai/finetune-transformer-lm). _GitHub_. [Archived](https://web.archive.org/web/20230519062127/https://github.com/openai/finetune-transformer-lm) from the original on 19 May 2023. Retrieved 2 January 2024.\\n  144. ^ _**a**_ _**b**_ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1810.04805v2](https://arxiv.org/abs/1810.04805v2) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  145. **^** Prickett, Nicole Hemsoth (2021-08-24). [\"Cerebras Shifts Architecture To Meet Massive AI/ML Models\"](https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/). _The Next Platform_. [Archived](https://web.archive.org/web/20230620151619/https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/) from the original on 2023-06-20. Retrieved 2023-06-20.\\n  146. **^** [\"BERT\"](https://github.com/google-research/bert). March 13, 2023. [Archived](https://web.archive.org/web/20210113211317/https://github.com/google-research/bert) from the original on January 13, 2021. Retrieved March 13, 2023 - via GitHub.\\n  147. **^** Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \"Bidirectional Language Models Are Also Few-shot Learners\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2209.14500](https://arxiv.org/abs/2209.14500) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  148. **^** Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1810.04805v2](https://arxiv.org/abs/1810.04805v2) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  149. ^ _**a**_ _**b**_ Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). [\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"](http://jmlr.org/papers/v21/20-074.html). _Journal of Machine Learning Research_. **21** (140): 1–67. [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1910.10683](https://arxiv.org/abs/1910.10683). [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [1533-7928](https://search.worldcat.org/issn/1533-7928).\\n  150. **^** [_google-research/text-to-text-transfer-transformer_](https://github.com/google-research/text-to-text-transfer-transformer), Google Research, 2024-04-02, [archived](https://web.archive.org/web/20240329112957/https://github.com/google-research/text-to-text-transfer-transformer) from the original on 2024-03-29, retrieved 2024-04-04\\n  151. **^** [\"Imagen: Text-to-Image Diffusion Models\"](https://imagen.research.google/). _imagen.research.google_. [Archived](https://web.archive.org/web/20240327201713/https://imagen.research.google/) from the original on 2024-03-27. Retrieved 2024-04-04.\\n  152. **^** [\"Pretrained models — transformers 2.0.0 documentation\"](https://huggingface.co/transformers/v2.0.0/pretrained_models.html). _huggingface.co_. [Archived](https://web.archive.org/web/20240805032110/https://huggingface.co/transformers/v2.0.0/pretrained_models.html) from the original on 2024-08-05. Retrieved 2024-08-05.\\n  153. **^** [\"xlnet\"](https://github.com/zihangdai/xlnet/). _GitHub_. [Archived](https://web.archive.org/web/20240102191842/https://github.com/zihangdai/xlnet/) from the original on 2 January 2024. Retrieved 2 January 2024.\\n  154. **^** Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[1906.08237](https://arxiv.org/abs/1906.08237) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  155. **^** [\"GPT-2: 1.5B Release\"](https://openai.com/blog/gpt-2-1-5b-release/). _OpenAI_. 2019-11-05. [Archived](https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2-1-5b-release/) from the original on 2019-11-14. Retrieved 2019-11-14.\\n  156. **^** [\"Better language models and their implications\"](https://openai.com/research/better-language-models). _openai.com_. [Archived](https://web.archive.org/web/20230316160730/https://openai.com/research/better-language-models) from the original on 2023-03-16. Retrieved 2023-03-13.\\n  157. ^ _**a**_ _**b**_ [\"OpenAI\\'s GPT-3 Language Model: A Technical Overview\"](https://lambdalabs.com/blog/demystifying-gpt-3). _lambdalabs.com_. 3 June 2020. [Archived](https://web.archive.org/web/20230327213811/https://lambdalabs.com/blog/demystifying-gpt-3) from the original on 27 March 2023. Retrieved 13 March 2023.\\n  158. ^ _**a**_ _**b**_ [\"openai-community/gpt2-xl · Hugging Face\"](https://huggingface.co/openai-community/gpt2-xl). _huggingface.co_. [Archived](https://web.archive.org/web/20240724041702/https://huggingface.co/openai-community/gpt2-xl) from the original on 2024-07-24. Retrieved 2024-07-24.\\n  159. **^** [\"gpt-2\"](https://github.com/openai/gpt-2). _GitHub_. [Archived](https://web.archive.org/web/20230311154936/https://github.com/openai/gpt-2) from the original on 11 March 2023. Retrieved 13 March 2023.\\n  160. **^** Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). \"Language Models are Few-Shot Learners\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2005.14165v4](https://arxiv.org/abs/2005.14165v4) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  161. **^** [\"ChatGPT: Optimizing Language Models for Dialogue\"](https://openai.com/blog/chatgpt/). _OpenAI_. 2022-11-30. [Archived](https://web.archive.org/web/20221130180912/https://openai.com/blog/chatgpt/) from the original on 2022-11-30. Retrieved 2023-01-13.\\n  162. **^** [\"GPT Neo\"](https://github.com/EleutherAI/gpt-neo). March 15, 2023. [Archived](https://web.archive.org/web/20230312225202/https://github.com/EleutherAI/gpt-neo) from the original on March 12, 2023. Retrieved March 12, 2023 - via GitHub.\\n  163. ^ _**a**_ _**b**_ _**c**_ Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2101.00027](https://arxiv.org/abs/2101.00027) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  164. ^ _**a**_ _**b**_ Iyer, Abhishek (15 May 2021). [\"GPT-3\\'s free alternative GPT-Neo is something to be excited about\"](https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/). _VentureBeat_. [Archived](https://web.archive.org/web/20230309012717/https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/) from the original on 9 March 2023. Retrieved 13 March 2023.\\n  165. **^** [\"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\"](https://web.archive.org/web/20230309205439/https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model). _www.forefront.ai_. Archived from [the original](https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model) on 2023-03-09. Retrieved 2023-02-28.\\n  166. ^ _**a**_ _**b**_ _**c**_ _**d**_ Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). \"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.03208](https://arxiv.org/abs/2304.03208) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  167. **^** Alvi, Ali; Kharya, Paresh (11 October 2021). [\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\'s Largest and Most Powerful Generative Language Model\"](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/). _Microsoft Research_. [Archived](https://web.archive.org/web/20230313180531/https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) from the original on 13 March 2023. Retrieved 13 March 2023.\\n  168. ^ _**a**_ _**b**_ Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2201.11990](https://arxiv.org/abs/2201.11990) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  169. ^ _**a**_ _**b**_ Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), [_DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale_](https://arxiv.org/abs/2201.05596), [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.48550/arXiv.2201.05596](https://doi.org/10.48550%2FarXiv.2201.05596), retrieved 2024-10-18\\n  170. **^** Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2112.12731](https://arxiv.org/abs/2112.12731) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  171. **^** [\"Product\"](https://www.anthropic.com/product). _Anthropic_. [Archived](https://web.archive.org/web/20230316145444/https://www.anthropic.com/product) from the original on 16 March 2023. Retrieved 14 March 2023.\\n  172. ^ _**a**_ _**b**_ Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). \"A General Language Assistant as a Laboratory for Alignment\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2112.00861](https://arxiv.org/abs/2112.00861) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  173. **^** Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). \"Constitutional AI: Harmlessness from AI Feedback\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2212.08073](https://arxiv.org/abs/2212.08073) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  174. **^** [\"Language modelling at scale: Gopher, ethical considerations, and retrieval\"](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval). _www.deepmind.com_. 8 December 2021. [Archived](https://web.archive.org/web/20230320082323/https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval) from the original on 20 March 2023. Retrieved 20 March 2023.\\n  175. ^ _**a**_ _**b**_ _**c**_ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). \"Training Compute-Optimal Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2203.15556](https://arxiv.org/abs/2203.15556) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  176. ^ _**a**_ _**b**_ _**c**_ _**d**_ Table 20 and page 66 of _[PaLM: Scaling Language Modeling with Pathways](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) [Archived](https://web.archive.org/web/20230610040050/https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) 2023-06-10 at the [Wayback Machine](/wiki/Wayback_Machine \"Wayback Machine\")_\\n  177. ^ _**a**_ _**b**_ Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). [\"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\"](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20220325014118/https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) from the original on 2022-03-25. Retrieved 2023-03-09.\\n  178. **^** Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). \"LaMDA: Language Models for Dialog Applications\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2201.08239](https://arxiv.org/abs/2201.08239) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  179. **^** Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). [_GPT-NeoX-20B: An Open-Source Autoregressive Language Model_](https://aclanthology.org/2022.bigscience-1.9/). Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Vol. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. pp. 95–136. [Archived](https://web.archive.org/web/20221210082456/https://aclanthology.org/2022.bigscience-1.9/) from the original on 2022-12-10. Retrieved 2022-12-19.\\n  180. ^ _**a**_ _**b**_ _**c**_ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). [\"An empirical analysis of compute-optimal large language model training\"](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training). _Deepmind Blog_. [Archived](https://web.archive.org/web/20220413014510/https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training) from the original on 13 April 2022. Retrieved 9 March 2023.\\n  181. **^** Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). [\"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\"](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20220404161447/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) from the original on 2022-04-04. Retrieved 2023-03-09.\\n  182. **^** Susan Zhang; Mona Diab; Luke Zettlemoyer. [\"Democratizing access to large-scale language models with OPT-175B\"](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/). _ai.facebook.com_. [Archived](https://web.archive.org/web/20230312231820/https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) from the original on 2023-03-12. Retrieved 2023-03-12.\\n  183. **^** Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \"OPT: Open Pre-trained Transformer Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2205.01068](https://arxiv.org/abs/2205.01068) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  184. **^** [\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\"](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles). _GitHub_. Retrieved 2024-10-18.\\n  185. ^ _**a**_ _**b**_ Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), [_YaLM 100B_](https://github.com/yandex/YaLM-100B), [archived](https://web.archive.org/web/20230616050056/https://github.com/yandex/YaLM-100B) from the original on 2023-06-16, retrieved 2023-03-18\\n  186. ^ _**a**_ _**b**_ Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \"Solving Quantitative Reasoning Problems with Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2206.14858](https://arxiv.org/abs/2206.14858) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  187. **^** [\"Minerva: Solving Quantitative Reasoning Problems with Language Models\"](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html). _ai.googleblog.com_. 30 June 2022. Retrieved 20 March 2023.\\n  188. **^** Ananthaswamy, Anil (8 March 2023). [\"In AI, is bigger always better?\"](https://www.nature.com/articles/d41586-023-00641-w). _Nature_. **615** (7951): 202–205. [Bibcode](/wiki/Bibcode_\\\\(identifier\\\\) \"Bibcode \\\\(identifier\\\\)\"):[2023Natur.615..202A](https://ui.adsabs.harvard.edu/abs/2023Natur.615..202A). [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1038/d41586-023-00641-w](https://doi.org/10.1038%2Fd41586-023-00641-w). [PMID](/wiki/PMID_\\\\(identifier\\\\) \"PMID \\\\(identifier\\\\)\") [36890378](https://pubmed.ncbi.nlm.nih.gov/36890378). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [257380916](https://api.semanticscholar.org/CorpusID:257380916). [Archived](https://web.archive.org/web/20230316181013/https://www.nature.com/articles/d41586-023-00641-w) from the original on 16 March 2023. Retrieved 9 March 2023.\\n  189. **^** [\"bigscience/bloom · Hugging Face\"](https://huggingface.co/bigscience/bloom). _huggingface.co_. [Archived](https://web.archive.org/web/20230412002547/https://huggingface.co/bigscience/bloom) from the original on 2023-04-12. Retrieved 2023-03-13.\\n  190. **^** Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \"Galactica: A Large Language Model for Science\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2211.09085](https://arxiv.org/abs/2211.09085) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  191. **^** [\"20B-parameter Alexa model sets new marks in few-shot learning\"](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning). _Amazon Science_. 2 August 2022. [Archived](https://web.archive.org/web/20230315190223/https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning) from the original on 15 March 2023. Retrieved 12 March 2023.\\n  192. **^** Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2208.01448](https://arxiv.org/abs/2208.01448) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  193. **^** [\"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\"](https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/). _aws.amazon.com_. 17 November 2022. [Archived](https://web.archive.org/web/20230313163933/https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/) from the original on 13 March 2023. Retrieved 13 March 2023.\\n  194. ^ _**a**_ _**b**_ _**c**_ [\"Introducing LLaMA: A foundational, 65-billion-parameter large language model\"](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/). _Meta AI_. 24 February 2023. [Archived](https://web.archive.org/web/20230303112302/https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) from the original on 3 March 2023. Retrieved 9 March 2023.\\n  195. ^ _**a**_ _**b**_ _**c**_ [\"The Falcon has landed in the Hugging Face ecosystem\"](https://huggingface.co/blog/falcon). _huggingface.co_. [Archived](https://web.archive.org/web/20230620002832/https://huggingface.co/blog/falcon) from the original on 2023-06-20. Retrieved 2023-06-20.\\n  196. **^** [\"GPT-4 Technical Report\"](https://cdn.openai.com/papers/gpt-4.pdf) (PDF). _[OpenAI](/wiki/OpenAI \"OpenAI\")_. 2023. [Archived](https://web.archive.org/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf) (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\n  197. **^** Schreiner, Maximilian (2023-07-11). [\"GPT-4 architecture, datasets, costs and more leaked\"](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). _THE DECODER_. [Archived](https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) from the original on 2023-07-12. Retrieved 2024-07-26.\\n  198. **^** [\"Insights: Breaking Down the Transformative Journey of GPT Models in AI, from GPT-1 to GPT-4\"](https://www.techopedia.com/gpt-series-evolution-insights).\\n  199. **^** Dickson, Ben (22 May 2024). [\"Meta introduces Chameleon, a state-of-the-art multimodal model\"](https://venturebeat.com/ai/meta-introduces-chameleon-a-state-of-the-art-multimodal-model/). _VentureBeat_.\\n  200. **^** Dey, Nolan (March 28, 2023). [\"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\"](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/). _Cerebras_. [Archived](https://web.archive.org/web/20230328213339/https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) from the original on March 28, 2023. Retrieved March 28, 2023.\\n  201. **^** [\"Abu Dhabi-based TII launches its own version of ChatGPT\"](https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/). _tii.ae_. [Archived](https://web.archive.org/web/20230403021729/https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/) from the original on 2023-04-03. Retrieved 2023-04-03.\\n  202. **^** Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2306.01116](https://arxiv.org/abs/2306.01116) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  203. **^** [\"tiiuae/falcon-40b · Hugging Face\"](https://huggingface.co/tiiuae/falcon-40b). _huggingface.co_. 2023-06-09. Retrieved 2023-06-20.\\n  204. **^** [UAE\\'s Falcon 40B, World\\'s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free](https://www.businesswire.com/news/home/20230531005608/en/UAE\\'s-Falcon-40B-World\\'s-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) [Archived](https://web.archive.org/web/20240208133040/https://www.businesswire.com/news/home/20230531005608/en/UAE%27s-Falcon-40B-World%27s-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) 2024-02-08 at the [Wayback Machine](/wiki/Wayback_Machine \"Wayback Machine\"), 31 May 2023\\n  205. **^** Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \"BloombergGPT: A Large Language Model for Finance\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.17564](https://arxiv.org/abs/2303.17564) [[cs.LG](https://arxiv.org/archive/cs.LG)].\\n  206. **^** Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.10845](https://arxiv.org/abs/2303.10845) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  207. **^** Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \"OpenAssistant Conversations – Democratizing Large Language Model Alignment\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2304.07327](https://arxiv.org/abs/2304.07327) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  208. **^** Wrobel, Sharon. [\"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\"](https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/). _www.timesofisrael.com_. [Archived](https://web.archive.org/web/20230724191823/https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/) from the original on 2023-07-24. Retrieved 2023-07-24.\\n  209. **^** Wiggers, Kyle (2023-04-13). [\"With Bedrock, Amazon enters the generative AI race\"](https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/). _TechCrunch_. [Archived](https://web.archive.org/web/20230724102458/https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/) from the original on 2023-07-24. Retrieved 2023-07-24.\\n  210. ^ _**a**_ _**b**_ Elias, Jennifer (16 May 2023). [\"Google\\'s newest A.I. model uses nearly five times more text data for training than its predecessor\"](https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html). _[CNBC](/wiki/CNBC \"CNBC\")_. [Archived](https://web.archive.org/web/20230516225326/https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html) from the original on 16 May 2023. Retrieved 18 May 2023.\\n  211. **^** [\"Introducing PaLM 2\"](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/). _Google_. May 10, 2023. [Archived](https://web.archive.org/web/20230518213209/https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) from the original on May 18, 2023. Retrieved May 18, 2023.\\n  212. ^ _**a**_ _**b**_ [\"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\"](https://ai.meta.com/llama/). _Meta AI_. 2023. [Archived](https://web.archive.org/web/20240105234629/https://ai.meta.com/llama/) from the original on 2024-01-05. Retrieved 2023-07-19.\\n  213. **^** [\"llama/MODEL_CARD.md at main · meta-llama/llama\"](https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md). _GitHub_. [Archived](https://web.archive.org/web/20240528090541/https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md) from the original on 2024-05-28. Retrieved 2024-05-28.\\n  214. **^** [\"Claude 2\"](https://www.anthropic.com/index/claude-2). _anthropic.com_. [Archived](https://web.archive.org/web/20231215212208/https://www.anthropic.com/index/claude-2) from the original on 15 December 2023. Retrieved 12 December 2023.\\n  215. **^** Nirmal, Dinesh (2023-09-07). [\"Building AI for business: IBM\\'s Granite foundation models\"](https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models). _IBM Blog_. [Archived](https://web.archive.org/web/20240722083855/https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/) from the original on 2024-07-22. Retrieved 2024-08-11.\\n  216. **^** [\"Announcing Mistral 7B\"](https://mistral.ai/news/announcing-mistral-7b/). _Mistral_. 2023. [Archived](https://web.archive.org/web/20240106051047/https://mistral.ai/news/announcing-mistral-7b/) from the original on 2024-01-06. Retrieved 2023-10-06.\\n  217. **^** [\"Introducing Claude 2.1\"](https://www.anthropic.com/index/claude-2-1). _anthropic.com_. [Archived](https://web.archive.org/web/20231215201726/https://www.anthropic.com/index/claude-2-1) from the original on 15 December 2023. Retrieved 12 December 2023.\\n  218. **^** [_xai-org/grok-1_](https://github.com/xai-org/grok-1), xai-org, 2024-03-19, [archived](https://web.archive.org/web/20240528170731/https://github.com/xai-org/grok-1) from the original on 2024-05-28, retrieved 2024-03-19\\n  219. **^** [\"Grok-1 model card\"](https://x.ai/model-card/). _x.ai_. Retrieved 12 December 2023.\\n  220. **^** [\"Gemini – Google DeepMind\"](https://deepmind.google/technologies/gemini/#capabilities). _deepmind.google_. [Archived](https://web.archive.org/web/20231208015607/https://deepmind.google/technologies/gemini/#capabilities) from the original on 8 December 2023. Retrieved 12 December 2023.\\n  221. **^** Franzen, Carl (11 December 2023). [\"Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance\"](https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/). _VentureBeat_. [Archived](https://web.archive.org/web/20231211213640/https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/) from the original on 11 December 2023. Retrieved 12 December 2023.\\n  222. **^** [\"Mixtral of experts\"](https://mistral.ai/news/mixtral-of-experts/). _mistral.ai_. 11 December 2023. [Archived](https://web.archive.org/web/20240213104049/https://mistral.ai/news/mixtral-of-experts/) from the original on 13 February 2024. Retrieved 12 December 2023.\\n  223. **^** AI, Mistral (2024-04-17). [\"Cheaper, Better, Faster, Stronger\"](https://mistral.ai/news/mixtral-8x22b/). _mistral.ai_. [Archived](https://web.archive.org/web/20240505023828/https://mistral.ai/news/mixtral-8x22b/) from the original on 2024-05-05. Retrieved 2024-05-05.\\n  224. ^ _**a**_ _**b**_ Hughes, Alyssa (12 December 2023). [\"Phi-2: The surprising power of small language models\"](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). _Microsoft Research_. [Archived](https://web.archive.org/web/20231212232647/https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) from the original on 12 December 2023. Retrieved 13 December 2023.\\n  225. **^** [\"Our next-generation model: Gemini 1.5\"](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window). _Google_. 15 February 2024. [Archived](https://web.archive.org/web/20240216003052/https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window) from the original on 16 February 2024. Retrieved 16 February 2024. \"This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we\\'ve also successfully tested up to 10 million tokens.\"\\n  226. **^** [\"Generative AI Funding Hits $25.2 Billion in 2023, Report Reveals\"](https://aibusiness.com/verticals/generative-ai-funding-hits-25-2-billion-in-2023-report-reveals).\\n  227. **^** [\"Gemma\"](https://ai.google.dev/gemma/terms) - via GitHub.\\n  228. **^** [\"Introducing the next generation of Claude\"](https://www.anthropic.com/news/claude-3-family). _www.anthropic.com_. [Archived](https://web.archive.org/web/20240304143650/https://www.anthropic.com/news/claude-3-family) from the original on 2024-03-04. Retrieved 2024-03-04.\\n  229. **^** [\"Fugaku-LLM/Fugaku-LLM-13B · Hugging Face\"](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B). _huggingface.co_. [Archived](https://web.archive.org/web/20240517135225/https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B) from the original on 2024-05-17. Retrieved 2024-05-17.\\n  230. **^** [\"Phi-3\"](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms). _azure.microsoft.com_. 23 April 2024. [Archived](https://web.archive.org/web/20240427043835/https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) from the original on 2024-04-27. Retrieved 2024-04-28.\\n  231. **^** [\"Phi-3 Model Documentation\"](https://huggingface.co/docs/transformers/main/en/model_doc/phi3). _huggingface.co_. [Archived](https://web.archive.org/web/20240513141513/https://huggingface.co/docs/transformers/main/en/model_doc/phi3) from the original on 2024-05-13. Retrieved 2024-04-28.\\n  232. **^** [\"Qwen2\"](https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5). _[GitHub](/wiki/GitHub \"GitHub\")_. [Archived](https://web.archive.org/web/20240617072401/https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5) from the original on 2024-06-17. Retrieved 2024-06-17.\\n  233. **^** [\"nvidia/Nemotron-4-340B-Base · Hugging Face\"](https://huggingface.co/nvidia/Nemotron-4-340B-Base). _huggingface.co_. 2024-06-14. [Archived](https://web.archive.org/web/20240615010323/https://huggingface.co/nvidia/Nemotron-4-340B-Base) from the original on 2024-06-15. Retrieved 2024-06-15.\\n  234. **^** [\"Nemotron-4 340B | Research\"](https://research.nvidia.com/publication/2024-06_nemotron-4-340b). _research.nvidia.com_. [Archived](https://web.archive.org/web/20240615010323/https://research.nvidia.com/publication/2024-06_nemotron-4-340b) from the original on 2024-06-15. Retrieved 2024-06-15.\\n  235. **^** [\"The Llama 3 Herd of Models\" (July 23, 2024) Llama Team, AI @ Meta](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)\\n  236. **^** [\"llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models\"](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md). _GitHub_. [Archived](https://web.archive.org/web/20240723151851/https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) from the original on 2024-07-23. Retrieved 2024-07-23.\\n\\n## Further reading\\n\\n[[edit](/w/index.php?title=Large_language_model&action=edit&section=39 \"Edit\\nsection: Further reading\")]\\n\\n  * [Jurafsky, Dan](/wiki/Dan_Jurafsky \"Dan Jurafsky\"), Martin, James. H. [_Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition_](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf), 3rd Edition draft, 2023.\\n  * Zhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2303.18223](https://arxiv.org/abs/2303.18223) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  * Kaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2307.10169](https://arxiv.org/abs/2307.10169) [[cs.CL](https://arxiv.org/archive/cs.CL)].\\n  * Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2023-06-01). \"A Survey on Multimodal Large Language Models\". [arXiv](/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv \\\\(identifier\\\\)\"):[2306.13549](https://arxiv.org/abs/2306.13549) [[cs.CV](https://arxiv.org/archive/cs.CV)].\\n  * [\"AI Index Report 2024 – Artificial Intelligence Index\"](https://aiindex.stanford.edu/report/). _aiindex.stanford.edu_. Retrieved 2024-05-05.\\n  * Frank, Michael C. (27 June 2023). [\"Baby steps in evaluating the capacities of large language models\"](https://www.nature.com/articles/s44159-023-00211-x). _Nature Reviews Psychology_. **2** (8): 451–452. [doi](/wiki/Doi_\\\\(identifier\\\\) \"Doi \\\\(identifier\\\\)\"):[10.1038/s44159-023-00211-x](https://doi.org/10.1038%2Fs44159-023-00211-x). [ISSN](/wiki/ISSN_\\\\(identifier\\\\) \"ISSN \\\\(identifier\\\\)\") [2731-0574](https://search.worldcat.org/issn/2731-0574). [S2CID](/wiki/S2CID_\\\\(identifier\\\\) \"S2CID \\\\(identifier\\\\)\") [259713140](https://api.semanticscholar.org/CorpusID:259713140). Retrieved 2 July 2023.\\n\\n  * [v](/wiki/Template:Natural_language_processing \"Template:Natural language processing\")\\n  * [t](/wiki/Template_talk:Natural_language_processing \"Template talk:Natural language processing\")\\n  * [e](/wiki/Special:EditPage/Template:Natural_language_processing \"Special:EditPage/Template:Natural language processing\")\\n\\n[Natural language processing](/wiki/Natural_language_processing \"Natural\\nlanguage processing\")  \\n---  \\nGeneral terms|\\n\\n  * [AI-complete](/wiki/AI-complete \"AI-complete\")\\n  * [Bag-of-words](/wiki/Bag-of-words_model \"Bag-of-words model\")\\n  * [n-gram](/wiki/N-gram \"N-gram\")\\n    * [Bigram](/wiki/Bigram \"Bigram\")\\n    * [Trigram](/wiki/Trigram \"Trigram\")\\n  * [Computational linguistics](/wiki/Computational_linguistics \"Computational linguistics\")\\n  * [Natural language understanding](/wiki/Natural_language_understanding \"Natural language understanding\")\\n  * [Stop words](/wiki/Stop_word \"Stop word\")\\n  * [Text processing](/wiki/Text_processing \"Text processing\")\\n\\n  \\n[Text analysis](/wiki/Text_mining \"Text mining\")|\\n\\n  * [Argument mining](/wiki/Argument_mining \"Argument mining\")\\n  * [Collocation extraction](/wiki/Collocation_extraction \"Collocation extraction\")\\n  * [Concept mining](/wiki/Concept_mining \"Concept mining\")\\n  * [Coreference resolution](/wiki/Coreference#Coreference_resolution \"Coreference\")\\n  * [Deep linguistic processing](/wiki/Deep_linguistic_processing \"Deep linguistic processing\")\\n  * [Distant reading](/wiki/Distant_reading \"Distant reading\")\\n  * [Information extraction](/wiki/Information_extraction \"Information extraction\")\\n  * [Named-entity recognition](/wiki/Named-entity_recognition \"Named-entity recognition\")\\n  * [Ontology learning](/wiki/Ontology_learning \"Ontology learning\")\\n  * [Parsing](/wiki/Parsing \"Parsing\")\\n    * [Semantic parsing](/wiki/Semantic_parsing \"Semantic parsing\")\\n    * [Syntactic parsing](/wiki/Syntactic_parsing_\\\\(computational_linguistics\\\\) \"Syntactic parsing \\\\(computational linguistics\\\\)\")\\n  * [Part-of-speech tagging](/wiki/Part-of-speech_tagging \"Part-of-speech tagging\")\\n  * [Semantic analysis](/wiki/Semantic_analysis_\\\\(machine_learning\\\\) \"Semantic analysis \\\\(machine learning\\\\)\")\\n  * [Semantic role labeling](/wiki/Semantic_role_labeling \"Semantic role labeling\")\\n  * [Semantic decomposition](/wiki/Semantic_decomposition_\\\\(natural_language_processing\\\\) \"Semantic decomposition \\\\(natural language processing\\\\)\")\\n  * [Semantic similarity](/wiki/Semantic_similarity \"Semantic similarity\")\\n  * [Sentiment analysis](/wiki/Sentiment_analysis \"Sentiment analysis\")\\n\\n  * [Terminology extraction](/wiki/Terminology_extraction \"Terminology extraction\")\\n  * [Text mining](/wiki/Text_mining \"Text mining\")\\n  * [Textual entailment](/wiki/Textual_entailment \"Textual entailment\")\\n  * [Truecasing](/wiki/Truecasing \"Truecasing\")\\n  * [Word-sense disambiguation](/wiki/Word-sense_disambiguation \"Word-sense disambiguation\")\\n  * [Word-sense induction](/wiki/Word-sense_induction \"Word-sense induction\")\\n\\n| [Text segmentation](/wiki/Text_segmentation \"Text segmentation\")|\\n\\n  * [Compound-term processing](/wiki/Compound-term_processing \"Compound-term processing\")\\n  * [Lemmatisation](/wiki/Lemmatisation \"Lemmatisation\")\\n  * [Lexical analysis](/wiki/Lexical_analysis \"Lexical analysis\")\\n  * [Text chunking](/wiki/Shallow_parsing \"Shallow parsing\")\\n  * [Stemming](/wiki/Stemming \"Stemming\")\\n  * [Sentence segmentation](/wiki/Sentence_boundary_disambiguation \"Sentence boundary disambiguation\")\\n  * [Word segmentation](/wiki/Word#Word_boundaries \"Word\")\\n\\n  \\n---|---  \\n  \\n[Automatic summarization](/wiki/Automatic_summarization \"Automatic\\nsummarization\")|\\n\\n  * [Multi-document summarization](/wiki/Multi-document_summarization \"Multi-document summarization\")\\n  * [Sentence extraction](/wiki/Sentence_extraction \"Sentence extraction\")\\n  * [Text simplification](/wiki/Text_simplification \"Text simplification\")\\n\\n  \\n[Machine translation](/wiki/Machine_translation \"Machine translation\")|\\n\\n  * [Computer-assisted](/wiki/Computer-assisted_translation \"Computer-assisted translation\")\\n  * [Example-based](/wiki/Example-based_machine_translation \"Example-based machine translation\")\\n  * [Rule-based](/wiki/Rule-based_machine_translation \"Rule-based machine translation\")\\n  * [Statistical](/wiki/Statistical_machine_translation \"Statistical machine translation\")\\n  * [Transfer-based](/wiki/Transfer-based_machine_translation \"Transfer-based machine translation\")\\n  * [Neural](/wiki/Neural_machine_translation \"Neural machine translation\")\\n\\n  \\n[Distributional semantics](/wiki/Distributional_semantics \"Distributional\\nsemantics\") models|\\n\\n  * [BERT](/wiki/BERT_\\\\(language_model\\\\) \"BERT \\\\(language model\\\\)\")\\n  * [Document-term matrix](/wiki/Document-term_matrix \"Document-term matrix\")\\n  * [Explicit semantic analysis](/wiki/Explicit_semantic_analysis \"Explicit semantic analysis\")\\n  * [fastText](/wiki/FastText \"FastText\")\\n  * [GloVe](/wiki/GloVe \"GloVe\")\\n  * [Language model](/wiki/Language_model \"Language model\") (large)\\n  * [Latent semantic analysis](/wiki/Latent_semantic_analysis \"Latent semantic analysis\")\\n  * [Seq2seq](/wiki/Seq2seq \"Seq2seq\")\\n  * [Word embedding](/wiki/Word_embedding \"Word embedding\")\\n  * [Word2vec](/wiki/Word2vec \"Word2vec\")\\n\\n  \\n[Language resources](/wiki/Language_resource \"Language resource\"),  \\ndatasets and corpora| | Types and  \\nstandards|\\n\\n  * [Corpus linguistics](/wiki/Corpus_linguistics \"Corpus linguistics\")\\n  * [Lexical resource](/wiki/Lexical_resource \"Lexical resource\")\\n  * [Linguistic Linked Open Data](/wiki/Linguistic_Linked_Open_Data \"Linguistic Linked Open Data\")\\n  * [Machine-readable dictionary](/wiki/Machine-readable_dictionary \"Machine-readable dictionary\")\\n  * [Parallel text](/wiki/Parallel_text \"Parallel text\")\\n  * [PropBank](/wiki/PropBank \"PropBank\")\\n  * [Semantic network](/wiki/Semantic_network \"Semantic network\")\\n  * [Simple Knowledge Organization System](/wiki/Simple_Knowledge_Organization_System \"Simple Knowledge Organization System\")\\n  * [Speech corpus](/wiki/Speech_corpus \"Speech corpus\")\\n  * [Text corpus](/wiki/Text_corpus \"Text corpus\")\\n  * [Thesaurus (information retrieval)](/wiki/Thesaurus_\\\\(information_retrieval\\\\) \"Thesaurus \\\\(information retrieval\\\\)\")\\n  * [Treebank](/wiki/Treebank \"Treebank\")\\n  * [Universal Dependencies](/wiki/Universal_Dependencies \"Universal Dependencies\")\\n\\n  \\n---|---  \\nData|\\n\\n  * [BabelNet](/wiki/BabelNet \"BabelNet\")\\n  * [Bank of English](/wiki/Bank_of_English \"Bank of English\")\\n  * [DBpedia](/wiki/DBpedia \"DBpedia\")\\n  * [FrameNet](/wiki/FrameNet \"FrameNet\")\\n  * [Google Ngram Viewer](/wiki/Google_Ngram_Viewer \"Google Ngram Viewer\")\\n  * [UBY](/wiki/UBY \"UBY\")\\n  * [WordNet](/wiki/WordNet \"WordNet\")\\n  * [Wikidata](/wiki/Wikidata \"Wikidata\")\\n\\n  \\n  \\n[Automatic identification  \\nand data capture](/wiki/Automatic_identification_and_data_capture \"Automatic\\nidentification and data capture\")|\\n\\n  * [Speech recognition](/wiki/Speech_recognition \"Speech recognition\")\\n  * [Speech segmentation](/wiki/Speech_segmentation \"Speech segmentation\")\\n  * [Speech synthesis](/wiki/Speech_synthesis \"Speech synthesis\")\\n  * [Natural language generation](/wiki/Natural_language_generation \"Natural language generation\")\\n  * [Optical character recognition](/wiki/Optical_character_recognition \"Optical character recognition\")\\n\\n  \\n[Topic model](/wiki/Topic_model \"Topic model\")|\\n\\n  * [Document classification](/wiki/Document_classification \"Document classification\")\\n  * [Latent Dirichlet allocation](/wiki/Latent_Dirichlet_allocation \"Latent Dirichlet allocation\")\\n  * [Pachinko allocation](/wiki/Pachinko_allocation \"Pachinko allocation\")\\n\\n  \\n[Computer-assisted  \\nreviewing](/wiki/Computer-assisted_reviewing \"Computer-assisted reviewing\")|\\n\\n  * [Automated essay scoring](/wiki/Automated_essay_scoring \"Automated essay scoring\")\\n  * [Concordancer](/wiki/Concordancer \"Concordancer\")\\n  * [Grammar checker](/wiki/Grammar_checker \"Grammar checker\")\\n  * [Predictive text](/wiki/Predictive_text \"Predictive text\")\\n  * [Pronunciation assessment](/wiki/Pronunciation_assessment \"Pronunciation assessment\")\\n  * [Spell checker](/wiki/Spell_checker \"Spell checker\")\\n\\n  \\n[Natural language  \\nuser interface](/wiki/Natural-language_user_interface \"Natural-language user\\ninterface\")|\\n\\n  * [Chatbot](/wiki/Chatbot \"Chatbot\")\\n  * [Interactive fiction](/wiki/Interactive_fiction \"Interactive fiction\") (c.f. [Syntax guessing](/wiki/Syntax_guessing \"Syntax guessing\"))\\n  * [Question answering](/wiki/Question_answering \"Question answering\")\\n  * [Virtual assistant](/wiki/Virtual_assistant \"Virtual assistant\")\\n  * [Voice user interface](/wiki/Voice_user_interface \"Voice user interface\")\\n\\n  \\nRelated|\\n\\n  * [Formal semantics](/wiki/Formal_semantics_\\\\(natural_language\\\\) \"Formal semantics \\\\(natural language\\\\)\")\\n  * [Hallucination](/wiki/Hallucination_\\\\(artificial_intelligence\\\\) \"Hallucination \\\\(artificial intelligence\\\\)\")\\n  * [Natural Language Toolkit](/wiki/Natural_Language_Toolkit \"Natural Language Toolkit\")\\n  * [spaCy](/wiki/SpaCy \"SpaCy\")\\n\\n  \\n  \\n![](https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1)\\n\\nRetrieved from\\n\"[https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1251988652](https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1251988652)\"\\n\\n[Categories](/wiki/Help:Category \"Help:Category\"):\\n\\n  * [Large language models](/wiki/Category:Large_language_models \"Category:Large language models\")\\n  * [Deep learning](/wiki/Category:Deep_learning \"Category:Deep learning\")\\n  * [Natural language processing](/wiki/Category:Natural_language_processing \"Category:Natural language processing\")\\n\\nHidden categories:\\n\\n  * [CS1: long volume value](/wiki/Category:CS1:_long_volume_value \"Category:CS1: long volume value\")\\n  * [Webarchive template wayback links](/wiki/Category:Webarchive_template_wayback_links \"Category:Webarchive template wayback links\")\\n  * [Articles with short description](/wiki/Category:Articles_with_short_description \"Category:Articles with short description\")\\n  * [Short description is different from Wikidata](/wiki/Category:Short_description_is_different_from_Wikidata \"Category:Short description is different from Wikidata\")\\n  * [Articles containing potentially dated statements from August 2024](/wiki/Category:Articles_containing_potentially_dated_statements_from_August_2024 \"Category:Articles containing potentially dated statements from August 2024\")\\n  * [All articles containing potentially dated statements](/wiki/Category:All_articles_containing_potentially_dated_statements \"Category:All articles containing potentially dated statements\")\\n  * [Articles containing potentially dated statements from 2024](/wiki/Category:Articles_containing_potentially_dated_statements_from_2024 \"Category:Articles containing potentially dated statements from 2024\")\\n  * [Articles containing potentially dated statements from June 2024](/wiki/Category:Articles_containing_potentially_dated_statements_from_June_2024 \"Category:Articles containing potentially dated statements from June 2024\")\\n  * [All accuracy disputes](/wiki/Category:All_accuracy_disputes \"Category:All accuracy disputes\")\\n  * [Articles with disputed statements from September 2024](/wiki/Category:Articles_with_disputed_statements_from_September_2024 \"Category:Articles with disputed statements from September 2024\")\\n  * [All articles with unsourced statements](/wiki/Category:All_articles_with_unsourced_statements \"Category:All articles with unsourced statements\")\\n  * [Articles with unsourced statements from February 2024](/wiki/Category:Articles_with_unsourced_statements_from_February_2024 \"Category:Articles with unsourced statements from February 2024\")\\n  * [Articles containing potentially dated statements from October 2024](/wiki/Category:Articles_containing_potentially_dated_statements_from_October_2024 \"Category:Articles containing potentially dated statements from October 2024\")\\n\\n  * This page was last edited on 19 October 2024, at 05:44 (UTC).\\n  * Text is available under the [Creative Commons Attribution-ShareAlike 4.0 License](/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License \"Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License\"); additional terms may apply. By using this site, you agree to the [Terms of Use](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use \"foundation:Special:MyLanguage/Policy:Terms of Use\") and [Privacy Policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy \"foundation:Special:MyLanguage/Policy:Privacy policy\"). Wikipedia® is a registered trademark of the [Wikimedia Foundation, Inc.](https://wikimediafoundation.org/), a non-profit organization.\\n\\n  * [Privacy policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy)\\n  * [About Wikipedia](/wiki/Wikipedia:About)\\n  * [Disclaimers](/wiki/Wikipedia:General_disclaimer)\\n  * [Contact Wikipedia](//en.wikipedia.org/wiki/Wikipedia:Contact_us)\\n  * [Code of Conduct](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct)\\n  * [Developers](https://developer.wikimedia.org)\\n  * [Statistics](https://stats.wikimedia.org/#/en.wikipedia.org)\\n  * [Cookie statement](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement)\\n  * [Mobile view](//en.m.wikipedia.org/w/index.php?title=Large_language_model&mobileaction=toggle_view_mobile)\\n\\n  * [![Wikimedia Foundation](/static/images/footer/wikimedia-button.svg)](https://wikimediafoundation.org/)\\n  * [![Powered by MediaWiki](/w/resources/assets/poweredby_mediawiki.svg)](https://www.mediawiki.org/)\\n\\n  *[v]: View this template\\n  *[t]: Discuss this template\\n  *[e]: Edit this template\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OUSaa8oB-ILU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_html_content(url):\n",
        "    \"\"\"Fetches the HTML content of the given URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTmVdg0-Imt",
        "outputId": "244be7df-9ef4-4043-9359-53f5dc0eeded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html>\n",
            "  <head>\n",
            "    <title>The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.</title>\n",
            "\n",
            "        <meta charset=\"utf-8\" />\n",
            "    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>\n",
            "    <meta http-equiv='X-UA-Compatible' content='IE=edge'>\n",
            "    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>\n",
            "\n",
            "    \n",
            "    <meta name=\"description\" content=\"Discussions:\n",
            "Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n",
            "\n",
            "\n",
            "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n",
            "\n",
            "Watch: MIT’s Deep Learning State of the Art lecture referencing this post\n",
            "\n",
            "Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\n",
            "\n",
            "In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n",
            "\n",
            "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n",
            "\n",
            "2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A High-Level Look\n",
            "Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "\" />\n",
            "    <meta property=\"og:description\" content=\"Discussions:\n",
            "Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n",
            "\n",
            "\n",
            "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n",
            "\n",
            "Watch: MIT’s Deep Learning State of the Art lecture referencing this post\n",
            "\n",
            "Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\n",
            "\n",
            "In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n",
            "\n",
            "The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n",
            "\n",
            "2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A High-Level Look\n",
            "Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "\" />\n",
            "    \n",
            "    <meta name=\"author\" content=\"Jay Alammar\" />\n",
            "\n",
            "    \n",
            "    <meta property=\"og:title\" content=\"The Illustrated Transformer\" />\n",
            "    <meta property=\"twitter:title\" content=\"The Illustrated Transformer\" />\n",
            "    \n",
            "\n",
            "    <!--[if lt IE 9]>\n",
            "      <script src=\"http://html5shiv.googlecode.com/svn/trunk/html5.js\"></script>\n",
            "    <![endif]-->\n",
            "\n",
            "    <script src=\"/js/jquery-3.1.1.slim.min.js\"></script>\n",
            "    <script type=\"text/javascript\" src=\"/js/d3.min.js\"></script>\n",
            "    <script type=\"text/javascript\" src=\"/js/d3-selection-multi.v0.4.min.js\"></script>\n",
            "    <script type=\"text/javascript\" src=\"/js/d3-jetpack.js\"></script>\n",
            "\n",
            "    <link rel=\"stylesheet\" href=\"/css/bootstrap.min.css\" />\n",
            "    <link rel=\"stylesheet\" href=\"/css/bootstrap-theme.min.css\" />\n",
            "    <script src=\"/js/bootstrap.min.js\" > </script>\n",
            "\n",
            "    <link rel=\"stylesheet\" type=\"text/css\" href=\"/bower_components/jquery.gifplayer/dist/gifplayer.css\"/>\n",
            "    <script type=\"text/javascript\" src=\"/bower_components/jquery.gifplayer/dist/jquery.gifplayer.js\"></script>\n",
            "\n",
            "    <!--\n",
            "    <script data-main=\"scripts/main\" src=\"scripts/require.js\"></script>\n",
            "    -->\n",
            "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css\" integrity=\"sha384-wE+lCONuEo/QSfLb4AfrSk7HjWJtc4Xc1OiB2/aDBzHzjnlBP4SX7vjErTcwlA8C\" crossorigin=\"anonymous\">\n",
            "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js\" integrity=\"sha384-tdtuPw3yx/rnUGmnLNWXtfjb9fpmwexsd+lr6HUYnUY4B7JhB5Ty7a1mYd+kto/s\" crossorigin=\"anonymous\"></script>\n",
            "\n",
            "    <link rel=\"stylesheet\" type=\"text/css\" href=\"/style.css\" />\n",
            "    <link rel=\"alternate\" type=\"application/rss+xml\" title=\"Jay Alammar - Visualizing machine learning one concept at a time.\" href=\"/feed.xml\" />\n",
            "\n",
            "    <meta name=\"viewport\" content=\"width=device-width\">\n",
            "    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->\n",
            "\n",
            "    <!-- Piwik -->\n",
            "    <!-- Piwik\n",
            "    <script type=\"text/javascript\">\n",
            "        var _paq = _paq || [];\n",
            "        _paq.push([\"setDomains\", [\"*.example.org\"]]);\n",
            "        _paq.push(['trackPageView']);\n",
            "        _paq.push(['enableLinkTracking']);\n",
            "        (function() {\n",
            "            var u=\"https://a.jalammar.com/\";\n",
            "            _paq.push(['setTrackerUrl', u+'piwik.php']);\n",
            "            _paq.push(['setSiteId', '1']);\n",
            "            var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];\n",
            "            g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);\n",
            "        })();\n",
            "    </script>\n",
            "    <noscript><p><img src=\"https://a.jalammar.com/piwik.php?idsite=1\" style=\"border:0;\" alt=\"\" /></p></noscript>-->\n",
            "    <!-- End Piwik Code -->\n",
            "\n",
            "    <!-- End Piwik Code -->\n",
            "  </head>\n",
            "\n",
            "  <body>\n",
            "    <div class=\"wrapper-masthead\">\n",
            "      <div class=\"container\">\n",
            "        <header class=\"masthead clearfix\">\n",
            "          <a href=\"/\" class=\"site-avatar\"><img src=\"https://avatars0.githubusercontent.com/u/1007956?s=460&v=4\" /></a>\n",
            "\n",
            "          <div class=\"site-info\">\n",
            "            <h1 class=\"site-name\"><a href=\"/\">Jay Alammar</a></h1>\n",
            "            <p class=\"site-description\">Visualizing machine learning one concept at a time.<br /><a href=\"https://twitter.com/JayAlammar\">@JayAlammar</a> on Twitter. <a href=\"https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ\">YouTube Channel</a></a></p>\n",
            "          </div>\n",
            "\n",
            "          <nav>\n",
            "            <a href=\"/\">Blog</a>\n",
            "            <a href=\"/about\">About</a>\n",
            "          </nav>\n",
            "        </header>\n",
            "      </div>\n",
            "    </div>\n",
            "\n",
            "    <div id=\"main\" role=\"main\" class=\"container\">\n",
            "      <article class=\"post\">\n",
            "  <h1>The Illustrated Transformer</h1>\n",
            "\n",
            "  <div class=\"entry prediction\">\n",
            "    <p><span class=\"discussion\">Discussions:\n",
            "<a href=\"https://news.ycombinator.com/item?id=18351674\" class=\"hn-link\">Hacker News (65 points, 4 comments)</a>, <a href=\"https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/\" class=\"\">Reddit r/MachineLearning (29 points, 3 comments)</a>\n",
            "</span>\n",
            "<br />\n",
            "<span class=\"discussion\">Translations: <a href=\"https://www.mundhor.site/post/post14\">Arabic</a>, <a href=\"https://blog.csdn.net/yujianmin1990/article/details/85221271\">Chinese (Simplified) 1</a>, <a href=\"https://blog.csdn.net/qq_36667170/article/details/124359818\">Chinese (Simplified) 2</a>, <a href=\"https://a-coles.github.io/2020/11/15/transformer-illustre.html\">French 1</a>, <a href=\"https://lbourdois.github.io/blog/nlp/Transformer/\">French 2</a>, <a href=\"https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348\">Italian</a>, <a href=\"https://tips-memo.com/translation-jayalmmar-transformer\">Japanese</a>, <a href=\"https://nlpinkorean.github.io/illustrated-transformer/\">Korean</a>, <a href=\"http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/\">Persian</a>, <a href=\"https://habr.com/ru/post/486358/\">Russian</a>, <a href=\"https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/\">Spanish 1</a>, <a href=\"https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp\">Spanish 2</a>, <a href=\"https://trituenhantao.io/tin-tuc/minh-hoa-transformer/\">Vietnamese</a></span>\n",
            "<br />\n",
            "<span class=\"discussion\">Watch: MIT’s <a href=\"https://youtu.be/53YvP6gdD7U?t=432\">Deep Learning State of the Art</a> lecture referencing this post</span>\n",
            "<br />\n",
            "<span class=\"discussion\">Featured in courses at <a href=\"https://web.stanford.edu/class/cs224n/\">Stanford</a>, <a href=\"https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers\">Harvard</a>, <a href=\"https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf\">MIT</a>, <a href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\">Princeton</a>, <a href=\"https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf\">CMU</a> and others</span></p>\n",
            "\n",
            "<p>In the <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href=\"https://cloud.google.com/tpu/\">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>\n",
            "\n",
            "<p>The Transformer was proposed in the paper <a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href=\"https://github.com/tensorflow/tensor2tensor\">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>\n",
            "\n",
            "<p><strong>2020 Update</strong>: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:</p>\n",
            "\n",
            "<div style=\"text-align:center\">\n",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-QH8fRhqFHM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" style=\"\n",
            " width: 100%;\n",
            " max-width: 560px;\" allowfullscreen=\"\"></iframe>\n",
            "</div>\n",
            "<h2 id=\"a-high-level-look\">A High-Level Look</h2>\n",
            "<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/the_transformer_3.png\" />\n",
            "</div>\n",
            "\n",
            "<!--more-->\n",
            "\n",
            "<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/The_transformer_encoders_decoders.png\" />\n",
            "</div>\n",
            "\n",
            "<p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/The_transformer_encoder_decoder_stack.png\" />\n",
            "</div>\n",
            "\n",
            "<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/Transformer_encoder.png\" />\n",
            "</div>\n",
            "\n",
            "<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>\n",
            "\n",
            "<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p>\n",
            "\n",
            "<p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">seq2seq models</a>).</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/Transformer_decoder.png\" />\n",
            "</div>\n",
            "\n",
            "<h2 id=\"bringing-the-tensors-into-the-picture\">Bringing The Tensors Into The Picture</h2>\n",
            "\n",
            "<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>\n",
            "\n",
            "<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an <a href=\"https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\">embedding algorithm</a>.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/embeddings.png\" />\n",
            "  <br />\n",
            "  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.\n",
            "</div>\n",
            "\n",
            "<p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>\n",
            "\n",
            "<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/encoder_with_tensors.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>\n",
            "\n",
            "<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>\n",
            "\n",
            "<h2 id=\"now-were-encoding\">Now We’re Encoding!</h2>\n",
            "\n",
            "<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/encoder_with_tensors_2.png\" />\n",
            "  <br />\n",
            "  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.\n",
            "</div>\n",
            "\n",
            "<h2 id=\"self-attention-at-a-high-level\">Self-Attention at a High Level</h2>\n",
            "<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>\n",
            "\n",
            "<p>Say the following sentence is an input sentence we want to translate:</p>\n",
            "\n",
            "<p>”<code class=\"language-plaintext highlighter-rouge\">The animal didn't cross the street because it was too tired</code>”</p>\n",
            "\n",
            "<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>\n",
            "\n",
            "<p>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p>\n",
            "\n",
            "<p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p>\n",
            "\n",
            "<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_self-attention_visualization.png\" />\n",
            "  <br />\n",
            "  As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\n",
            "</div>\n",
            "\n",
            "<p>Be sure to check out the <a href=\"https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb\">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>\n",
            "\n",
            "<h2 id=\"self-attention-in-detail\">Self-Attention in Detail</h2>\n",
            "<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.</p>\n",
            "\n",
            "<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>\n",
            "\n",
            "<p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_self_attention_vectors.png\" />\n",
            "  <br />\n",
            "  Multiplying <span class=\"encoder\">x1</span> by the <span class=\"decoder\">WQ</span> weight matrix produces <span class=\"decoder\">q1</span>, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\n",
            "</div>\n",
            "\n",
            "<p><br />\n",
            "<br /></p>\n",
            "\n",
            "<p>What are the “query”, “key”, and “value” vectors?\n",
            "<br />\n",
            "<br />\n",
            "They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p>\n",
            "\n",
            "<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p>\n",
            "\n",
            "<p>The score is calculated by taking the dot product of the <span class=\"decoder\">query vector</span> with the <span class=\"context\">key vector</span> of the respective word we’re scoring. So if we’re processing the self-attention for the word in position <span class=\"encoder\">#1</span>, the first score would be the dot product of <span class=\"decoder\">q1</span> and <span class=\"context\">k1</span>. The second score would be the dot product of <span class=\"decoder\">q1</span> and <span class=\"context\">k2</span>.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_self_attention_score.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>The <strong>third and fourth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/self-attention_softmax.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>\n",
            "\n",
            "<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/self-attention-output.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p>\n",
            "\n",
            "<h2 id=\"matrix-calculation-of-self-attention\">Matrix Calculation of Self-Attention</h2>\n",
            "<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix <span class=\"encoder\">X</span>, and multiplying it by the weight matrices we’ve trained (<span class=\"decoder\">WQ</span>, <span class=\"context\">WK</span>, <span class=\"step_no\">WV</span>).</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/self-attention-matrix-calculation.png\" />\n",
            "  <br />\n",
            "  Every row in the <span class=\"encoder\">X</span> matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/self-attention-matrix-calculation-2.png\" />\n",
            "  <br />\n",
            "  The self-attention calculation in matrix form\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<h2 id=\"the-beast-with-many-heads\">The Beast With Many Heads</h2>\n",
            "\n",
            "<p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p>\n",
            "\n",
            "<ol>\n",
            "  <li>\n",
            "    <p>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.</p>\n",
            "  </li>\n",
            "  <li>\n",
            "    <p>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>\n",
            "  </li>\n",
            "</ol>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "   <img src=\"/images/t/transformer_attention_heads_qkv.png\" />\n",
            "   <br />\n",
            "   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\n",
            " </div>\n",
            "\n",
            "<p><br />\n",
            "If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_attention_heads_z.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p>\n",
            "\n",
            "<p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_attention_heads_weight_matrix_o.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_multi-headed_self-attention-recap.png\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_self-attention_visualization_2.png\" />\n",
            "  <br />\n",
            "  As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model's representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_self-attention_visualization_3.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<h2 id=\"representing-the-order-of-the-sequence-using-positional-encoding\">Representing The Order of The Sequence Using Positional Encoding</h2>\n",
            "<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>\n",
            "\n",
            "<p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_positional_encoding_vectors.png\" />\n",
            "  <br />\n",
            "  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\n",
            "</div>\n",
            "<p><br /></p>\n",
            "\n",
            "<p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_positional_encoding_example.png\" />\n",
            "  <br />\n",
            "  A real example of positional encoding with a toy embedding size of 4\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>What might this pattern look like?</p>\n",
            "\n",
            "<p>In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_positional_encoding_large_example.png\" />\n",
            "  <br />\n",
            "  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\n",
            "</div>\n",
            "\n",
            "<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href=\"https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py\"><code class=\"language-plaintext highlighter-rouge\">get_timing_signal_1d()</code></a>. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>\n",
            "\n",
            "<p><strong>July 2020 Update:</strong> \n",
            "The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href=\"https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb\">Here’s the code to generate it</a>:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/attention-is-all-you-need-positional-encoding.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<h2 id=\"the-residuals\">The Residuals</h2>\n",
            "<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href=\"https://arxiv.org/abs/1607.06450\">layer-normalization</a> step.</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_resideual_layer_norm.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_resideual_layer_norm_2.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_resideual_layer_norm_3.png\" />\n",
            "  <br />\n",
            "</div>\n",
            "\n",
            "<h2 id=\"the-decoder-side\">The Decoder Side</h2>\n",
            "<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>\n",
            "\n",
            "<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_decoding_1.gif\" />\n",
            "  <br />\n",
            "  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\n",
            "</div>\n",
            "\n",
            "<p>The following steps repeat the process until a special <end of=\"\" sentence=\"\"> symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</end></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_decoding_2.gif\" />\n",
            "  <br />\n",
            "\n",
            "</div>\n",
            "\n",
            "<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>\n",
            "\n",
            "<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code class=\"language-plaintext highlighter-rouge\">-inf</code>) before the softmax step in the self-attention calculation.</p>\n",
            "\n",
            "<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>\n",
            "\n",
            "<h2 id=\"the-final-linear-and-softmax-layer\">The Final Linear and Softmax Layer</h2>\n",
            "\n",
            "<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>\n",
            "\n",
            "<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>\n",
            "\n",
            "<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>\n",
            "\n",
            "<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<div class=\"img-div-any-width\">\n",
            "  <img src=\"/images/t/transformer_decoder_output_softmax.png\" />\n",
            "  <br />\n",
            "  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<h2 id=\"recap-of-training\">Recap Of Training</h2>\n",
            "<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>\n",
            "\n",
            "<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>\n",
            "\n",
            "<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;” (short for ‘end of sentence’)).</p>\n",
            "\n",
            "<div class=\"img-div\">\n",
            "   <img src=\"/images/t/vocabulary.png\" />\n",
            "   <br />\n",
            "   The output vocabulary of our model is created in the preprocessing phase before we even begin training.\n",
            " </div>\n",
            "\n",
            "<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:</p>\n",
            "\n",
            "<div class=\"img-div\">\n",
            "  <img src=\"/images/t/one-hot-vocabulary-example.png\" />\n",
            "  <br />\n",
            "  Example: one-hot encoding of our output vocabulary\n",
            "</div>\n",
            "\n",
            "<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>\n",
            "\n",
            "<h2 id=\"the-loss-function\">The Loss Function</h2>\n",
            "<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>\n",
            "\n",
            "<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>\n",
            "\n",
            "<div class=\"img-div\">\n",
            "  <img src=\"/images/t/transformer_logits_output_and_label.png\" />\n",
            "  <br />\n",
            "  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\n",
            "</div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  <a href=\"https://colah.github.io/posts/2015-09-Visual-Information/\">cross-entropy</a> and <a href=\"https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\">Kullback–Leibler divergence</a>.</p>\n",
            "\n",
            "<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>\n",
            "\n",
            "<ul>\n",
            "  <li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>\n",
            "  <li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>\n",
            "  <li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>\n",
            "  <li>And so on, until the fifth output distribution indicates ‘<code class=\"language-plaintext highlighter-rouge\">&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>\n",
            "</ul>\n",
            "\n",
            "<div class=\"img-div\">\n",
            "   <img src=\"/images/t/output_target_probability_distributions.png\" />\n",
            "   <br />\n",
            "   The targeted probability distributions we'll train our model against in the training example for one sample sentence.\n",
            " </div>\n",
            "\n",
            "<p><br /></p>\n",
            "\n",
            "<p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p>\n",
            "\n",
            "<div class=\"img-div\">\n",
            "    <img src=\"/images/t/output_trained_model_probability_distributions.png\" />\n",
            "    <br />\n",
            "    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href=\"https://www.youtube.com/watch?v=TIgfjmp-4BA\">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.\n",
            "</div>\n",
            "\n",
            "<p>Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>\n",
            "\n",
            "<h2 id=\"go-forth-and-transform\">Go Forth And Transform</h2>\n",
            "\n",
            "<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>\n",
            "\n",
            "<ul>\n",
            "  <li>Read the <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> paper, the Transformer blog post (<a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href=\"https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html\">Tensor2Tensor announcement</a>.</li>\n",
            "  <li>Watch <a href=\"https://www.youtube.com/watch?v=rBCqOTEfxvg\">Łukasz Kaiser’s talk</a> walking through the model and its details</li>\n",
            "  <li>Play with the <a href=\"https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb\">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>\n",
            "  <li>Explore the <a href=\"https://github.com/tensorflow/tensor2tensor\">Tensor2Tensor repo</a>.</li>\n",
            "</ul>\n",
            "\n",
            "<p>Follow-up works:</p>\n",
            "\n",
            "<ul>\n",
            "  <li><a href=\"https://arxiv.org/abs/1706.03059\">Depthwise Separable Convolutions for Neural Machine Translation</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1706.05137\">One Model To Learn Them All</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1801.09797\">Discrete Autoencoders for Sequence Models</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1801.10198\">Generating Wikipedia by Summarizing Long Sequences</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1802.05751\">Image Transformer</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1804.00247\">Training Tips for the Transformer Model</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1803.02155\">Self-Attention with Relative Position Representations</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1803.03382\">Fast Decoding in Sequence Models using Discrete Latent Variables</a></li>\n",
            "  <li><a href=\"https://arxiv.org/abs/1804.04235\">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>\n",
            "</ul>\n",
            "\n",
            "<h2 id=\"acknowledgements\">Acknowledgements</h2>\n",
            "<p>Thanks to <a href=\"https://twitter.com/ilblackdragon\">Illia Polosukhin</a>, <a href=\"http://jakob.uszkoreit.net/\">Jakob Uszkoreit</a>, <a href=\"https://www.linkedin.com/in/llion-jones-9ab3064b\">Llion Jones </a>, <a href=\"https://ai.google/research/people/LukaszKaiser\">Lukasz Kaiser</a>, <a href=\"https://twitter.com/nikiparmar09\">Niki Parmar</a>, and <a href=\"https://dblp.org/pers/hd/s/Shazeer:Noam\">Noam Shazeer</a> for providing feedback on earlier versions of this post.</p>\n",
            "\n",
            "<p>Please hit me up on <a href=\"https://twitter.com/JayAlammar\">Twitter</a> for any corrections or feedback.</p>\n",
            "\n",
            "  </div>\n",
            "\n",
            "  <div class=\"date\">\n",
            "    Written on June 27, 2018\n",
            "  </div>\n",
            "\n",
            "  \n",
            "</article>\n",
            "\n",
            "    </div>\n",
            "\n",
            "\n",
            "\n",
            "    <!-- Begin Mailchimp Signup Form -->\n",
            "    <link href=\"//cdn-images.mailchimp.com/embedcode/classic-10_7.css\" rel=\"stylesheet\" type=\"text/css\">\n",
            "    <style type=\"text/css\">\n",
            "    \t#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }\n",
            "    \t/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.\n",
            "    \t   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */\n",
            "    </style>\n",
            "    <div id=\"mc_embed_signup\">\n",
            "    <form action=\"https://github.us19.list-manage.com/subscribe/post?u=2a4ade7dafcdbbf2eb4aae3cf&amp;id=f1f8c03f13\" method=\"post\" id=\"mc-embedded-subscribe-form\" name=\"mc-embedded-subscribe-form\" class=\"validate\" target=\"_blank\" novalidate>\n",
            "        <div id=\"mc_embed_signup_scroll\">\n",
            "    \t<h2>Subscribe to get notified about upcoming posts by email</h2>\n",
            "    <div class=\"mc-field-group\">\n",
            "    \t<label for=\"mce-EMAIL\">Email Address </label>\n",
            "    \t<input type=\"email\" value=\"\" name=\"EMAIL\" class=\"required email\" id=\"mce-EMAIL\">\n",
            "    </div>\n",
            "    \t<div id=\"mce-responses\" class=\"clear\">\n",
            "    \t\t<div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\n",
            "    \t\t<div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\n",
            "    \t</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\n",
            "        <div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_2a4ade7dafcdbbf2eb4aae3cf_f1f8c03f13\" tabindex=\"-1\" value=\"\"></div>\n",
            "        <div class=\"clear\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\n",
            "        </div>\n",
            "    </form>\n",
            "    </div>\n",
            "    <script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script>\n",
            "    <!--End mc_embed_signup-->\n",
            "\n",
            "<div style=\"padding: 10px 0 10px 3%; color: #555; font-size:85%\">\n",
            "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
            "\n",
            "<br/>\n",
            "Attribution example:\n",
            "<br/>\n",
            "<i>Alammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from <a href=\"https://jalammar.github.io/illustrated-transformer/\">https://jalammar.github.io/illustrated-transformer/</a></i>\n",
            "\n",
            "<br/><br/>\n",
            "Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the <a href=\"/about\">about page</a>.\n",
            "</div>\n",
            "\n",
            "\n",
            "    <div class=\"wrapper-footer\">\n",
            "      <div class=\"container\">\n",
            "        <footer class=\"footer\">\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "<a href=\"https://github.com/jalammar\"><i class=\"svg-icon github\"></i></a>\n",
            "\n",
            "<a href=\"https://www.linkedin.com/in/jalammar\"><i class=\"svg-icon linkedin\"></i></a>\n",
            "\n",
            "\n",
            "<a href=\"https://www.twitter.com/jayalammar\"><i class=\"svg-icon twitter\"></i></a>\n",
            "\n",
            "\n",
            "\n",
            "        </footer>\n",
            "      </div>\n",
            "    </div>\n",
            "\n",
            "    \n",
            "\t<!-- Google tag (gtag.js) -->\n",
            "\t<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-R9S1R9LV9P\"></script>\n",
            "\t<script>\n",
            "\t  window.dataLayer = window.dataLayer || [];\n",
            "\t  function gtag(){dataLayer.push(arguments);}\n",
            "\t  gtag('js', new Date());\n",
            "\n",
            "\t  gtag('config', 'G-R9S1R9LV9P');\n",
            "\t</script>\n",
            "\t<!-- End Google Analytics -->\n",
            "\n",
            "\n",
            "  </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "url = 'https://jalammar.github.io/illustrated-transformer/'\n",
        "html_content = get_html_content(url)\n",
        "if html_content:\n",
        "    print(html_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMq1xagC1FP"
      },
      "source": [
        "#### PDF Parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTVw0NuuS34N"
      },
      "outputs": [],
      "source": [
        "chromadb_client = chromadb.EphemeralClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYr7C__43k9U"
      },
      "outputs": [],
      "source": [
        "pdf_folder_location = \"papers\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcB04z_U3dy2"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyPDFDirectoryLoader(pdf_folder_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "170i0BkE3ves"
      },
      "outputs": [],
      "source": [
        "character_text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=16,\n",
        "    separator=''\n",
        ")\n",
        "\n",
        "report_chunks = pdf_loader.load_and_split(character_text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNaj4AOi4UIY",
        "outputId": "fc1bcae3-bafd-4c58-9f0f-5b735c78f9d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'papers/Unified model of spatial and episodic memory.pdf', 'page': 0}, page_content='Received 20 December 2001\\nAccepted 11 March 2002\\nPublishedonline 9 May 2002\\nA uniﬁed model of spatial and episodic memory\\nEdmund T. Rolls*, Simon M. Stringer and Thomas P. Trappenberg\\nDepartmentofExperimentalPsychology,UniversityofOxford,SouthParksRoad,OxfordOX13UD,UK\\nMedial temporal lobe structures including the hippocampus are implicated by separate investigations in\\nboth episodic memory and spatial function. We show that a single recurrent attractor network can store\\nboth the discrete memories that char')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "report_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO7HNTt-4Qup",
        "outputId": "be0cf069-2ba0-4b0b-f749-fa8c2725ccec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "590"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(report_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUdS1yLi7o3R"
      },
      "outputs": [],
      "source": [
        "chunks = [Document(id=i, page_content = d.page_content, metadata = d.metadata) for i, d in zip(range(len(report_chunks)),report_chunks)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbAYiYHMPy6J"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    chunks,\n",
        "    embedding_model,\n",
        "    collection_name=\"full_document_chunks\",\n",
        "    persist_directory='./reports_db'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVChpoYPTT2i"
      },
      "outputs": [],
      "source": [
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"Source\",\n",
        "        description=\"Name of the research paper\",\n",
        "        type=\"string\"\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"page\",\n",
        "        description=\"page within the research paper that the document belongs to\",\n",
        "        type=\"integer\"\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZginbx-TXGv"
      },
      "outputs": [],
      "source": [
        "\n",
        "document_content_description = \"information mentioned in the research paper\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E7TtiCiTZiN"
      },
      "outputs": [],
      "source": [
        "structured_retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectorstore,\n",
        "    document_content_description,\n",
        "    metadata_field_info\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r03nQgZTWJDL"
      },
      "outputs": [],
      "source": [
        "user_input = \"what are the parts involved in memory?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ReBTPnPTbzw",
        "outputId": "a17d5d62-f009-4f1e-bbc0-44c4d7b9912a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': 'papers/Unified model of spatial and episodic memory.pdf'}, page_content='hippocampus\\n1. INTRODUCTION\\nEvidence implicating the hippocampus and connected\\nstructures in the medial temporal lobe in episodic memory\\nis that in humans with bilateral damage to this region ante-\\nrograde amnesia is produced, with the memory for parti-\\ncular events and episodes being particularly impaired\\n(Squire 1992; Rempel-Clower et al. 1996). We use the\\nterm episodic memory to capture the memory of a single\\nevent that might occur when a previous particular\\noccasion is recalled. The event (something th'),\n",
              " Document(metadata={'page': 1, 'source': 'papers/The role of REM sleep theta in emotional memory.pdf'}, page_content='chances of survival ( Hamann ,2001). The neural mechanism\\nunderlying the influence of emotion on long-term memory\\nretention involves co-activation of the hippocampus and the\\namygdala—the emotional center of the brain. The amygdala\\nappears to modulate hippocampal activity, thus facilitating the\\npreferential encoding of emotional memories and potentially\\ntheir tagging for future consolidation.\\nFrontiers in Psychology |www.frontiersin.org October 2015 |Volume 6|Article 1439 2'),\n",
              " Document(metadata={'page': 10, 'source': 'papers/evolution of hippo in relation to neocortex.pdf'}, page_content='Jul;  51(3):  1242–52.\\nZammit AR, Ezzati A, Zimmerman ME, Lipton \\nRB, Lipton ML, Katz MJ. Roles of hippocam-\\npal subfields in verbal and visual episodic \\nmemory. Behav Brain Res. 2017 Jan;  317:  157–\\n62.\\nZhao M. Human spatial representation: what we \\ncannot learn from the studies of rodent navi-\\ngation. J Neurophysiol. 2018 Nov;  120(5):  \\n2453–65.\\nDownloaded by: \\nLund University Libraries\\n130.235.66.10 - 7/21/2019 4:51:43 AM'),\n",
              " Document(metadata={'page': 6, 'source': 'papers/Unified model of spatial and episodic memory.pdf'}, page_content='. R. & Amaral,\\nD. G. 1996 Three cases of enduring memory impairmentafter bilateral damage limited to the hippocampal formation.J.Neurosci. 16, 5233 –5255.\\nRobertson, R. G., Rolls, E. T. & Georges-Francois, P. 1998\\nSpatial view cells in the primate hippocampus: effects ofremoval of view details. J.Neurophysiol. 79, 1145 –1156.\\nRolls, E. T. 1987 Information representation, processing and\\nstorage in the brain: analysis at the single neuron level. In\\nThe neural and molecular bases of learning (ed. J.-P.\\nProc.R.')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_retriever.invoke(user_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHnRG0ufY_BE"
      },
      "outputs": [],
      "source": [
        "docs = vectorstore.similarity_search(user_input, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leWyJqH6ZJfl",
        "outputId": "c6111baf-ee13-442c-c3ad-7de2f4dde7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source papers/Unified model of spatial and episodic memory.pdf\n",
            "page 0 \n",
            "\n",
            "hippocampus\n",
            "1. INTRODUCTION\n",
            "Evidence implicating the hippocampus and connected\n",
            "structures in the medial temporal lobe in episodic memory\n",
            "is that in humans with bilateral damage to this region ante-\n",
            "rograde amnesia is produced, with the memory for parti-\n",
            "cular events and episodes being particularly impaired\n",
            "(Squire 1992; Rempel-Clower et al. 1996). We use the\n",
            "term episodic memory to capture the memory of a single\n",
            "event that might occur when a previous particular\n",
            "occasion is recalled. The event (something th\n",
            "source papers/Unified model of spatial and episodic memory.pdf\n",
            "page 0 \n",
            "\n",
            "nt (something that happens\n",
            "at a particular time) might include both spatial and non-\n",
            "spatial components, including for example where the\n",
            "event occurred, who was present, and which objects were\n",
            "seen. (We use the term episodic memory to refer to the\n",
            "memory of a particular event, and are not concerned in\n",
            "this paper with describing linked temporal sequences of\n",
            "events.) The hippocampus is also implicated in spatial\n",
            "memory. For example, damage to the hippocampal system\n",
            "in monkeys produces deﬁcits in learning abo\n",
            "source papers/Unified model of spatial and episodic memory.pdf\n",
            "page 3 \n",
            "\n",
            "ts dominate the ﬁring of the neurons.\n",
            "However, at time 700 ms both the spatial and the memory\n",
            "inputs were removed, and the network then moved into a\n",
            "state which was consistent with its memory, and the parti-\n",
            "cular state it happened to fall into was the one in which\n",
            "the memory was of spatial location 300 and of the object\n",
            "originally trained at that location. This mode of operation\n",
            "is appropriate in that incoming sensory inputs dominate,\n",
            "and not memories, and this matches human perceptual\n",
            "and memory performa\n",
            "source papers/Unified model of spatial and episodic memory.pdf\n",
            "page 0 \n",
            "\n",
            "lobe structures are\n",
            "involved in episodic memory or spatial function. In this\n",
            "paper we show that this question can be resolved by\n",
            "revealing that a single neural network can implement both\n",
            "episodic and spatial memory.\n",
            "The simplest form of network that can store discrete\n",
            "(separate) patterns that characterize the single events, or\n",
            "objects that are the parts of an episodic memory and can\n",
            "retrieve the whole memory from any of its parts, is an\n",
            "autoassociation network, illustrated in ﬁgure 1 (Kohonen\n",
            "1977; Hopﬁe\n",
            "source papers/The role of REM sleep theta in emotional memory.pdf\n",
            "page 1 \n",
            "\n",
            "chances of survival ( Hamann ,2001). The neural mechanism\n",
            "underlying the influence of emotion on long-term memory\n",
            "retention involves co-activation of the hippocampus and the\n",
            "amygdala—the emotional center of the brain. The amygdala\n",
            "appears to modulate hippocampal activity, thus facilitating the\n",
            "preferential encoding of emotional memories and potentially\n",
            "their tagging for future consolidation.\n",
            "Frontiers in Psychology |www.frontiersin.org October 2015 |Volume 6|Article 1439 2\n"
          ]
        }
      ],
      "source": [
        "for i in docs:\n",
        "  print(\"source\", i.metadata['source'])\n",
        "  print(\"page\", i.metadata['page'],\"\\n\")\n",
        "  print(i.page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nv-MIf5cJ7A"
      },
      "source": [
        "## Semantic Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTu2zzjdb73l"
      },
      "outputs": [],
      "source": [
        "# Semantic chunking\n",
        "\n",
        "semantic_text_splitter = SemanticChunker(\n",
        "    embedding_model,\n",
        "    breakpoint_threshold_type='gradient'\n",
        ")\n",
        "\n",
        "semantic_chunks = pdf_loader.load_and_split(semantic_text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Sl6VghcGBk",
        "outputId": "cead65dc-37b7-4a33-9007-91b8ca9432cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'papers/Unified model of spatial and episodic memory.pdf', 'page': 0}, page_content='spatial) and inherently separate (e.g. different objects). This approach uniﬁes the spatial and episodic approaches\\nto medial temporal lobe function, showing that both types\\nof representation must be stored in the same network if\\nassociations between them are to be learned and retrieved. Previous models of hippocampal function have focused\\nprimarily on networks that can store discrete patterns suit-\\nable for episodic memory (Marr 1971; Treves & Rolls\\n1992, 1994; McClelland et al. 1995; Rolls 1996), or on\\ncontinuous patterns suitable for spatial memory\\n(Samsonovich & McNaughton 1997; Redish & Touretzky')"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_chunks[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnTNoMUOc80N",
        "outputId": "df9925a2-de10-482e-b10a-e56ac17dcf32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "272"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(semantic_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f99LgDOeDA-m"
      },
      "outputs": [],
      "source": [
        "semantic_chunks = [Document(id=i, page_content = d.page_content, metadata = d.metadata) for i, d in zip(range(len(semantic_chunks)),semantic_chunks)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NK-xHupzenVz",
        "outputId": "d53e27da-0524-40b0-9b32-b28ce09e0a91"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: plot a graph of number of characters in each chunk of semantic_chunks\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'semantic_chunks' is defined from the previous code\n",
        "chunk_lengths = [len(chunk.page_content) for chunk in semantic_chunks]\n",
        "\n",
        "plt.hist(chunk_lengths,bins = 25, edgecolor = 'black')\n",
        "plt.xlabel('Chunk Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Number of Characters in Each Semantic Chunk')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS66HJ1Ncicn"
      },
      "outputs": [],
      "source": [
        "semanticstore = Chroma.from_documents(\n",
        "    semantic_chunks,\n",
        "    embedding_model,\n",
        "    collection_name=\"semantic_chunks\",\n",
        "    persist_directory='./reports_db')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLTpIYQxcpIR"
      },
      "outputs": [],
      "source": [
        "structured_retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    semanticstore,\n",
        "    document_content_description,\n",
        "    metadata_field_info\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JiVnAPXctvk",
        "outputId": "8c44ab84-ad92-4158-ab1d-129e636bacc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 10, 'source': 'papers/The role of REM sleep theta in emotional memory.pdf'}, page_content='545, 307–311. doi: 10.1016/0006-\\n8993(91)91303-I\\nDolcos, F., LaBar, K. S., and Cabeza, R. (2004). Interaction between the amygdala\\nand the medial temporal lobe memory system predicts better memory\\nfor emotional events. Neuron 42, 855–863. doi: 10.1016/S0896-6273(04)\\n00289-2\\nDragoi, G., and Buzsáki, G. (2006). Temporal encoding of place\\nsequences by hippocampal cell assemblies. Neuron 50, 145–157. doi:\\n10.1016/j.neuron.2006.02.023\\nDragoi, G., Carpi, D., Recce, M., Csicsvari, J., and Buzsaki, G. (1999). Interactions\\nbetween hippocampus and medial septum during sharp waves and theta\\noscillation in the behaving rat. J. Neurosci. 19, 6191–6199. Dragoi, G., and Tonegawa, S. (2011). Preplay of future place cell sequences\\nby hippocampal cellular assemblies. Nature 469, 397–401. doi:\\n10.1038/nature09633\\nEgo-Stengel, V., and Wilson, M. A. (2010). Disruption of ripple-associated\\nhippocampal activity during rest impairs spatial learning in the rat. Hippocampus 20, 1–10. doi: 10.1002/hipo.20707\\nEkstrom,A.D.,Caplan,J.B.,Ho,E.,Shattuck,K.,Fried,I.,andKahana,M.J.(2005). Human hippocampal theta activity during virtual navigation. Hippocampus 15,\\n881–889. doi: 10.1002/hipo.20109\\nEkstrom, A. D., Kahana, M. J., Caplan, J. B., Fields, T. A., Isham, E. A., Newman, E. L., et al. (2003). Cellular networks underlying human spatial navigation. Nature\\n425, 184–188. doi: 10.1038/nature01964Fishbein, W. (1971). Disruptive effects of rapid eye movement sleep deprivation\\non long-term memory. Physiol. Behav. 6, 279–282. doi: 10.1016/0031-\\n9384(71)90155-7\\nFogel, S., and Smith, C. (2006). Learning-dependent changes in sleep spindles and\\nStage 2 sleep. J. Sleep Res. 15, 250–255. doi: 10.1111/j.1365-2869.2006.00522.x\\nFogel, S. M., Smith, C. T., and Beninger, R. J. (2009). Evidence for 2-stage models\\nof sleep and memory: learning-dependent changes in spindles and theta in rats.'),\n",
              " Document(metadata={'page': 13, 'source': 'papers/The role of REM sleep theta in emotional memory.pdf'}, page_content='Learn. Mem. 6,500–508. doi: 10.1101/lm.6.5.500\\nRichardson, M. P., Strange, B. A., and Dolan, R. J. (2004). Encoding of emotional\\nmemories depends on amygdala and hippocampus and their interactions. Nat. Neurosci. 7, 278–285. doi: 10.1038/nn1190\\nRosa, R., Bonnet, M., and Kramer, M. (1983). The relationship of sleep and\\nanxiety in anxious subjects. Biol. Psychol. 16, 119–126. doi: 10.1016/0301-\\n0511(83)90058-3\\nRuch, S., Markes, O., Duss, S. B., Oppliger, D., Reber, T. P., Koenig, T., et al. (2012). Sleep stage II contributes to the consolidation of declarative memories. Neuropsychologia 50, 2389–2396. doi: 10.1016/j.neuropsychologia.2012.06.008\\nRutishauser, U., Ross, I. B., Mamelak, A. N., and Schuman, E. M. (2010). Human\\nmemory strength is predicted by theta-frequency phase-locking of single\\nneurons. Nature 464, 903–907. doi: 10.1038/nature08860\\nSagales, T., and Domino, E. F. (1973). Effects of stress and REM sleep deprivation\\non the patterns of avoidance learning and brain acetylcholine in the mouse. Psychopharmacologia 29, 307–315. doi: 10.1007/BF00429278\\nScheffzük, C., Kukushka, V. I., Vyssotski, A. L., Draguhn, A., Tort, A. B. L., and\\nBrankačk, J. (2011). Selective coupling between theta phase and neocortical\\nfast gamma oscillations during REM-sleep in mice. PLoS ONE 6:e28489. doi:\\n10.1371/journal.pone.0028489\\nSeager, M. A., Johnson, L. D., Chabot, E. S., Asaka, Y., and Berry, S.'),\n",
              " Document(metadata={'page': 13, 'source': 'papers/Boosting slow oscillation during sleep to improve memory in old people.pdf'}, page_content='Neurobiol. Dis. 2020 , 104865. [CrossRef] [PubMed]\\n4. Rasch, B.; Born, J. About sleep’s role in memory. Physiol. Rev. 2013 ,93, 681–766. [CrossRef]\\n5. Peigneux, P .; Fogel, S.; Smith, C. Memory processing in relation to sleep. In Principles and Practice of Sleep\\nMedicine , 6th ed.; Kryger, M., Roth, T., Dement, B., Eds.; Elsevier: Philadelphia, PA, USA, 2017; pp.'),\n",
              " Document(metadata={'page': 1, 'source': 'papers/evolution of hippo in relation to neocortex.pdf'}, page_content='Though seemingly disparate cognitive processes, \\nnavigation and episodic memory are both thought to rely \\non similar mental representations [Buzsáki and Moser, \\n2013; Ekstrom and Ranganath, 2018; Rolls and Wirth, \\n2018] and a core neural substrate located in the hippo-\\ncampal formation (HF) [Burgess et al., 2002; Buzsáki, \\n2005; Horner et al., 2016].')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo2lJkCgTdBq"
      },
      "outputs": [],
      "source": [
        "# Hypothetical Questions\n",
        "\n",
        "hypothetical_questions_prompt = \"\"\"\n",
        "Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\n",
        "{doc}\n",
        "Generate only a list of questions. Do not mention anything before or after the list.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU_zvow9Tn0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "hypothetical_question_documents = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eums-_-RTpp2"
      },
      "outputs": [],
      "source": [
        "\n",
        "for document in semantic_chunks:\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(hypothetical_questions_prompt.format(doc=document))\n",
        "\n",
        "        questions = response.content\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        questions = \"\"\n",
        "\n",
        "    questions_metadata = {\n",
        "        'parent_chunk_id': document.id,\n",
        "        'parent_collection': 'full_document_chunks'\n",
        "    }\n",
        "\n",
        "    hypothetical_question_documents.append(\n",
        "        Document(\n",
        "            id=document.id,\n",
        "            page_content=questions,\n",
        "            metadata=questions_metadata\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SQWoFb-TsUy"
      },
      "outputs": [],
      "source": [
        "collection = chromadb_client.get_or_create_collection(\n",
        "    name='hypothetical_questions',\n",
        "    metadata={\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function=embedding_function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZo8kerwCZDx"
      },
      "outputs": [],
      "source": [
        "# for i,d in zip(range(len(hypothetical_question_documents)), hypothetical_question_documents):\n",
        "\n",
        "#   d.metadata['parent_chunk_id'] = str(i)\n",
        "#   d.id = str(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKMBd44yUAjV"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    ids=[d.id for d in hypothetical_question_documents],\n",
        "    documents=[d.page_content for d in hypothetical_question_documents],\n",
        "    metadatas=[d.metadata for d in hypothetical_question_documents]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwFgMJWjT1JC"
      },
      "outputs": [],
      "source": [
        "# Retriever over questions\n",
        "\n",
        "hypstore = Chroma(\n",
        "    client=chromadb_client,\n",
        "    collection_name=\"hypothetical_questions\",\n",
        "    embedding_function=embedding_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K3Zhe2WUHSX"
      },
      "outputs": [],
      "source": [
        "hypo_retriever = hypstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 5}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SdcqKSNULLw"
      },
      "outputs": [],
      "source": [
        "hypothetical_questions_retrieved = hypo_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w13S4BzHUQYY",
        "outputId": "f2f777f1-f46a-4e97-9427-b57d31f9c554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?'),\n",
              " Document(metadata={'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}, page_content='1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?'),\n",
              " Document(metadata={'parent_chunk_id': 119, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the amygdala influence the consolidation of emotionally arousing memories?\\n2. What role does REM sleep play in facilitating adaptive waking behavior?\\n3. How do NMDA receptors in the dentate gyrus contribute to pattern separation in the hippocampal network?'),\n",
              " Document(metadata={'parent_chunk_id': 31, 'parent_collection': 'full_document_chunks'}, page_content='1. What factors influence the capacity of autoassociative memories in the brain?\\n2. Why might the hippocampal CA3 network require two distinct input systems?\\n3. How do attractor neural network models explain spatial maps in the hippocampus?'),\n",
              " Document(metadata={'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?')]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hypothetical_questions_retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW97PNsBUUOg"
      },
      "outputs": [],
      "source": [
        "# Query Expansion\n",
        "\n",
        "query_expansion_prompt = \"\"\"\n",
        "You are an neuroscience domain expert assisting in answering questions related to research papers.\n",
        "Perform query expansion on the question received. If there are multiple common ways of phrasing a user question \\\n",
        "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
        "of the query with the different phrasings.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
        "\n",
        "Return at least 3 versions of the question as a list.\n",
        "Generate only a list of questions. Do not mention anything before or after the list.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWNkU1eUfed"
      },
      "outputs": [],
      "source": [
        "query_expansions = llm.invoke(query_expansion_prompt.format(question=user_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI23nYnsUjGb"
      },
      "outputs": [],
      "source": [
        "query_expansions_list = query_expansions.content.split(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ridNclSHUk0m",
        "outputId": "392a1234-9283-4a75-c527-e6a6342f5041"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['- What are the brain regions involved in memory?',\n",
              " '- Which areas of the brain are responsible for memory?',\n",
              " '- What brain structures are associated with memory?']"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_expansions_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaUOvllOOAt7",
        "outputId": "9a6beb49-e814-4993-d45c-3b9b33b6afc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?'),\n",
              " Document(metadata={'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}, page_content='1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?'),\n",
              " Document(metadata={'parent_chunk_id': 119, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the amygdala influence the consolidation of emotionally arousing memories?\\n2. What role does REM sleep play in facilitating adaptive waking behavior?\\n3. How do NMDA receptors in the dentate gyrus contribute to pattern separation in the hippocampal network?'),\n",
              " Document(metadata={'parent_chunk_id': 31, 'parent_collection': 'full_document_chunks'}, page_content='1. What factors influence the capacity of autoassociative memories in the brain?\\n2. Why might the hippocampal CA3 network require two distinct input systems?\\n3. How do attractor neural network models explain spatial maps in the hippocampus?'),\n",
              " Document(metadata={'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?')]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hypo_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrTOcdbhUwG3"
      },
      "outputs": [],
      "source": [
        "context_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EthFHMcwU20J",
        "outputId": "b3d7a141-8517-4158-e606-36ea472ddc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- What are the brain regions involved in memory?\n",
            "[Document(metadata={'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}, page_content='1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?'), Document(metadata={'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?'), Document(metadata={'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?'), Document(metadata={'parent_chunk_id': 253, 'parent_collection': 'full_document_chunks'}, page_content='1. How does grid-like processing contribute to imagined navigation according to Horner et al. (2016)?\\n2. What roles do hippocampal subfields and anterior-posterior subregions play in memory encoding and retrieval as discussed by Hrybouski et al. (2016)?\\n3. What are the differences between longitudinal and transverse axis involvement in memory processes based on the findings of Hrybouski et al. (2016)?'), Document(metadata={'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}, page_content='1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?')]\n",
            "\n",
            "\n",
            "- Which areas of the brain are responsible for memory?\n",
            "[Document(metadata={'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?'), Document(metadata={'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}, page_content='1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?'), Document(metadata={'parent_chunk_id': 132, 'parent_collection': 'full_document_chunks'}, page_content='1. What are the mechanisms underlying the head direction sense in the brain?\\n2. How do hippocampal place-cell sequences contribute to the depiction of future paths to remembered goals?\\n3. What role do theta oscillations play in the consolidation of fear memory during REM sleep?'), Document(metadata={'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}, page_content='1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?'), Document(metadata={'parent_chunk_id': 229, 'parent_collection': 'full_document_chunks'}, page_content='1. How do neurons encoding abstract schemas in the hippocampus contribute to episodic-like memory in primates?\\n2. What evidence supports the existence of uniquely complex representations in the hippocampi of humans and macaques?\\n3. How do interactions between hippocampal neurons and HF influence memory processes in primates?')]\n",
            "\n",
            "\n",
            "- What brain structures are associated with memory?\n",
            "[Document(metadata={'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}, page_content='1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?'), Document(metadata={'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}, page_content='1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?'), Document(metadata={'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?'), Document(metadata={'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}, page_content='1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?'), Document(metadata={'parent_chunk_id': 31, 'parent_collection': 'full_document_chunks'}, page_content='1. What factors influence the capacity of autoassociative memories in the brain?\\n2. Why might the hippocampal CA3 network require two distinct input systems?\\n3. How do attractor neural network models explain spatial maps in the hippocampus?')]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query in query_expansions_list:\n",
        "    print(query)\n",
        "    context_list.extend([str(d.metadata) + d.page_content for d in hypo_retriever.invoke(query)])\n",
        "    print(hypo_retriever.invoke(query))\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhuMI8s8U3zE",
        "outputId": "25c6ec38-9f8e-4fff-c6da-c0fb8f00f206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"{'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?\",\n",
              " \"{'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?\",\n",
              " \"{'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?\",\n",
              " \"{'parent_chunk_id': 253, 'parent_collection': 'full_document_chunks'}1. How does grid-like processing contribute to imagined navigation according to Horner et al. (2016)?\\n2. What roles do hippocampal subfields and anterior-posterior subregions play in memory encoding and retrieval as discussed by Hrybouski et al. (2016)?\\n3. What are the differences between longitudinal and transverse axis involvement in memory processes based on the findings of Hrybouski et al. (2016)?\",\n",
              " \"{'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?\",\n",
              " \"{'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?\",\n",
              " \"{'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?\",\n",
              " \"{'parent_chunk_id': 132, 'parent_collection': 'full_document_chunks'}1. What are the mechanisms underlying the head direction sense in the brain?\\n2. How do hippocampal place-cell sequences contribute to the depiction of future paths to remembered goals?\\n3. What role do theta oscillations play in the consolidation of fear memory during REM sleep?\",\n",
              " \"{'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?\",\n",
              " \"{'parent_chunk_id': 229, 'parent_collection': 'full_document_chunks'}1. How do neurons encoding abstract schemas in the hippocampus contribute to episodic-like memory in primates?\\n2. What evidence supports the existence of uniquely complex representations in the hippocampi of humans and macaques?\\n3. How do interactions between hippocampal neurons and HF influence memory processes in primates?\",\n",
              " \"{'parent_chunk_id': 137, 'parent_collection': 'full_document_chunks'}1. How do the amygdala and hippocampus interact to encode emotional memories?\\n2. What is the relationship between sleep stages and the consolidation of declarative memories?\\n3. How does REM sleep deprivation affect avoidance learning and brain acetylcholine levels in mice?\",\n",
              " \"{'parent_chunk_id': 25, 'parent_collection': 'full_document_chunks'}1. What are the key functions of neuronal networks in the hippocampus and neocortex related to memory?\\n2. How is information represented, processed, and stored at the single neuron level in the brain?\\n3. What experimental and theoretical approaches are used to model neural plasticity?\",\n",
              " \"{'parent_chunk_id': 42, 'parent_collection': 'full_document_chunks'}1. How does the interaction between the amygdala and hippocampus influence the recall of emotional memories?\\n2. What findings did Dolcos et al. (2004) present regarding the neural mechanisms of emotional memory recall?\\n3. What role does event-related fMRI play in studying the interaction between the amygdala and hippocampus in emotional memory?\",\n",
              " \"{'parent_chunk_id': 92, 'parent_collection': 'full_document_chunks'}1. How does the interaction between the amygdala and the medial temporal lobe memory system influence memory for emotional events?\\n2. What are the effects of disrupting ripple-associated hippocampal activity during rest on spatial learning in rats?\\n3. How do learning-dependent changes in sleep spindles and Stage 2 sleep relate to memory consolidation?\",\n",
              " \"{'parent_chunk_id': 31, 'parent_collection': 'full_document_chunks'}1. What factors influence the capacity of autoassociative memories in the brain?\\n2. Why might the hippocampal CA3 network require two distinct input systems?\\n3. How do attractor neural network models explain spatial maps in the hippocampus?\"]"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk0mYPLOU53f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcAxZ2hoOuzm"
      },
      "source": [
        "## Compression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQmgfbEVRJY3"
      },
      "outputs": [],
      "source": [
        "normal_context = structured_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJoZW3e8RxqF"
      },
      "outputs": [],
      "source": [
        "print(normal_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PpPTAw4Ox97"
      },
      "outputs": [],
      "source": [
        "compressor = LLMChainExtractor.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_J6kVcWWQwI"
      },
      "outputs": [],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=structured_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rCAsNyeWS1a"
      },
      "outputs": [],
      "source": [
        "compressed_docs = compression_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEOnNGeoWTr9",
        "outputId": "cbb484f0-0a8a-46ed-99e8-8317e227050e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interaction between the amygdala and the medial temporal lobe memory system predicts better memory for emotional events. Neuron 42, 855–863. doi: 10.1016/S0896-6273(04) 00289-2\n",
            "\n",
            "Dragoi, G., and Buzsáki, G. (2006). Temporal encoding of place sequences by hippocampal cell assemblies. Neuron 50, 145–157. doi: 10.1016/j.neuron.2006.02.023\n",
            "\n",
            "Dragoi, G., Carpi, D., Recce, M., Csicsvari, J., and Buzsaki, G. (1999). Interactions between hippocampus and medial septum during sharp waves and theta oscillation in the behaving rat. J. Neurosci. 19, 6191–6199. \n",
            "\n",
            "Ego-Stengel, V., and Wilson, M. A. (2010). Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat. Hippocampus 20, 1–10. doi: 10.1002/hipo.20707\n",
            "\n",
            "Ekstrom,A.D.,Caplan,J.B.,Ho,E.,Shattuck,K.,Fried,I.,andKahana,M.J.(2005). Human hippocampal theta activity during virtual navigation. Hippocampus 15, 881–889. doi: 10.1002/hipo.20109\n",
            "Encoding of emotional memories depends on amygdala and hippocampus and their interactions.\n"
          ]
        }
      ],
      "source": [
        "for compressed_doc in compressed_docs:\n",
        "    print(compressed_doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koiHWZdwRV7_"
      },
      "source": [
        "## Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXoWU8YxWXK8"
      },
      "outputs": [],
      "source": [
        "# Reranking\n",
        "\n",
        "context_query_pairs_for_scoring = [[user_input, doc_text.page_content] for doc_text in normal_context]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "b0825ce46390470aa40e3641c01bc0e4",
            "6e1b98bfdae84452a2a8825deaf34957",
            "2f17b135685d4da0bb7fcc52e8edf8ad",
            "90db47783a054bc6ae74d0ba30f88b4d",
            "0e8bc0a398514bf4b9ab6d102ae40690",
            "5b1a41b94b51400586bb44edda71a8e4",
            "7e268ec7be934948ad3b2bbc4be5ead5",
            "6a382ec6cc9d4a38b0b5e8b888b0086d",
            "8d15d1c7354544b2ac19b64e242ddd30",
            "7782b836718f4aef81d576ce00b2b975",
            "a6c6981ca7ce4da9a50f787f8c0d6de8",
            "c08cbaf9ea3448ee9d4812aa39beb141",
            "9191959442ca4a39b66fba5f73db50bd",
            "92974ea03fae4f32a98807a5a591717c",
            "e39e95ae8f4e45578caf3ff82e703822",
            "43743f7cc7344e50b97545dea30575d8",
            "88c2cf799a8a4b5880b3d6255f728e72",
            "8388130c6dcf4c458c42ca8be687d9da",
            "1ebd9438ec8341b8b6e19ef459073296",
            "aa7e019d9467488c9536068842609070",
            "fb80bd56c96649b496215bc0ca3e8ac5",
            "cc5509d92af445dd9fd48fd23c9ce839",
            "8eeb4589234a4ec4a72e4f93fc89cbd8",
            "3b7094549bad4ff5b4362aa85a1dccc9",
            "6cc98c97bcee495093006e9f31195474",
            "33c1d58f12094af6933e04cf31e461ea",
            "2230d72debb04101bbd72a99b590c38f",
            "846a45eea8954baf94377284f65f2970",
            "acf95dcfc58240409d5fe37e5da252a4",
            "02cc0b1feaf04ecabac433657dd1e095",
            "5f74e154a25a447cbe889a92a3f3b13c",
            "358dca717dcd4d5fb65aa2de36bba7e7",
            "ade5a9db1ff94f44bd135d680a4fad75",
            "85f48927db504dd99970d6f47201d4f3",
            "d992033fe62f4334ba1dc6764fe4dc85",
            "c6e3976bbd3d413191203a2edb1566f3",
            "a547c8bee63c4754b5fd5e9f9db4d8a0",
            "c14e4c7d21a54b2fad9d91f1b5275cd3",
            "ca9f93d51fcf4d1597828a6dc5815e3f",
            "0654bac16c6f497f80f195a3b0603690",
            "f34cb487f03a4c3baf537de0d18f08c0",
            "db73312f5e2f4e28b7ff9041f6e19e5e",
            "091881f7729d4770aa0ecaf74bcd08aa",
            "2913fa9c79b3453d966660f958903294",
            "d8b3ebf679354b0886bd0ea8b74a5086",
            "5dfc735817b94dfe807788e9c466daab",
            "07ef033329eb45d9afbc9fb1c5496974",
            "e87442a65fd24e7e9d30c8227813ff5c",
            "8bfd42c91be64af5973e586e89e7c502",
            "2b3046813686432982d55e62f45dde5d",
            "148135ab3d434ab6b79228fe825302cd",
            "026d8e2290b24668b374324ac9d14399",
            "acc61491f2c84ed6a58687c0af9f9c0b",
            "d2b46af9a0184d348571d0d03841416b",
            "d95c8817342745ac8e1b80c7ba5f53c1"
          ]
        },
        "id": "UacTRhG2WY2Q",
        "outputId": "c8d342a8-4c1f-472c-b4e6-b93089490f63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0825ce46390470aa40e3641c01bc0e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c08cbaf9ea3448ee9d4812aa39beb141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eeb4589234a4ec4a72e4f93fc89cbd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85f48927db504dd99970d6f47201d4f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8b3ebf679354b0886bd0ea8b74a5086",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "crossencoder = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeKBH3lFWaDJ",
        "outputId": "5d40a848-929a-4c06-96ca-dfdd6f2f9f4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-4.842785 , -5.8221684, -5.3490076, -3.1788046], dtype=float32)"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crossencoder.score(context_query_pairs_for_scoring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi4J1QKZWbwg"
      },
      "outputs": [],
      "source": [
        "reranker = CrossEncoderReranker(model=crossencoder, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA-pD63lWdbc"
      },
      "outputs": [],
      "source": [
        "reranker_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=structured_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1cX0XiYWe3t"
      },
      "outputs": [],
      "source": [
        "reranked_docs = reranker_retriever.invoke(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLWZhkhgWhiB",
        "outputId": "5acb69fd-0f5e-41f6-81dd-0977f9da13a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document at rank: 0 after reranking differs from the original order\n",
            "Document at rank: 1 after reranking differs from the original order\n",
            "Document at rank: 2 after reranking differs from the original order\n",
            "Document at rank: 3 after reranking differs from the original order\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(reranked_docs)):\n",
        "    if reranked_docs[i].page_content == context_list[i]:\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"Document at rank: {i} after reranking differs from the original order\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srlCv6AmTHsZ",
        "outputId": "0e62c9fd-27d8-4ddd-d72b-900759933900"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 1, 'source': 'papers/evolution of hippo in relation to neocortex.pdf'}, page_content='Though seemingly disparate cognitive processes, \\nnavigation and episodic memory are both thought to rely \\non similar mental representations [Buzsáki and Moser, \\n2013; Ekstrom and Ranganath, 2018; Rolls and Wirth, \\n2018] and a core neural substrate located in the hippo-\\ncampal formation (HF) [Burgess et al., 2002; Buzsáki, \\n2005; Horner et al., 2016].'),\n",
              " Document(metadata={'page': 10, 'source': 'papers/The role of REM sleep theta in emotional memory.pdf'}, page_content='545, 307–311. doi: 10.1016/0006-\\n8993(91)91303-I\\nDolcos, F., LaBar, K. S., and Cabeza, R. (2004). Interaction between the amygdala\\nand the medial temporal lobe memory system predicts better memory\\nfor emotional events. Neuron 42, 855–863. doi: 10.1016/S0896-6273(04)\\n00289-2\\nDragoi, G., and Buzsáki, G. (2006). Temporal encoding of place\\nsequences by hippocampal cell assemblies. Neuron 50, 145–157. doi:\\n10.1016/j.neuron.2006.02.023\\nDragoi, G., Carpi, D., Recce, M., Csicsvari, J., and Buzsaki, G. (1999). Interactions\\nbetween hippocampus and medial septum during sharp waves and theta\\noscillation in the behaving rat. J. Neurosci. 19, 6191–6199. Dragoi, G., and Tonegawa, S. (2011). Preplay of future place cell sequences\\nby hippocampal cellular assemblies. Nature 469, 397–401. doi:\\n10.1038/nature09633\\nEgo-Stengel, V., and Wilson, M. A. (2010). Disruption of ripple-associated\\nhippocampal activity during rest impairs spatial learning in the rat. Hippocampus 20, 1–10. doi: 10.1002/hipo.20707\\nEkstrom,A.D.,Caplan,J.B.,Ho,E.,Shattuck,K.,Fried,I.,andKahana,M.J.(2005). Human hippocampal theta activity during virtual navigation. Hippocampus 15,\\n881–889. doi: 10.1002/hipo.20109\\nEkstrom, A. D., Kahana, M. J., Caplan, J. B., Fields, T. A., Isham, E. A., Newman, E. L., et al. (2003). Cellular networks underlying human spatial navigation. Nature\\n425, 184–188. doi: 10.1038/nature01964Fishbein, W. (1971). Disruptive effects of rapid eye movement sleep deprivation\\non long-term memory. Physiol. Behav. 6, 279–282. doi: 10.1016/0031-\\n9384(71)90155-7\\nFogel, S., and Smith, C. (2006). Learning-dependent changes in sleep spindles and\\nStage 2 sleep. J. Sleep Res. 15, 250–255. doi: 10.1111/j.1365-2869.2006.00522.x\\nFogel, S. M., Smith, C. T., and Beninger, R. J. (2009). Evidence for 2-stage models\\nof sleep and memory: learning-dependent changes in spindles and theta in rats.'),\n",
              " Document(metadata={'page': 13, 'source': 'papers/Boosting slow oscillation during sleep to improve memory in old people.pdf'}, page_content='Neurobiol. Dis. 2020 , 104865. [CrossRef] [PubMed]\\n4. Rasch, B.; Born, J. About sleep’s role in memory. Physiol. Rev. 2013 ,93, 681–766. [CrossRef]\\n5. Peigneux, P .; Fogel, S.; Smith, C. Memory processing in relation to sleep. In Principles and Practice of Sleep\\nMedicine , 6th ed.; Kryger, M., Roth, T., Dement, B., Eds.; Elsevier: Philadelphia, PA, USA, 2017; pp.'),\n",
              " Document(metadata={'page': 13, 'source': 'papers/The role of REM sleep theta in emotional memory.pdf'}, page_content='Learn. Mem. 6,500–508. doi: 10.1101/lm.6.5.500\\nRichardson, M. P., Strange, B. A., and Dolan, R. J. (2004). Encoding of emotional\\nmemories depends on amygdala and hippocampus and their interactions. Nat. Neurosci. 7, 278–285. doi: 10.1038/nn1190\\nRosa, R., Bonnet, M., and Kramer, M. (1983). The relationship of sleep and\\nanxiety in anxious subjects. Biol. Psychol. 16, 119–126. doi: 10.1016/0301-\\n0511(83)90058-3\\nRuch, S., Markes, O., Duss, S. B., Oppliger, D., Reber, T. P., Koenig, T., et al. (2012). Sleep stage II contributes to the consolidation of declarative memories. Neuropsychologia 50, 2389–2396. doi: 10.1016/j.neuropsychologia.2012.06.008\\nRutishauser, U., Ross, I. B., Mamelak, A. N., and Schuman, E. M. (2010). Human\\nmemory strength is predicted by theta-frequency phase-locking of single\\nneurons. Nature 464, 903–907. doi: 10.1038/nature08860\\nSagales, T., and Domino, E. F. (1973). Effects of stress and REM sleep deprivation\\non the patterns of avoidance learning and brain acetylcholine in the mouse. Psychopharmacologia 29, 307–315. doi: 10.1007/BF00429278\\nScheffzük, C., Kukushka, V. I., Vyssotski, A. L., Draguhn, A., Tort, A. B. L., and\\nBrankačk, J. (2011). Selective coupling between theta phase and neocortical\\nfast gamma oscillations during REM-sleep in mice. PLoS ONE 6:e28489. doi:\\n10.1371/journal.pone.0028489\\nSeager, M. A., Johnson, L. D., Chabot, E. S., Asaka, Y., and Berry, S.')]"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reranked_docs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "026d8e2290b24668b374324ac9d14399": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02cc0b1feaf04ecabac433657dd1e095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0654bac16c6f497f80f195a3b0603690": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07ef033329eb45d9afbc9fb1c5496974": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_026d8e2290b24668b374324ac9d14399",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acc61491f2c84ed6a58687c0af9f9c0b",
            "value": 112
          }
        },
        "091881f7729d4770aa0ecaf74bcd08aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8bc0a398514bf4b9ab6d102ae40690": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148135ab3d434ab6b79228fe825302cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ebd9438ec8341b8b6e19ef459073296": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2230d72debb04101bbd72a99b590c38f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2913fa9c79b3453d966660f958903294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3046813686432982d55e62f45dde5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f17b135685d4da0bb7fcc52e8edf8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a382ec6cc9d4a38b0b5e8b888b0086d",
            "max": 794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d15d1c7354544b2ac19b64e242ddd30",
            "value": 794
          }
        },
        "33c1d58f12094af6933e04cf31e461ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358dca717dcd4d5fb65aa2de36bba7e7",
            "placeholder": "​",
            "style": "IPY_MODEL_ade5a9db1ff94f44bd135d680a4fad75",
            "value": " 316/316 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "358dca717dcd4d5fb65aa2de36bba7e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7094549bad4ff5b4362aa85a1dccc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_846a45eea8954baf94377284f65f2970",
            "placeholder": "​",
            "style": "IPY_MODEL_acf95dcfc58240409d5fe37e5da252a4",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "43743f7cc7344e50b97545dea30575d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1a41b94b51400586bb44edda71a8e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dfc735817b94dfe807788e9c466daab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3046813686432982d55e62f45dde5d",
            "placeholder": "​",
            "style": "IPY_MODEL_148135ab3d434ab6b79228fe825302cd",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5f74e154a25a447cbe889a92a3f3b13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a382ec6cc9d4a38b0b5e8b888b0086d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cc98c97bcee495093006e9f31195474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02cc0b1feaf04ecabac433657dd1e095",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f74e154a25a447cbe889a92a3f3b13c",
            "value": 316
          }
        },
        "6e1b98bfdae84452a2a8825deaf34957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1a41b94b51400586bb44edda71a8e4",
            "placeholder": "​",
            "style": "IPY_MODEL_7e268ec7be934948ad3b2bbc4be5ead5",
            "value": "config.json: 100%"
          }
        },
        "7782b836718f4aef81d576ce00b2b975": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e268ec7be934948ad3b2bbc4be5ead5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8388130c6dcf4c458c42ca8be687d9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "846a45eea8954baf94377284f65f2970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f48927db504dd99970d6f47201d4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d992033fe62f4334ba1dc6764fe4dc85",
              "IPY_MODEL_c6e3976bbd3d413191203a2edb1566f3",
              "IPY_MODEL_a547c8bee63c4754b5fd5e9f9db4d8a0"
            ],
            "layout": "IPY_MODEL_c14e4c7d21a54b2fad9d91f1b5275cd3"
          }
        },
        "88c2cf799a8a4b5880b3d6255f728e72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bfd42c91be64af5973e586e89e7c502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d15d1c7354544b2ac19b64e242ddd30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8eeb4589234a4ec4a72e4f93fc89cbd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b7094549bad4ff5b4362aa85a1dccc9",
              "IPY_MODEL_6cc98c97bcee495093006e9f31195474",
              "IPY_MODEL_33c1d58f12094af6933e04cf31e461ea"
            ],
            "layout": "IPY_MODEL_2230d72debb04101bbd72a99b590c38f"
          }
        },
        "90db47783a054bc6ae74d0ba30f88b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7782b836718f4aef81d576ce00b2b975",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c6981ca7ce4da9a50f787f8c0d6de8",
            "value": " 794/794 [00:00&lt;00:00, 43.1kB/s]"
          }
        },
        "9191959442ca4a39b66fba5f73db50bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88c2cf799a8a4b5880b3d6255f728e72",
            "placeholder": "​",
            "style": "IPY_MODEL_8388130c6dcf4c458c42ca8be687d9da",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "92974ea03fae4f32a98807a5a591717c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ebd9438ec8341b8b6e19ef459073296",
            "max": 90903017,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa7e019d9467488c9536068842609070",
            "value": 90903017
          }
        },
        "a547c8bee63c4754b5fd5e9f9db4d8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091881f7729d4770aa0ecaf74bcd08aa",
            "placeholder": "​",
            "style": "IPY_MODEL_2913fa9c79b3453d966660f958903294",
            "value": " 232k/232k [00:00&lt;00:00, 4.12MB/s]"
          }
        },
        "a6c6981ca7ce4da9a50f787f8c0d6de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa7e019d9467488c9536068842609070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acc61491f2c84ed6a58687c0af9f9c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acf95dcfc58240409d5fe37e5da252a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ade5a9db1ff94f44bd135d680a4fad75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0825ce46390470aa40e3641c01bc0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e1b98bfdae84452a2a8825deaf34957",
              "IPY_MODEL_2f17b135685d4da0bb7fcc52e8edf8ad",
              "IPY_MODEL_90db47783a054bc6ae74d0ba30f88b4d"
            ],
            "layout": "IPY_MODEL_0e8bc0a398514bf4b9ab6d102ae40690"
          }
        },
        "c08cbaf9ea3448ee9d4812aa39beb141": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9191959442ca4a39b66fba5f73db50bd",
              "IPY_MODEL_92974ea03fae4f32a98807a5a591717c",
              "IPY_MODEL_e39e95ae8f4e45578caf3ff82e703822"
            ],
            "layout": "IPY_MODEL_43743f7cc7344e50b97545dea30575d8"
          }
        },
        "c14e4c7d21a54b2fad9d91f1b5275cd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e3976bbd3d413191203a2edb1566f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f34cb487f03a4c3baf537de0d18f08c0",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db73312f5e2f4e28b7ff9041f6e19e5e",
            "value": 231508
          }
        },
        "ca9f93d51fcf4d1597828a6dc5815e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc5509d92af445dd9fd48fd23c9ce839": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2b46af9a0184d348571d0d03841416b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b3ebf679354b0886bd0ea8b74a5086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dfc735817b94dfe807788e9c466daab",
              "IPY_MODEL_07ef033329eb45d9afbc9fb1c5496974",
              "IPY_MODEL_e87442a65fd24e7e9d30c8227813ff5c"
            ],
            "layout": "IPY_MODEL_8bfd42c91be64af5973e586e89e7c502"
          }
        },
        "d95c8817342745ac8e1b80c7ba5f53c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d992033fe62f4334ba1dc6764fe4dc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca9f93d51fcf4d1597828a6dc5815e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_0654bac16c6f497f80f195a3b0603690",
            "value": "vocab.txt: 100%"
          }
        },
        "db73312f5e2f4e28b7ff9041f6e19e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e39e95ae8f4e45578caf3ff82e703822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb80bd56c96649b496215bc0ca3e8ac5",
            "placeholder": "​",
            "style": "IPY_MODEL_cc5509d92af445dd9fd48fd23c9ce839",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 128MB/s]"
          }
        },
        "e87442a65fd24e7e9d30c8227813ff5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2b46af9a0184d348571d0d03841416b",
            "placeholder": "​",
            "style": "IPY_MODEL_d95c8817342745ac8e1b80c7ba5f53c1",
            "value": " 112/112 [00:00&lt;00:00, 4.14kB/s]"
          }
        },
        "f34cb487f03a4c3baf537de0d18f08c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb80bd56c96649b496215bc0ca3e8ac5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
