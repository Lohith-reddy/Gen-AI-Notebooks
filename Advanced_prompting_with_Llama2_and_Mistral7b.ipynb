{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJLtsQobpd0R"
      },
      "source": [
        "<center>\n",
        "<h1><b>Advanced Prompt Engineering with Llama2 and Mistral 7b</b>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNCeIToJPahi"
      },
      "source": [
        "# Objectives\n",
        "\n",
        "- Present two new techniques for prompt engineering: self-consistency, and tree-of-thought using Llama2 13b and Mistral 7b.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bv8-ZB_uH8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE6uVb5EAjG1"
      },
      "source": [
        "Note: This notebook needs to be executed with a GPU runtime since we load and use LLaMA2 & Mistral 7b for inference. Both these models benefit from parallel execution offered by a GPU runtime (code in this notebook runs up to 4 times faster). See screenshots below to create a GPU runtime on a Colab instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IzhFH5hBVlI"
      },
      "source": [
        "*Step 1:*\n",
        "\n",
        "Select the `Runtime` option from the main menu and select the `Change runtime type` from the dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikT4l7AvBkMn"
      },
      "source": [
        "Select `T4 GPU` from the options presented under Hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkxoqr0Wf0-"
      },
      "source": [
        "You should now see the T4 GPU mentioned in the runtime logo (screenshot below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyfJIA4Uzy5"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2F-qpOK3jDy",
        "outputId": "5610e9db-0b2c-452d-b953-8462dadcd03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python==0.2.28\n",
            "  Downloading llama_cpp_python-0.2.28.tar.gz (9.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/9.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/9.4 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.28)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.28)\n",
            "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m184.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.28)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m185.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m221.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.28-cp310-cp310-linux_x86_64.whl size=8778602 sha256=482fd63fcf26733d9601380b377d8d4d416922828186434681106e7d6332d8b8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lskzzcjn/wheels/93/6e/a9/478cce089dc2a082bdcffe468a1c65465c91b25d911b30da82\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.28\n",
            "    Uninstalling llama_cpp_python-0.2.28:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.28\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.28 numpy-2.1.3 typing-extensions-4.12.2\n"
          ]
        }
      ],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --upgrade --no-cache-dir --verbose -q 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZYAmbRJUEbM"
      },
      "outputs": [],
      "source": [
        "# For downloading the models from HF Hub\n",
        "!pip install huggingface_hub==0.23.2 -q 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFZj9kr8Pahl"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3gwxSqQPahl"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKcL7M9r1Uo3"
      },
      "source": [
        "# Llama2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5K3mCDr407D"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKcktmfa45ke"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\" # the model is in gguf format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "e07b31013f86488a8c91f3bf0294577a",
            "9dc5eccec23d42d6ae165aaacec20de9",
            "d305f57af6d74a29ab4684fbb3bf4165",
            "39612669f7ef418787a69793adf4e5c2",
            "47d783445fa44f9d8991cbcbe56e51b1",
            "b52923855c9942768d84dc1f9e2337d0",
            "6acb11b723f74caf91f0ebe424216f93",
            "4dc5d0f9c2094089818e09806b912243",
            "f78067731ed84f84840a3551cf0673a2",
            "96cd4a2eb1ba4848b2a5971f19ebebc2",
            "60b57cd21e654530b2655f96b0e2d0dd"
          ]
        },
        "id": "k7HQra5c46V-",
        "outputId": "1d32462d-19a6-487d-97ae-0ff8d5d64866"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e07b31013f86488a8c91f3bf0294577a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd_9HkIl4_9q",
        "outputId": "6383f6d0-1409-4012-dfdf-9f7dcd1fa5c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096 # Context window\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC4111SC1AVv"
      },
      "outputs": [],
      "source": [
        "llama2_prompt_template = \"\"\"<s>[INST]<<SYS>>\n",
        "{system_message}\n",
        "<</SYS>>\n",
        "\n",
        "{user_message}[/INST]\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQph6H1PBqp0"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "\n",
        "In this section, we will explore the process of scaling a baking recipe to accommodate different serving sizes. Baking is both an art and a science, where precise measurements are crucial for achieving the desired outcome. When a recipe is designed for a specific number of servings, adjusting the ingredient quantities for larger or smaller batches can be challenging.\n",
        "\n",
        "Through iterative prompting techniques, we will investigate how to effectively modify ingredient amounts while ensuring consistency and accuracy in the final product. We will start with a basic recipe and progressively apply different prompting methods to refine our adjustments. By comparing outputs from various approaches, we aim to identify the most effective strategies for scaling recipes, thereby enhancing our understanding of mathematical reasoning in practical cooking scenarios. This exploration will not only highlight the importance of clear calculations but also illustrate how different methodologies can lead to improved recipe outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8XCqcZFNQgw"
      },
      "source": [
        "## Simple Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtMesH9dmEl",
        "outputId": "c2bfffa7-ca64-4453-d838-3caf8100f5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple Answer (Adjusted Recipe):\n",
            "- 2 eggs\n",
            "- 1 teaspoon of vanilla extract\n",
            "- 1/2 teaspoon of salt\n",
            "- 1 cup of milk\n",
            "- 1/4 cup of heavy cream\n",
            "- 1/2 cup of chocolate chips (semi sweet)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Preheat oven to 375 degrees Fahrenheit and grease a 9x13 inch baking dish with butter or cooking spray.\n",
            "2. In a medium-sized mixing bowl, whisk together flour, sugar, salt, and baking powder until well combined.\n",
            "3. Add the softened butter to the dry ingredients and mix until the mixture resembles coarse crumbs.\n",
            "4. Press half of the dough into the prepared baking dish.\n",
            "5. In a separate bowl, whisk together eggs, vanilla extract, milk, heavy cream, and chocolate chips until well combined.\n",
            "6. Pour the wet mixture over the dry ingredients in the baking dish.\n",
            "7. Sprinkle the remaining dough on top of the wet mixture.\n",
            "8. Bake for 25-30 minutes or until a toothpick inserted into the center comes out clean.\n",
            "9. Allow the brownies to cool completely before cutting and serving.\n",
            "\n",
            "Adjusted Recipe (Serves 10):\n",
            "\n",
            "* Increase all ingredients by 2.5 times:\n",
            "\t+ 5 cups of flour\n",
            "\t+ 2.5 cups of sugar\n",
            "\t+ 1.25 cups of butter\n",
            "\t+ 4 eggs\n",
            "\t+ 2 teaspoons of vanilla extract\n",
            "\t+ 1.25 teaspoons of salt\n",
            "\t+ 2.5 cups of milk\n",
            "\t+ 0.625 cups of heavy cream\n",
            "\t+ 1.25 cups of chocolate chips (semi sweet)\n",
            "* Follow the same instructions as the original recipe, but adjust the baking time accordingly.\n"
          ]
        }
      ],
      "source": [
        "# Simple prompting to adjust recipe ingredients\n",
        "simple_prompt = \"\"\"\n",
        "Adjust the following recipe to serve 10 people.\n",
        "Original Recipe (Serves 4):\n",
        "- 2 cups of flour\n",
        "- 1 cup of sugar\n",
        "- 0.5 cup of butter\n",
        "\"\"\"\n",
        "\n",
        "response = lcpp_llm(\n",
        "    prompt=simple_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "simple_answer = response[\"choices\"][0][\"text\"]\n",
        "print(\"Simple Answer (Adjusted Recipe):\")\n",
        "print(simple_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ_gYCA78bMr"
      },
      "source": [
        "### Analyzing Simple Prompting Output\n",
        "\n",
        "The output generated through simple prompting presents a recipe that is clear and straightforward, making it easy for beginners to follow. However, it falls short in several critical areas. Notably, the lack of detailed calculations for adjusting ingredient quantities based on the desired increase in servings may leave learners confused about the scaling process. Additionally, there is no explanation provided on how to adjust these quantities, limiting their understanding of the underlying mathematical principles involved in recipe scaling. Furthermore, the inclusion of unnecessary ingredients and instructional details not relevant to the task contributes to the phenomenon of \"hallucination\" in the output, which can further mislead learners and detract from the overall quality of the recipe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtK5EoQfZ5nE"
      },
      "source": [
        "## Self-consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJqjso69Z7-p"
      },
      "source": [
        "In self-consistency, we generate multiple answers to the same question and pick the answer that is repeated the most across these occurrences. This is particularly valuable for factual questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wck8jVQaeRWw",
        "outputId": "f7c1ff19-0b4b-4bc3-f525-de7f0b3b531c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Answers:\n",
            "  Sure, I'd be happy to help! Here are five different adjustments to the original recipe to serve 10 people:\n",
            "\n",
            "Adjustment 1 (Classic Portion):\n",
            "\n",
            "* Flour: 4 cups (double the amount for 10 servings)\n",
            "* Sugar: 2 cups (double the amount for 10 servings)\n",
            "* Butter: 1 cup (keep the same amount, but adjusted for 10 servings)\n",
            "\n",
            "Adjustment 2 (Large Portion):\n",
            "\n",
            "* Flour: 5 cups (increase by 1/3 for 10 servings)\n",
            "* Sugar: 2.5 cups (increase by 1/3 for 10 servings)\n",
            "* Butter: 1.25 cups (increase by 1/4 for 10 servings)\n",
            "\n",
            "Adjustment 3 (Small Portion):\n",
            "\n",
            "* Flour: 3 cups (decrease by 1/3 for 10 servings)\n",
            "* Sugar: 1.5 cups (decrease by 1/3 for 10 servings)\n",
            "* Butter: 0.8 cups (decrease by 1/4 for 10 servings)\n",
            "\n",
            "Adjustment 4 (Low-Fat Version):\n",
            "\n",
            "* Flour: 3.5 cups (reduce flour by 1/2 cup for a lower carb count)\n",
            "* Sugar: 1.75 cups (reduce sugar by 0.25 cups for a lower calorie count)\n",
            "* Butter: 0.6 cups (reduce butter by 0.4 cups for a lower fat count)\n",
            "\n",
            "Adjustment 5 (Vegan Version):\n",
            "\n",
            "* Flour: 3.75 cups (replace butter with an equal amount of vegan butter substitute)\n",
            "* Sugar: 1.875 cups (reduce sugar by 0.25 cups for a lower calorie count)\n",
            "\n",
            "Note that these adjustments are just suggestions and can be modified based on personal preference or dietary needs. Additionally, the cooking time may need to be adjusted depending on the size of the cookies.\n"
          ]
        }
      ],
      "source": [
        "# System message to provide context for the LLM\n",
        "system_message = \"\"\"\n",
        "You are a cooking assistant. Adjust recipe ingredient quantities based on desired servings.\n",
        "\"\"\"\n",
        "\n",
        "# Template for generating multiple answers\n",
        "answers_template = \"\"\"\n",
        "The original recipe serves 4 people:\n",
        "- 2 cups of flour\n",
        "- 1 cup of sugar\n",
        "- 0.5 cup of butter\n",
        "\n",
        "Adjust the following recipe to serve 10 people. Provide {num_answers} different adjustments.\n",
        "\"\"\"\n",
        "\n",
        "# Formatting the prompt to generate multiple answers (Self-Consistency)\n",
        "answers_prompt = llama2_prompt_template.format(\n",
        "    system_message=system_message,\n",
        "    user_message=answers_template.format(\n",
        "        num_answers=5  # Generate 5 distinct answers\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending the request to generate multiple answers\n",
        "response = lcpp_llm(\n",
        "    prompt=answers_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "# Extracting the generated answers\n",
        "factual_answers = response[\"choices\"][0][\"text\"]\n",
        "print(\"Generated Answers:\")\n",
        "print(factual_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYLwLHDreS2p",
        "outputId": "be704032-1b64-43ff-ef90-c399c8db015e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Answer (Self-Consistency):\n",
            "  Sure! Based on the five adjustments provided, here are the most frequently suggested quantities for each ingredient for a recipe serving 10 people:\n",
            "\n",
            "Adjustment 1 (Classic Portion):\n",
            "\n",
            "* Flour: 4 cups\n",
            "* Sugar: 2 cups\n",
            "* Butter: 1 cup\n",
            "\n",
            "These are the original amounts from the recipe, so there is no need to adjust them for a classic portion.\n"
          ]
        }
      ],
      "source": [
        "# Template for selecting the most frequent answer\n",
        "consistency_template = \"\"\"\n",
        "Here are {num_answers} answers to the recipe adjustment:\n",
        "Answers:\n",
        "{answers}\n",
        "\n",
        "Choose the most frequently suggested quantities for each ingredient.\n",
        "Final Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Formatting the prompt to select the most frequent answer\n",
        "consistency_prompt = llama2_prompt_template.format(\n",
        "    system_message=system_message,\n",
        "    user_message=consistency_template.format(\n",
        "        num_answers=5,  # Number of answers generated previously\n",
        "        answers=factual_answers  # Pass the generated answers from the previous step\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending the request to select the most frequent answer\n",
        "response = lcpp_llm(\n",
        "    prompt=consistency_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "# Extracting and printing the final answer\n",
        "final_answer = response[\"choices\"][0][\"text\"]\n",
        "print(\"Final Answer (Self-Consistency):\")\n",
        "print(final_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSoWgHJq9TvP"
      },
      "source": [
        "### Analyzing Self-consistency Prompting Output\n",
        "\n",
        "The self-consistency prompting approach yields an output that is more tailored to the specific needs of the recipe, addressing some of the issues present in the simple prompting method. This output maintains a degree of consistency in ingredient adjustments and offers multiple perspectives by showcasing different methods for recipe scaling, catering to various preferences. However, it still contains inconsistencies in ingredient amounts for the same serving size, leading to potential confusion regarding measurements like sugar and flour. Additionally, the output lacks clear explanations for why certain quantities were selected over others, leaving users uncertain about which method to trust. This ambiguity could undermine the confidence of learners who are trying to grasp the intricacies of recipe adjustment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihVZtA6cl2ak"
      },
      "source": [
        "## Tree-of-Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-3pbuuymH_a"
      },
      "source": [
        "Tree-of-thought prompting is a generalization of chain-of-thought prompting where the model is prompted to take multiple reasoning paths. This forces the LLM into a deliberate reasoning mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv_KRZb_ewml",
        "outputId": "2c5368c6-a4f3-4d2a-c284-2f0406e79bbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Solutions (Tree of Thought):\n",
            "  Sure, I'd be happy to help! Here are five possible solutions for calculating the ingredient adjustments for the recipe:\n",
            "\n",
            "Solution 1: Adjustment Factor Method\n",
            "\n",
            "* Calculate the adjustment factor: 10 servings / 4 servings = 2.5\n",
            "* Adjust each ingredient by multiplying by the adjustment factor:\n",
            "\t+ Flour: 2 cups x 2.5 = 5 cups\n",
            "\t+ Sugar: 1 cup x 2.5 = 2.5 cups\n",
            "\t+ Butter: 0.5 cup x 2.5 = 1.25 cups\n",
            "\n",
            "Solution 2: Proportional Method\n",
            "\n",
            "* Divide each ingredient by the number of servings (10) and multiply by the desired number of servings (4):\n",
            "\t+ Flour: 2 cups / 10 = 0.2 cups/serving x 4 = 0.8 cups\n",
            "\t+ Sugar: 1 cup / 10 = 0.1 cups/serving x 4 = 0.4 cups\n",
            "\t+ Butter: 0.5 cup / 10 = 0.05 cups/serving x 4 = 0.2 cups\n",
            "\n",
            "Solution 3: Scale Method\n",
            "\n",
            "* Multiply each ingredient by the desired number of servings (4) and divide by the original number of servings (4):\n",
            "\t+ Flour: 2 cups x 4 / 4 = 2 cups x 1 = 2 cups\n",
            "\t+ Sugar: 1 cup x 4 / 4 = 1 cup x 1 = 1 cup\n",
            "\t+ Butter: 0.5 cup x 4 / 4 = 0.5 cup x 1 = 0.5 cups\n",
            "\n",
            "Solution 4: Increase Percentage Method\n",
            "\n",
            "* Increase each ingredient by a percentage of the original amount based on the desired number of servings:\n",
            "\t+ Flour: 2 cups x (10/4)% = 2 cups x 25% = 0.5 cups (rounded to 0.5 cups)\n",
            "\t+ Sugar: 1 cup x (10/4)% = 1 cup x 25% = 0.25 cups (rounded to 0.25 cups)\n",
            "\t+ Butter: 0.5 cup x (10/4)% = 0.5 cup x 25% = 0.125 cups (rounded to 0.125 cups)\n",
            "\n",
            "Solution 5: Decrease Percentage Method\n",
            "\n",
            "* Decrease each ingredient by a percentage of the original amount based on the desired number of servings:\n",
            "\t+ Flour: 2 cups x (4/10)% = 2 cups x 20% = 0.4 cups (rounded to 0.4 cups)\n",
            "\t+ Sugar: 1 cup x (4/10)% = 1 cup x 20% = 0.2 cups (rounded to 0.2 cups)\n",
            "\t+ Butter: 0.5 cup x (4/10)% = 0.5 cup x 20% = 0.1 cups (rounded to 0.1 cups)\n",
            "\n",
            "In all five solutions, the total quantity of ingredients is adjusted based on the desired number of servings. The specific method used to adjust the quantities may vary depending on personal preference and the desired level of precision.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Tree of Thought Prompting to generate multiple solutions\n",
        "tot_prompt_template = \"\"\"\n",
        "Generate {num_solutions} possible solutions for the following problem:\n",
        "Problem:\n",
        "Calculate the ingredient adjustments for the following recipe:\n",
        "Original Recipe (Serves 4):\n",
        "- 2 cups of flour\n",
        "- 1 cup of sugar\n",
        "- 0.5 cup of butter\n",
        "Desired Servings: 10 people.\n",
        "\n",
        "1. Calculate the adjustment factor mathematically: (10 servings / 4 servings).\n",
        "2. Adjust each ingredient using the adjustment factor.\n",
        "3. Present the adjusted quantities in bullet points.\n",
        "\"\"\"\n",
        "\n",
        "tot_prompt = llama2_prompt_template.format(\n",
        "    system_message=system_message,\n",
        "    user_message=tot_prompt_template.format(\n",
        "        num_solutions=5  # Generate 5 distinct solutions\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending the request to the LLM for generating solutions\n",
        "response = lcpp_llm(\n",
        "    prompt=tot_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "# Extracting and printing generated solutions\n",
        "tot_solutions = response[\"choices\"][0][\"text\"]\n",
        "print(\"Generated Solutions (Tree of Thought):\")\n",
        "print(tot_solutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCnqxDyeezv_",
        "outputId": "31603062-598d-49c6-d889-c60d584721e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation of Solutions:\n",
            "  As a cooking assistant, I would evaluate each solution based on accuracy, mathematical reasoning, and practicality in the kitchen. Here's my evaluation of each solution:\n",
            "\n",
            "Solution 1: Adjustment Factor Method\n",
            "Strengths:\n",
            "\n",
            "* Simple to implement and easy to understand\n",
            "* Provides a consistent adjustment factor for all ingredients\n",
            "\n",
            "Weaknesses:\n",
            "\n",
            "* Does not take into account the specific volume or weight of each ingredient\n",
            "* May result in over- or under-adjusting certain ingredients, leading to inconsistent flavors or textures\n",
            "\n",
            "Solution 2: Proportional Method\n",
            "Strengths:\n",
            "\n",
            "* Takes into account the specific volume or weight of each ingredient\n",
            "* Provides a more precise adjustment for each ingredient\n",
            "\n",
            "Weaknesses:\n",
            "\n",
            "* Requires more mathematical calculations and can be more time-consuming\n",
            "* May not be as intuitive or easy to implement as other methods\n",
            "\n",
            "Solution 3: Scale Method\n",
            "Strengths:\n",
            "\n",
            "* Provides a clear and visual representation of the adjustment process\n",
            "* Easy to understand and implement\n",
            "\n",
            "Weaknesses:\n",
            "\n",
            "* Does not take into account the specific volume or weight of each ingredient\n",
            "* May result in over- or under-adjusting certain ingredients, leading to inconsistent flavors or textures\n",
            "\n",
            "Solution 4: Increase Percentage Method\n",
            "Strengths:\n",
            "\n",
            "* Provides a consistent percentage increase for all ingredients\n",
            "* Easy to understand and implement\n",
            "\n",
            "Weaknesses:\n",
            "\n",
            "* May not be as precise as other methods, especially if the desired percentage increase is small\n",
            "* Can result in over-adjusting certain ingredients, leading to inconsistent flavors or textures\n",
            "\n",
            "Solution 5: Decrease Percentage Method\n",
            "Strengths:\n",
            "\n",
            "* Provides a consistent percentage decrease for all ingredients\n",
            "* Easy to understand and implement\n",
            "\n",
            "Weaknesses:\n",
            "\n",
            "* May not be as precise as other methods, especially if the desired percentage decrease is small\n",
            "* Can result in under-adjusting certain ingredients, leading to inconsistent flavors or textures\n",
            "\n",
            "In conclusion, each solution has its strengths and weaknesses, and the best method will depend on the specific needs and preferences of the cook. However, based on accuracy and mathematical reasoning, the Proportional Method (Solution 2) is the most precise and reliable method for adjusting ingredient quantities based on desired servings. The Adjustment Factor Method (Solution 1) and the Scale Method (Solution 3) are also easy to understand and implement, but may result in less precise adjustments. The Increase Percentage Method (Solution 4) and the Decrease Percentage Method (Solution 5) are simple to implement, but may not be as precise as other methods.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Evaluation prompt template\n",
        "evaluation_template = \"\"\"\n",
        "For the following problem: Adjust the recipe to serve 10 people, evaluate each solution based on accuracy and mathematical reasoning:\n",
        "Solutions:\n",
        "{solutions}\n",
        "\n",
        "Present your evaluation of each solution, highlighting strengths and weaknesses.\n",
        "\"\"\"\n",
        "\n",
        "# Create a new prompt to evaluate the solutions\n",
        "evaluation_prompt = llama2_prompt_template.format(\n",
        "    system_message=system_message,\n",
        "    user_message=evaluation_template.format(\n",
        "        solutions=tot_solutions\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending the request to evaluate the solutions\n",
        "response = lcpp_llm(\n",
        "    prompt=evaluation_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "# Extract and print evaluation results\n",
        "evaluation_results = response[\"choices\"][0][\"text\"]\n",
        "print(\"Evaluation of Solutions:\")\n",
        "print(evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td_eD6BJe24n",
        "outputId": "7b2ac784-2d3e-4f0b-e885-9659e136dfce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranked Solutions:\n",
            "  Based on accuracy and mathematical reasoning, the best solution for adjusting ingredient quantities based on desired servings is the Proportional Method (Solution 2). This method takes into account the specific volume or weight of each ingredient, providing a more precise adjustment for each ingredient. While it may be more time-consuming and require more mathematical calculations, the precision it offers makes it the most reliable method.\n",
            "\n",
            "The Adjustment Factor Method (Solution 1) is a simple and easy-to-understand method, but it does not take into account the specific volume or weight of each ingredient, which can result in over- or under-adjusting certain ingredients. The Scale Method (Solution 3) provides a clear visual representation of the adjustment process, but it may also result in less precise adjustments.\n",
            "\n",
            "The Increase Percentage Method (Solution 4) and the Decrease Percentage Method (Solution 5) are easy to understand and implement, but they may not be as precise as other methods, especially if the desired percentage increase or decrease is small. Additionally, these methods can result in over- or under-adjusting certain ingredients, leading to inconsistent flavors or textures.\n",
            "\n",
            "Therefore, based on accuracy and mathematical reasoning, the Proportional Method (Solution 2) is the best solution for adjusting ingredient quantities based on desired servings.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Ranking prompt template\n",
        "ranking_template = \"\"\"\n",
        "For the following problem: Adjust the recipe to serve 10 people, rank the following solutions based on their accuracy and mathematical reasoning:\n",
        "Evaluations:\n",
        "{evaluations}\n",
        "\n",
        "Rank the solutions and explain which one is the best and why.\n",
        "\"\"\"\n",
        "\n",
        "# Create a new prompt to rank the solutions\n",
        "ranking_prompt = llama2_prompt_template.format(\n",
        "    system_message=system_message,\n",
        "    user_message=ranking_template.format(\n",
        "        evaluations=evaluation_results\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending the request to rank the solutions\n",
        "response = lcpp_llm(\n",
        "    prompt=ranking_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    echo=False  # Do not return the prompt\n",
        ")\n",
        "\n",
        "# Extract and print the ranked solutions\n",
        "ranked_solutions = response[\"choices\"][0][\"text\"]\n",
        "print(\"Ranked Solutions:\")\n",
        "print(ranked_solutions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxLJvvFv90Fy"
      },
      "source": [
        "### Analyzing Tree-of-Thought Prompting Output\n",
        "\n",
        "The tree-of-thought prompting technique provides comprehensive solutions by breaking down each methodical process for scaling recipes, significantly enhancing understanding of various mathematical approaches involved. Unlike the self-consistency prompting approach, which presents conflicting ingredient amounts and lacks clarity on selection rationale, the ToT method systematically organizes information, allowing for the evaluation of each solution based on a clear ranking scheme. By showcasing multiple strategies for achieving the same goal, this output encourages critical thinking and helps in discerning the most effective method for different contexts. Additionally, the structured nature of the ToT output reduces ambiguity, making it easier to grasp complex concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czBuw5wv9_yR"
      },
      "source": [
        "# Mistral 7b\n",
        "\n",
        "In this section, we will shift our focus to the Mistral family of models, particularly the Mistral 7B, to showcase the effectiveness of various prompting techniques. By leveraging this powerful model, we will explore different methods for crafting prompts that enhance the model's ability to generate accurate and relevant responses. This exploration will highlight how same prompting strategies can influence the Mistral model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad8dtJJm9_yd"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScvUkGoK9_yd"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\" # the model is in gguf format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGmdZWdJ9_yd",
        "outputId": "eeb39764-f0c6-4dbe-d3ff-eff8a5d6d972"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfZcdJms9_ye",
        "outputId": "305c0aee-178a-4282-d254-620bca6290ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096 # Context window\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6tMq8xt9_ye"
      },
      "outputs": [],
      "source": [
        "mistral_prompt_template = \"\"\"<s>[INST]{prompt}[/INST]\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBd1IvGO9_ye"
      },
      "source": [
        "## Self-consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsCKVAbj9_ye"
      },
      "source": [
        "In self-consistency, we generate multiple answers to the same question and pick the answer that is repeated the most across these occurrences. This is particularly valuable for factual questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzHVDpvE9_yf"
      },
      "outputs": [],
      "source": [
        "answers_template = \"\"\"\n",
        "Context:\n",
        "{context}\n",
        "===\n",
        "Using the context above generate {num_answers} distinct answers to the following question:\n",
        "Question:\n",
        "{question}.\n",
        "\n",
        "Arrange your answers in numbered bullet points.\n",
        "Present only the answers in bullet points.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DnnKjMd9_yf"
      },
      "source": [
        "Here is an extract from the [Tesla 2022 10-K](https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm) statement that will be used as context for this demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qB0M43W9_yf"
      },
      "outputs": [],
      "source": [
        "tesla_annual_report_context =\"\"\"\n",
        "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
        "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxBNfIBb9_yf"
      },
      "outputs": [],
      "source": [
        "factual_question = \"What was the increase in annual revenue in 2022 compared to 2021?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfFfOK7K9_yg"
      },
      "outputs": [],
      "source": [
        "answers_prompt = mistral_prompt_template.format(\n",
        "    prompt=answers_template.format(\n",
        "        context=tesla_annual_report_context,\n",
        "        question=factual_question,\n",
        "        num_answers=3\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK0_2HXp9_yg"
      },
      "outputs": [],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=answers_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False # do not return the prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgQWsrYP9_yh"
      },
      "outputs": [],
      "source": [
        "factual_answers = response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EThHefWq9_yg",
        "outputId": "13047bff-2eeb-4b7a-bc63-5f0870ac0fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* The increase in annual revenue from 2021 to 2022 was $27.64 billion.\n",
            "* In 2022, total revenues were $81.46 billion, which is an increase of $27.64 billion compared to the previous year.\n",
            "* The company experienced a revenue growth of approximately $27.64 billion between 2021 and 2022.\n"
          ]
        }
      ],
      "source": [
        "print(factual_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z8CvXEA9_yh"
      },
      "outputs": [],
      "source": [
        "consistency_template = \"\"\"\n",
        "Here are {num_answers} answers to the question mentioned below:\n",
        "Question:\n",
        "{question}\n",
        "Answers:\n",
        "{answers}\n",
        "\n",
        "Observe the answers mentioned above and choose the answer that occurs most.\n",
        "Present only the most frequent solution in the following format.\n",
        "Final Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYEv2BOA9_yh"
      },
      "outputs": [],
      "source": [
        "consistency_prompt = mistral_prompt_template.format(\n",
        "    prompt=consistency_template.format(\n",
        "        num_answers=3,\n",
        "        question=factual_question,\n",
        "        answers=factual_answers\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBhPEaNH9_yh",
        "outputId": "fdbb2bba-215b-4775-8c9e-8f6047d39800"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=consistency_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False # do not return the prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doqJRiyO9_yh",
        "outputId": "1f7ccb8e-5756-40ae-ae16-7617838f91ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Answer: The increase in annual revenue from 2021 to 2022 was $27.64 billion.\n"
          ]
        }
      ],
      "source": [
        "print(response[\"choices\"][0][\"text\"].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDZZ24K9_yi"
      },
      "source": [
        "## Tree-of-Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qtiQV29_yi"
      },
      "source": [
        "Tree-of-thought prompting is a generalization of chain-of-thought prompting where the model is prompted to take multiple reasoning paths. This forces the LLM into a deliberate reasoning mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObOvaY_j9_yi"
      },
      "outputs": [],
      "source": [
        "solutions_template = \"\"\"\n",
        "Generate {num_solutions} distinct solutions for the following problem:\n",
        "Problem:\n",
        "{problem}.\n",
        "--\n",
        "\n",
        "Consider the following factors in coming up with your solutions.\n",
        "Factors:\n",
        "{factors}\n",
        "\n",
        "Present the {num_solutions} solutions in numbered bullet points. Present only the solutions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTrEs2Wd9_yi"
      },
      "outputs": [],
      "source": [
        "climate_problem = \"Reduce the impact of climate change on the occurrence of extreme events in the Earth's atmosphere.\"\n",
        "\n",
        "climate_factors = \"\"\"\n",
        "1. Renewable Energy Transition\n",
        "2. Reforestation\n",
        "3. Sustainable Agricultural Practises\n",
        "4. Carbon capture and storage\n",
        "5. Climate-resilient infrastructure\n",
        "6. Circular economy practises\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CInwKcCF9_yi"
      },
      "outputs": [],
      "source": [
        "solutions_prompt = mistral_prompt_template.format(\n",
        "    prompt=solutions_template.format(\n",
        "        num_solutions=3,\n",
        "        problem=climate_problem,\n",
        "        factors=climate_factors\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZozUID1C9_yj",
        "outputId": "d350aa68-ac22-42a8-8849-68f385b2cbf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=solutions_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False # do not return the prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T6PIF_v9_yj"
      },
      "outputs": [],
      "source": [
        "climate_solutions = response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4E2S72w9_yj",
        "outputId": "4d506e3b-5644-43e7-806b-faa76889fce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution 1:\n",
            "- Accelerate the transition to renewable energy sources by investing in solar, wind, hydroelectric, and other clean energy technologies.\n",
            "- Encourage governments and businesses to set ambitious targets for reducing greenhouse gas emissions.\n",
            "- Implement policies that incentivize individuals and organizations to adopt renewable energy solutions.\n",
            "\n",
            "Solution 2:\n",
            "- Increase global reforestation efforts by planting new trees, protecting existing forests, and promoting sustainable forest management practices.\n",
            "- Encourage the use of agroforestry systems, which combine agriculture with tree cultivation, to help sequester carbon and reduce deforestation.\n",
            "- Support research into innovative approaches for afforestation (planting new forests in previously treeless areas) and reforestation (reestablishing forests on degraded or deforested land).\n",
            "\n",
            "Solution 3:\n",
            "- Implement climate-resilient infrastructure projects, such as green roofs, rainwater harvesting systems, and permeable pavements.\n",
            "- Encourage the use of sustainable agricultural practices that reduce emissions and increase carbon sequestration in soil, such as regenerative agriculture, agroforestry, and organic farming.\n",
            "- Promote circular economy practises, which minimize waste and maximize resource efficiency, to help reduce greenhouse gas emissions throughout supply chains.\n",
            "- Invest in research and development of new technologies for carbon capture and storage (CCS) to help mitigate the impact of industrial processes that are difficult to decarbonize.\n"
          ]
        }
      ],
      "source": [
        "print(climate_solutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdQh3kDx9_yj"
      },
      "outputs": [],
      "source": [
        "evaluation_template = \"\"\"\n",
        "For the following problem: {problem}, evaluate each solution in the following proposed solutions: \\n{solutions}\\n.\n",
        "Analyze pros, cons, feasibility, and probability of success for each solution.\n",
        "Present your evaluations of each solutions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8WNm7dr9_yj"
      },
      "outputs": [],
      "source": [
        "evaluations_prompt = mistral_prompt_template.format(\n",
        "    prompt=evaluation_template.format(\n",
        "        problem=climate_problem,\n",
        "        solutions=climate_solutions\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFRhLojH9_yj",
        "outputId": "c895065e-ac17-438c-daad-fc84f6b0da68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=evaluations_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False # do not return the prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnPDdwLQ9_yk"
      },
      "outputs": [],
      "source": [
        "climate_proposal_evaluations = response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC8kXBr-9_yk",
        "outputId": "f297df88-3ab7-4407-a4c0-e011513495b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution 1: Transitioning to Renewable Energy Sources\n",
            "Pros:\n",
            "- Reduces greenhouse gas emissions from the power sector, which is a major contributor to climate change.\n",
            "- Creates jobs and stimulates economic growth in the renewable energy industry.\n",
            "- Decreases dependence on fossil fuels, reducing vulnerability to price volatility and supply disruptions.\n",
            "Cons:\n",
            "- High upfront costs for infrastructure development and implementation of renewable energy technologies.\n",
            "- Intermittency of some renewable energy sources (e.g., solar and wind) may require backup power or energy storage solutions.\n",
            "- Political challenges in implementing policies that incentivize the transition to renewable energy, particularly in countries with strong fossil fuel industries.\n",
            "Feasibility: Renewable energy technologies have become increasingly cost-competitive with traditional fossil fuels, making them a viable alternative for many applications. However, significant investment is still required to build out infrastructure and replace existing power plants.\n",
            "Probability of Success: The transition to renewable energy has gained momentum in recent years, with many countries setting ambitious targets for reducing greenhouse gas emissions and increasing their use of clean energy sources. However, the pace of this transition will depend on continued investment and policy support at both the national and international levels.\n",
            "\n",
            "Solution 2: Reforestation and Forest Management\n",
            "Pros:\n",
            "- Trees absorb carbon dioxide from the atmosphere during photosynthesis, helping to mitigate climate change.\n",
            "- Forests provide a range of ecosystem services, including water filtration, soil stabilization, and habitat for biodiversity.\n",
            "- Agroforestry systems can help sequester carbon while also providing food and other benefits.\n",
            "Cons:\n",
            "- Deforestation continues to be a major driver of greenhouse gas emissions, particularly in tropical regions.\n",
            "- Forest restoration efforts can face challenges related to land ownership, funding, and political will.\n",
            "- Sustainable forest management practices may not always be economically viable for small farmers or communities.\n",
            "Feasibility: Reforestation and sustainable forest management are feasible solutions that have been shown to effectively sequester carbon and provide a range of ecosystem services. However, significant investment is required to scale up these efforts and address the root causes of deforestation.\n",
            "Probability of Success: The success of reforestation and forest management initiatives will depend on continued investment in research, policy support, and community engagement. There have been some successful examples of large-scale reforestation projects, but more needs to be done to address the underlying drivers of deforestation and ensure that these efforts are sustainable over the long term.\n",
            "\n",
            "Solution 3: Climate-Resilient Infrastructure and Circular Economy Practices\n",
            "Pros:\n",
            "- Reduces greenhouse gas emissions by improving energy efficiency, reducing waste, and promoting resource circularity.\n",
            "- Increases resilience to extreme weather events and other climate impacts.\n",
            "- Creates jobs and stimulates economic growth in the renewable energy and circular economy sectors.\n",
            "Cons:\n",
            "- High upfront costs for infrastructure development and implementation of climate-resilient technologies.\n",
            "- Challenges related to retrofitting existing infrastructure and changing business models to adopt circular economy practices.\n",
            "- Political challenges in implementing policies that incentivize the adoption of these solutions, particularly in industries with strong resistance to change.\n",
            "Feasibility: Climate-resilient infrastructure projects and circular economy practices are technically feasible solutions that have been shown to effectively reduce greenhouse gas emissions and increase resilience to climate impacts. However, significant investment is required to build out this infrastructure and implement these practices at scale.\n",
            "Probability of Success: The success of climate-resilient infrastructure and circular economy initiatives will depend on continued investment in research, policy support, and public awareness campaigns. There have been some successful examples of large-scale projects in this area, but more needs to be done to address the underlying drivers of greenhouse gas emissions and ensure that these solutions are adopted widely across industries and sectors.\n"
          ]
        }
      ],
      "source": [
        "print(climate_proposal_evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlhZYy349_yk"
      },
      "outputs": [],
      "source": [
        "ranking_template = \"\"\"\n",
        "For the following problem: {problem}, rank the solutions presented in the following evaluations: \\n{evaluations}\\n.\n",
        "Pick most promising solution and present implementation strategies and methods to handle potential obstacles for this solution.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Fo2XuS9_yk"
      },
      "outputs": [],
      "source": [
        "ranking_prompt = mistral_prompt_template.format(\n",
        "    prompt=ranking_template.format(\n",
        "        problem=climate_problem,\n",
        "        evaluations=climate_proposal_evaluations\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4RiDutG9_yl",
        "outputId": "43742033-a6d9-4777-b89d-ba8cc63a4715"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=ranking_prompt,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=False # do not return the prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvaJ0mMg9_yl"
      },
      "outputs": [],
      "source": [
        "climate_proposal_rankings = response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXY8jumi9_yl",
        "outputId": "2c0a2a92-0eb7-42ef-e92c-693ce22aa78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the evaluations provided, all three solutions - transitioning to renewable energy sources, reforestation and forest management, and climate-resilient infrastructure and circular economy practices - have their merits in reducing the impact of climate change on extreme events in the Earth's atmosphere. However, considering the current state of technology, economics, and political will, I would rank Solution 1: Transitioning to Renewable Energy Sources as the most promising solution for several reasons.\n",
            "\n",
            "Firstly, renewable energy sources have become increasingly cost-competitive with traditional fossil fuels in many applications, making them a viable alternative for power generation. This trend is expected to continue as technology advances and economies of scale are achieved. Moreover, transitioning to renewable energy reduces greenhouse gas emissions from the power sector, which is one of the largest contributors to climate change. Additionally, it creates jobs and stimulates economic growth in the renewable energy industry while decreasing dependence on fossil fuels, reducing vulnerability to price volatility and supply disruptions.\n",
            "\n",
            "However, there are some potential obstacles that need to be addressed for a successful implementation of this solution:\n",
            "\n",
            "1. High upfront costs for infrastructure development and implementation of renewable energy technologies: To overcome this challenge, governments and private sector investors can collaborate to provide incentives such as tax credits, subsidies, and grants to encourage the adoption of renewable energy sources. Additionally, public-private partnerships can be formed to share risks and costs associated with infrastructure development.\n",
            "2. Intermittency of some renewable energy sources: To address this challenge, a combination of renewable energy sources (e.g., solar, wind, hydro) can be used to ensure a consistent power supply. Energy storage solutions such as batteries or pumped hydroelectricity can also be employed to store excess energy generated during periods of high production and release it during times of low production.\n",
            "3. Political challenges in implementing policies that incentivize the transition to renewable energy: To overcome this challenge, international cooperation is essential to create a level playing field for renewable energy adoption across countries. This can be achieved through multilateral agreements such as the Paris Agreement and bilateral partnerships between governments. Additionally, public awareness campaigns can help build support for renewable energy among citizens and stakeholders.\n",
            "\n",
            "In conclusion, transitioning to renewable energy sources is a promising solution for reducing the impact of climate change on extreme events in the Earth's atmosphere. To ensure its successful implementation, it is essential to address potential obstacles such as high upfront costs, intermittency, and political challenges through collaborative efforts between governments, private sector investors, and civil society organizations.\n"
          ]
        }
      ],
      "source": [
        "print(climate_proposal_rankings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LW_ttDKZhfn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39612669f7ef418787a69793adf4e5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96cd4a2eb1ba4848b2a5971f19ebebc2",
            "placeholder": "​",
            "style": "IPY_MODEL_60b57cd21e654530b2655f96b0e2d0dd",
            "value": " 9.23G/9.23G [00:49&lt;00:00, 232MB/s]"
          }
        },
        "47d783445fa44f9d8991cbcbe56e51b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc5d0f9c2094089818e09806b912243": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b57cd21e654530b2655f96b0e2d0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6acb11b723f74caf91f0ebe424216f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96cd4a2eb1ba4848b2a5971f19ebebc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dc5eccec23d42d6ae165aaacec20de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52923855c9942768d84dc1f9e2337d0",
            "placeholder": "​",
            "style": "IPY_MODEL_6acb11b723f74caf91f0ebe424216f93",
            "value": "llama-2-13b-chat.Q5_K_M.gguf: 100%"
          }
        },
        "b52923855c9942768d84dc1f9e2337d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d305f57af6d74a29ab4684fbb3bf4165": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc5d0f9c2094089818e09806b912243",
            "max": 9229924224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f78067731ed84f84840a3551cf0673a2",
            "value": 9229924224
          }
        },
        "e07b31013f86488a8c91f3bf0294577a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9dc5eccec23d42d6ae165aaacec20de9",
              "IPY_MODEL_d305f57af6d74a29ab4684fbb3bf4165",
              "IPY_MODEL_39612669f7ef418787a69793adf4e5c2"
            ],
            "layout": "IPY_MODEL_47d783445fa44f9d8991cbcbe56e51b1"
          }
        },
        "f78067731ed84f84840a3551cf0673a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
